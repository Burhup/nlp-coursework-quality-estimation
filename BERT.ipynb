{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "omEQHVdOS61G"
   },
   "source": [
    "# NLP Coursework\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lweXud1Wpemd"
   },
   "source": [
    "## English-German"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yu6s3YOf_C93"
   },
   "source": [
    "### Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13411,
     "status": "ok",
     "timestamp": 1582886822510,
     "user": {
      "displayName": "Telefrom",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBh9maPEAruT7k1G9f7Tn7Ky24UYPIhWeOGtpcXzQ=s64",
      "userId": "18319353318992339821"
     },
     "user_tz": 0
    },
    "id": "scs7ICZrPFcs",
    "outputId": "30b5219e-99f6-425e-9e42-7b671dba8041"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bert-for-tf2 in /data/anaconda/envs/mli/lib/python3.6/site-packages (0.13.5)\n",
      "Requirement already satisfied: py-params>=0.7.3 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from bert-for-tf2) (0.8.2)\n",
      "Requirement already satisfied: params-flow>=0.7.1 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from bert-for-tf2) (0.7.4)\n",
      "Requirement already satisfied: numpy in /data/anaconda/envs/mli/lib/python3.6/site-packages (from params-flow>=0.7.1->bert-for-tf2) (1.18.1)\n",
      "Requirement already satisfied: tqdm in /data/anaconda/envs/mli/lib/python3.6/site-packages (from params-flow>=0.7.1->bert-for-tf2) (4.42.1)\n",
      "Requirement already satisfied: sentencepiece in /data/anaconda/envs/mli/lib/python3.6/site-packages (0.1.85)\n",
      "Requirement already satisfied: transformers in /data/anaconda/envs/mli/lib/python3.6/site-packages (2.5.1)\n",
      "Requirement already satisfied: requests in /data/anaconda/envs/mli/lib/python3.6/site-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: filelock in /data/anaconda/envs/mli/lib/python3.6/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: tokenizers==0.5.2 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: boto3 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from transformers) (1.12.9)\n",
      "Requirement already satisfied: tqdm>=4.27 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from transformers) (4.42.1)\n",
      "Requirement already satisfied: sentencepiece in /data/anaconda/envs/mli/lib/python3.6/site-packages (from transformers) (0.1.85)\n",
      "Requirement already satisfied: sacremoses in /data/anaconda/envs/mli/lib/python3.6/site-packages (from transformers) (0.0.38)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from transformers) (2020.2.20)\n",
      "Requirement already satisfied: numpy in /data/anaconda/envs/mli/lib/python3.6/site-packages (from transformers) (1.18.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from requests->transformers) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from requests->transformers) (2.9)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from boto3->transformers) (0.9.5)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.9 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from boto3->transformers) (1.15.9)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from boto3->transformers) (0.3.3)\n",
      "Requirement already satisfied: joblib in /data/anaconda/envs/mli/lib/python3.6/site-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: click in /data/anaconda/envs/mli/lib/python3.6/site-packages (from sacremoses->transformers) (7.0)\n",
      "Requirement already satisfied: six in /data/anaconda/envs/mli/lib/python3.6/site-packages (from sacremoses->transformers) (1.14.0)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.9->boto3->transformers) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.9->boto3->transformers) (2.8.1)\n"
     ]
    }
   ],
   "source": [
    "# Download and unzip the data\n",
    "from os.path import exists\n",
    "if not exists('ende_data.zip'):\n",
    "    !wget -O ende_data.zip https://competitions.codalab.org/my/datasets/download/c748d2c0-d6be-4e36-9f12-ca0e88819c4d\n",
    "    !unzip ende_data.zip\n",
    "!pip install bert-for-tf2\n",
    "!pip install sentencepiece\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 155
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13378,
     "status": "ok",
     "timestamp": 1582886822520,
     "user": {
      "displayName": "Telefrom",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBh9maPEAruT7k1G9f7Tn7Ky24UYPIhWeOGtpcXzQ=s64",
      "userId": "18319353318992339821"
     },
     "user_tz": 0
    },
    "id": "jPy_iwHnOSAZ",
    "outputId": "429c0ec0-331a-4f30-c213-6c084bd761de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---EN-DE---\n",
      "\n",
      "Source:  José Ortega y Gasset visited Husserl at Freiburg in 1934.\n",
      "\n",
      "Translation:  1934 besuchte José Ortega y Gasset Husserl in Freiburg.\n",
      "\n",
      "Score:  1.1016968715664406\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the files\n",
    "import io\n",
    "#English-German\n",
    "print(\"---EN-DE---\")\n",
    "print()\n",
    "\n",
    "with open(\"./train.ende.src\", encoding=\"utf8\") as ende_src:\n",
    "  print(\"Source: \", ende_src.readline())\n",
    "with open(\"./train.ende.mt\", encoding=\"utf8\") as ende_mt:\n",
    "  print(\"Translation: \", ende_mt.readline())\n",
    "with open(\"./train.ende.scores\", encoding=\"utf8\") as ende_scores:\n",
    "  print(\"Score: \", ende_scores.readline())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wiFHVnfH_Jpv"
   },
   "source": [
    "### Computing Sentence Embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g05fv5GiSyQ4"
   },
   "source": [
    "For this baseline model, we will simply use pre-trained GloVe embeddings via the Spacy module and compute the vector for each word and take the global mean for each sentence. We will do the same for both source and translation sentences. \n",
    "\n",
    "This is a very simplistic approach so feel free to be more creative and play around with how the sentence embeddings are computed for example ;)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 38680,
     "status": "ok",
     "timestamp": 1582886847860,
     "user": {
      "displayName": "Telefrom",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBh9maPEAruT7k1G9f7Tn7Ky24UYPIhWeOGtpcXzQ=s64",
      "userId": "18319353318992339821"
     },
     "user_tz": 0
    },
    "id": "96bRtBbuZLJe",
    "outputId": "3c5f06dc-e351-4392-cce8-13c4bda0116a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_md==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.2.5/en_core_web_md-2.2.5.tar.gz#egg=en_core_web_md==2.2.5 in /data/anaconda/envs/mli/lib/python3.6/site-packages (2.2.5)\n",
      "Requirement already satisfied: spacy>=2.2.2 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from en_core_web_md==2.2.5) (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.18.1)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.0.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.23.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.6.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.1)\n",
      "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (7.3.1)\n",
      "Requirement already satisfied: setuptools in /data/anaconda/envs/mli/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (45.2.0.post20200210)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.2)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /data/anaconda/envs/mli/lib/python3.6/site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (1.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2019.11.28)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (1.25.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2.9)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from thinc<7.4.0,>=7.3.0->spacy>=2.2.2->en_core_web_md==2.2.5) (4.42.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (2.2.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_md')\n",
      "\n",
      "\u001b[38;5;1m✘ Link 'en300' already exists\u001b[0m\n",
      "To overwrite an existing link, use the --force flag\n",
      "\n",
      "Requirement already satisfied: de_core_news_md==2.2.5 from https://github.com/explosion/spacy-models/releases/download/de_core_news_md-2.2.5/de_core_news_md-2.2.5.tar.gz#egg=de_core_news_md==2.2.5 in /data/anaconda/envs/mli/lib/python3.6/site-packages (2.2.5)\n",
      "Requirement already satisfied: spacy>=2.2.2 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from de_core_news_md==2.2.5) (2.2.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from spacy>=2.2.2->de_core_news_md==2.2.5) (3.0.2)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from spacy>=2.2.2->de_core_news_md==2.2.5) (1.0.1)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from spacy>=2.2.2->de_core_news_md==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from spacy>=2.2.2->de_core_news_md==2.2.5) (2.23.0)\n",
      "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from spacy>=2.2.2->de_core_news_md==2.2.5) (7.3.1)\n",
      "Requirement already satisfied: setuptools in /data/anaconda/envs/mli/lib/python3.6/site-packages (from spacy>=2.2.2->de_core_news_md==2.2.5) (45.2.0.post20200210)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from spacy>=2.2.2->de_core_news_md==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from spacy>=2.2.2->de_core_news_md==2.2.5) (0.6.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from spacy>=2.2.2->de_core_news_md==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from spacy>=2.2.2->de_core_news_md==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from spacy>=2.2.2->de_core_news_md==2.2.5) (2.0.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from spacy>=2.2.2->de_core_news_md==2.2.5) (1.18.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_md==2.2.5) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_md==2.2.5) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_md==2.2.5) (1.25.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_md==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from thinc<7.4.0,>=7.3.0->spacy>=2.2.2->de_core_news_md==2.2.5) (4.42.1)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /data/anaconda/envs/mli/lib/python3.6/site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_md==2.2.5) (1.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_md==2.2.5) (2.2.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('de_core_news_md')\n",
      "\n",
      "\u001b[38;5;1m✘ Link 'de300' already exists\u001b[0m\n",
      "To overwrite an existing link, use the --force flag\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Downloading spacy models for english and german\n",
    "\n",
    "!spacy download en_core_web_md\n",
    "!spacy link en_core_web_md en300\n",
    "\n",
    "!spacy download de_core_news_md\n",
    "!spacy link de_core_news_md de300\n",
    "\n",
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "import bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Om6kQX5bX2mB"
   },
   "source": [
    "We can now write our functions that will return the average embeddings for a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nhT2I6WYavY4"
   },
   "source": [
    "#### Pre-processing with Spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 128926,
     "status": "ok",
     "timestamp": 1582886938153,
     "user": {
      "displayName": "Telefrom",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBh9maPEAruT7k1G9f7Tn7Ky24UYPIhWeOGtpcXzQ=s64",
      "userId": "18319353318992339821"
     },
     "user_tz": 0
    },
    "id": "1yebpcR_IaA9",
    "outputId": "e68c971a-8753-4a98-e66c-022eb4e34166"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/group-59/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(sen):\n",
    "    # Removing html tags\n",
    "    sentence = remove_tags(sen)\n",
    "\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "\n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "\n",
    "    return sentence\n",
    "    \n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import download\n",
    "\n",
    "nlp_de = spacy.load('de300')\n",
    "nlp_en = spacy.load('en300')\n",
    "\n",
    "#downloading stopwords from the nltk package\n",
    "download('stopwords') #stopwords dictionary, run once\n",
    "\n",
    "stop_words_en = set(stopwords.words('english'))\n",
    "stop_words_de = set(stopwords.words('german'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 136350,
     "status": "ok",
     "timestamp": 1582886945620,
     "user": {
      "displayName": "Telefrom",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBh9maPEAruT7k1G9f7Tn7Ky24UYPIhWeOGtpcXzQ=s64",
      "userId": "18319353318992339821"
     },
     "user_tz": 0
    },
    "id": "g_NGZh_pIaBD",
    "outputId": "ceca45b7-84ba-4960-e04a-e57a86ac9e7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "file_de = open(\"./train.ende.src\") \n",
    "file_en = open(\"./train.ende.mt\") \n",
    "\n",
    "lines_de = file_de.readlines() \n",
    "lines_en = file_en.readlines() \n",
    "\n",
    "max_length = 0\n",
    "lengths_de = []\n",
    "lengths_en = []\n",
    "\n",
    "for i in range(len(lines_de)):\n",
    "    text_de = lines_en[i].lower()\n",
    "    text_en = lines_de[i].lower()\n",
    "\n",
    "    l = [token.lemma_ for token in nlp_en.tokenizer(text_en)]\n",
    "    #l = [word for word in l if word not in stop_words_en.union(set([\"\\n\", \".\", \",\"]))]\n",
    "    l = [word for word in l if word not in stop_words_en]\n",
    "    lengths_en.append(len(l))\n",
    "    if len(l) > max_length: \n",
    "        max_length = len(l)\n",
    "        \n",
    "    l = [token.lemma_ for token in nlp_de.tokenizer(text_de)]\n",
    "    #l = [word for word in l if word not in stop_words_de.union(set([\"\\n\", \".\", \",\"]))]\n",
    "    l = [word for word in l if word not in stop_words_de]\n",
    "    lengths_de.append(len(l))\n",
    "    if len(l) > max_length: \n",
    "        max_length = len(l)\n",
    "    \n",
    "print(max_length)\n",
    "\n",
    "# plt.figure()\n",
    "# plt.hist(lengths_en, bins=28)\n",
    "\n",
    "# plt.figure()\n",
    "# plt.hist(lengths_de, bins=31)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l2IBW5bgdjnJ"
   },
   "source": [
    "#### Preprocessing with Bert Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "19gsNCgnW8ZT"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "BertTokenizer = bert.bert_tokenization.FullTokenizer\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\", trainable=False)\n",
    "vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = BertTokenizer(vocabulary_file, to_lower_case)\n",
    "\n",
    "## Tokenize each sentences with Bert and adding the tags [CLS] and [SEP] to demarcate sentences\n",
    "def get_sentence_emb(line, nlp, lang):\n",
    "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize('[CLS] '+line+' [SEP]')) \n",
    "\n",
    "\n",
    "def get_embeddings(f,nlp,lang):\n",
    "    file = open(f) \n",
    "    lines = file.readlines()\n",
    "    sentences_vectors =[]\n",
    "\n",
    "    for l in lines:\n",
    "        vec = get_sentence_emb(l,nlp,lang)\n",
    "\n",
    "        if vec is not None:\n",
    "            sentences_vectors.append(vec)\n",
    "        else:\n",
    "            print(\"didn't work :\", l)\n",
    "            sentences_vectors.append(0)\n",
    "\n",
    "    return sentences_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NUKMgbo2sreI"
   },
   "source": [
    "#### Getting Training and Validation Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZXqZamIKs30T"
   },
   "source": [
    "We will now run the code fo the English-German translations and getting our training and validation sets ready for the regression task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 167710,
     "status": "ok",
     "timestamp": 1582886977040,
     "user": {
      "displayName": "Telefrom",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBh9maPEAruT7k1G9f7Tn7Ky24UYPIhWeOGtpcXzQ=s64",
      "userId": "18319353318992339821"
     },
     "user_tz": 0
    },
    "id": "LwoUIDj0otbf",
    "outputId": "fce26aff-bf75-41ae-b713-508daac9ab70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "## define our device, if possible cuda\n",
    "import torch\n",
    "GPU = True\n",
    "device_idx = 0\n",
    "if GPU:\n",
    "    device = torch.device(\"cuda:\" + str(device_idx) if torch.cuda.is_available() else \"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "# EN-DE files\n",
    "de_train_src = get_embeddings(\"./train.ende.src\",nlp_en,'en')\n",
    "de_train_mt = get_embeddings(\"./train.ende.mt\",nlp_de,'de')\n",
    "\n",
    "f_train_scores = open(\"./train.ende.scores\",'r')\n",
    "de_train_scores = f_train_scores.readlines()\n",
    "\n",
    "de_val_src = get_embeddings(\"./dev.ende.src\",nlp_en,'en')\n",
    "de_val_mt = get_embeddings(\"./dev.ende.mt\",nlp_de,'de')\n",
    "f_val_scores = open(\"./dev.ende.scores\",'r')\n",
    "de_val_scores = f_val_scores.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 167680,
     "status": "ok",
     "timestamp": 1582886977045,
     "user": {
      "displayName": "Telefrom",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBh9maPEAruT7k1G9f7Tn7Ky24UYPIhWeOGtpcXzQ=s64",
      "userId": "18319353318992339821"
     },
     "user_tz": 0
    },
    "id": "U_K1CHl5VxiE",
    "outputId": "b21422cd-c18e-4f5c-fc47-1cd0edbdf933"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mt: 7000 Training src: 7000\n",
      "Validation mt: 1000 Validation src: 1000\n"
     ]
    }
   ],
   "source": [
    "# Verify the preprocessing\n",
    "print(f\"Training mt: {len(de_train_mt)} Training src: {len(de_train_src)}\")\n",
    "print(f\"Validation mt: {len(de_val_mt)} Validation src: {len(de_val_src)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nU2YgORDee1d"
   },
   "source": [
    "### Prepare Data for Bert classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TzfsMTxZfC-B"
   },
   "source": [
    "#### Transform sentences and score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 167883,
     "status": "ok",
     "timestamp": 1582886977287,
     "user": {
      "displayName": "Telefrom",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBh9maPEAruT7k1G9f7Tn7Ky24UYPIhWeOGtpcXzQ=s64",
      "userId": "18319353318992339821"
     },
     "user_tz": 0
    },
    "id": "Px7ikaGoy9r0",
    "outputId": "b4fb518f-cf4b-48bc-a43d-c14c223b6f78"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Transformation into Torch tensor\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#regroup german and english sentences together\n",
    "import random \n",
    "X_train_en = de_train_src\n",
    "X_train_de = de_train_mt\n",
    "X_train = []\n",
    "\n",
    "for i in range(len(X_train_en)):\n",
    "    X_train.append(X_train_en[i]+X_train_de[i])\n",
    "\n",
    "X_train[0]\n",
    "    \n",
    "X_val_en = de_val_src\n",
    "X_val_de = de_val_mt\n",
    "X_val = []\n",
    "\n",
    "for i in range(len(X_val_en)):\n",
    "    X_val.append(X_val_en[i]+X_val_de[i])\n",
    "\n",
    "# use padding for our regroup sentences \n",
    "\n",
    "max_train=max([len(sen) for sen in X_train])\n",
    "input_ids = pad_sequences(X_train, maxlen=max_train, dtype=\"long\", \n",
    "                          value=0, truncating=\"post\", padding=\"post\")\n",
    "val_ids = pad_sequences(X_val, maxlen=max_train, dtype=\"long\", \n",
    "                          value=0, truncating=\"post\", padding=\"post\")\n",
    "\n",
    "# transform sentences into tensor\n",
    "train_inputs = torch.tensor(input_ids)\n",
    "validation_inputs = torch.tensor(val_ids)\n",
    "\n",
    "\n",
    "# normalized labels and create tensor\n",
    "train_scores = np.array(de_train_scores).astype(float)\n",
    "val_scores = np.array(de_val_scores).astype(float)\n",
    "min_t =np.amin( train_scores)\n",
    "max_t = np.amax( train_scores)\n",
    "\n",
    "train_labels = torch.tensor(train_scores).float()\n",
    "validation_labels = torch.tensor(val_scores).float()\n",
    "train_labels=(train_labels - min_t)/(max_t-min_t)\n",
    "validation_labels=(validation_labels - min_t)/(max_t-min_t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "42FS3HpGfauC"
   },
   "source": [
    "#### Create mask for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5I9j7XiYJdU8"
   },
   "outputs": [],
   "source": [
    "# Create attention masks\n",
    "train_masks = []\n",
    "\n",
    "# For each sentence of training set\n",
    "for sent in input_ids:\n",
    "    \n",
    "    # Create the attention mask.\n",
    "    #   Set the mask to 1\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]\n",
    "                \n",
    "    \n",
    "    # Store the attention mask for this sentence.\n",
    "    train_masks.append(att_mask)\n",
    "\n",
    "validation_masks = []\n",
    "\n",
    "# For each sentence of test set\n",
    "for sent in val_ids:\n",
    "    \n",
    "    # Create the attention mask.\n",
    "    #  set the mask to 1\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]\n",
    "    \n",
    "    # Store the attention mask for this sentence.\n",
    "    validation_masks.append(att_mask)\n",
    "\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uJVJ_tIpoadu"
   },
   "source": [
    "# Bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RZe9reAUgCnI"
   },
   "source": [
    "### Install model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 172144,
     "status": "ok",
     "timestamp": 1582886981600,
     "user": {
      "displayName": "Telefrom",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBh9maPEAruT7k1G9f7Tn7Ky24UYPIhWeOGtpcXzQ=s64",
      "userId": "18319353318992339821"
     },
     "user_tz": 0
    },
    "id": "RxQAAh4dttjC",
    "outputId": "3e946d5f-3f3b-4129-fd88-450e90092845"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-pretrained-bert in /data/anaconda/envs/mli/lib/python3.6/site-packages (0.6.2)\n",
      "Requirement already satisfied: pytorch-nlp in /data/anaconda/envs/mli/lib/python3.6/site-packages (0.5.0)\n",
      "Requirement already satisfied: regex in /data/anaconda/envs/mli/lib/python3.6/site-packages (from pytorch-pretrained-bert) (2020.2.20)\n",
      "Requirement already satisfied: boto3 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from pytorch-pretrained-bert) (1.12.9)\n",
      "Requirement already satisfied: numpy in /data/anaconda/envs/mli/lib/python3.6/site-packages (from pytorch-pretrained-bert) (1.18.1)\n",
      "Requirement already satisfied: tqdm in /data/anaconda/envs/mli/lib/python3.6/site-packages (from pytorch-pretrained-bert) (4.42.1)\n",
      "Requirement already satisfied: requests in /data/anaconda/envs/mli/lib/python3.6/site-packages (from pytorch-pretrained-bert) (2.23.0)\n",
      "Requirement already satisfied: torch>=0.4.1 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from pytorch-pretrained-bert) (1.4.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from boto3->pytorch-pretrained-bert) (0.3.3)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.9 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from boto3->pytorch-pretrained-bert) (1.15.9)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from boto3->pytorch-pretrained-bert) (0.9.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (2019.11.28)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (1.25.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.9->boto3->pytorch-pretrained-bert) (2.8.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.9->boto3->pytorch-pretrained-bert) (0.15.2)\n",
      "Requirement already satisfied: six>=1.5 in /data/anaconda/envs/mli/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.16.0,>=1.15.9->boto3->pytorch-pretrained-bert) (1.14.0)\n"
     ]
    }
   ],
   "source": [
    "# install\n",
    "!pip install pytorch-pretrained-bert pytorch-nlp\n",
    "\n",
    "# BERT imports\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig, BertModel\n",
    "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1FTlL_OAs_2O"
   },
   "outputs": [],
   "source": [
    "# Select a batch size for training. \n",
    "batch_size = 50\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader \n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2gBBzvYfyBbm"
   },
   "outputs": [],
   "source": [
    "#define model\n",
    "import torch.nn as nn\n",
    "class BertBinaryClassifier(nn.Module):\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super(BertBinaryClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(768, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, tokens, masks=None):\n",
    "        _, pooled_output = self.bert(tokens, attention_mask=masks, output_all_encoded_layers=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        proba = self.sigmoid(linear_output)\n",
    "        return proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q-u9p6hLgad-"
   },
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_clf = BertBinaryClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 897
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4899540,
     "status": "ok",
     "timestamp": 1582859752919,
     "user": {
      "displayName": "Telefrom",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBh9maPEAruT7k1G9f7Tn7Ky24UYPIhWeOGtpcXzQ=s64",
      "userId": "18319353318992339821"
     },
     "user_tz": 0
    },
    "id": "qkfzPF4ewP5W",
    "outputId": "994737cd-c091-476c-f8ec-e83103a98060"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================\n",
      "Epoch:  1\n",
      "======================================================\n",
      "\n",
      "tensor([0.7045, 0.7602, 0.7792, 0.7589, 0.7353, 0.7326, 0.7798, 0.7322, 0.7030,\n",
      "        0.7280, 0.7466, 0.7559, 0.7432, 0.7716, 0.7497, 0.7015, 0.7630, 0.7996,\n",
      "        0.7434, 0.7848, 0.7262, 0.7279, 0.7899, 0.7052, 0.7394, 0.7534, 0.6954,\n",
      "        0.7851, 0.7634, 0.6853, 0.7711, 0.7725, 0.7316, 0.7331, 0.6764, 0.7748,\n",
      "        0.7449, 0.7418, 0.7578, 0.7718, 0.8075, 0.7934, 0.7971, 0.7080, 0.7935,\n",
      "        0.7485, 0.7934, 0.7536, 0.7694, 0.7372], grad_fn=<ViewBackward>)\n",
      "tensor([0.7885, 0.7716, 0.7502, 0.7410, 0.7445, 0.5946, 0.7078, 0.7857, 0.7601,\n",
      "        0.7343, 0.7701, 0.7054, 0.7376, 0.7748, 0.7449, 0.7154, 0.7514, 0.7514,\n",
      "        0.7623, 0.7450, 0.6558, 0.7983, 0.7779, 0.6974, 0.8266, 0.7364, 0.7547,\n",
      "        0.7872, 0.7383, 0.7492, 0.7721, 0.7616, 0.5043, 0.7364, 0.7767, 0.7579,\n",
      "        0.7092, 0.7860, 0.6155, 0.7780, 0.7296, 0.8204, 0.8002, 0.7852, 0.7793,\n",
      "        0.6415, 0.8038, 0.7296, 0.7373, 0.7424])\n",
      "\n",
      "0/140.0 loss: 0.5715100765228271 \n",
      "tensor([0.7588, 0.7195, 0.7288, 0.7793, 0.7125, 0.7240, 0.7289, 0.7339, 0.7518,\n",
      "        0.7828, 0.7602, 0.7773, 0.7333, 0.7524, 0.7685, 0.7741, 0.7721, 0.7376,\n",
      "        0.7439, 0.7346, 0.7600, 0.7327, 0.7187, 0.7718, 0.7385, 0.7551, 0.7418,\n",
      "        0.7241, 0.7353, 0.7093, 0.7498, 0.7438, 0.6741, 0.7055, 0.7077, 0.7713,\n",
      "        0.7307, 0.7750, 0.7226, 0.7288, 0.7592, 0.6826, 0.7439, 0.7466, 0.7784,\n",
      "        0.7757, 0.7506, 0.7831, 0.7487, 0.7665], grad_fn=<ViewBackward>)\n",
      "tensor([0.7899, 0.7391, 0.7531, 0.7598, 0.7688, 0.7682, 0.7003, 0.7728, 0.5420,\n",
      "        0.7053, 0.7729, 0.5614, 0.7170, 0.7852, 0.7251, 0.7471, 0.7297, 0.7863,\n",
      "        0.8612, 0.7876, 0.5913, 0.7838, 0.6665, 0.7641, 0.7762, 0.7263, 0.7670,\n",
      "        0.7715, 0.7542, 0.8104, 0.7490, 0.7938, 0.7955, 0.8702, 0.7966, 0.7848,\n",
      "        0.7554, 0.8007, 0.7112, 0.8100, 0.7662, 0.7748, 0.7084, 0.7628, 0.7330,\n",
      "        0.8511, 0.7707, 0.8003, 0.7716, 0.7304])\n",
      "\n",
      "1/140.0 loss: 0.5663003921508789 \n",
      "tensor([0.6776, 0.7816, 0.7666, 0.7557, 0.7579, 0.7793, 0.7527, 0.7794, 0.7012,\n",
      "        0.7371, 0.7456, 0.7533, 0.7813, 0.7305, 0.6626, 0.7586, 0.7378, 0.7328,\n",
      "        0.7578, 0.7157, 0.7329, 0.7244, 0.7632, 0.7292, 0.7489, 0.7333, 0.7500,\n",
      "        0.6781, 0.7537, 0.7449, 0.7411, 0.7741, 0.7218, 0.7536, 0.7368, 0.7718,\n",
      "        0.7906, 0.7230, 0.6973, 0.7442, 0.7876, 0.7035, 0.7430, 0.7108, 0.7136,\n",
      "        0.7617, 0.7230, 0.7765, 0.7612, 0.7586], grad_fn=<ViewBackward>)\n",
      "tensor([0.7659, 0.7259, 0.8036, 0.7290, 0.7634, 0.7291, 0.7540, 0.7439, 0.7910,\n",
      "        0.7285, 0.7263, 0.7643, 0.7242, 0.7501, 0.5385, 0.7188, 0.7677, 0.7683,\n",
      "        0.7852, 0.7775, 0.5590, 0.7708, 0.7780, 0.6818, 0.8041, 0.7869, 0.7607,\n",
      "        0.1385, 0.7338, 0.7846, 0.8233, 0.6657, 0.7317, 0.5623, 0.7287, 0.7856,\n",
      "        0.7796, 0.7633, 0.8098, 0.7525, 0.7910, 0.7539, 0.7752, 0.7573, 0.7842,\n",
      "        0.7770, 0.7987, 0.7384, 0.7380, 0.8086])\n",
      "\n",
      "2/140.0 loss: 0.5692192514737447 \n",
      "tensor([0.7483, 0.7249, 0.7638, 0.7778, 0.7489, 0.7914, 0.7643, 0.7814, 0.7276,\n",
      "        0.6842, 0.7447, 0.7825, 0.7375, 0.7341, 0.7128, 0.7693, 0.7306, 0.7071,\n",
      "        0.7543, 0.7559, 0.7239, 0.7401, 0.7732, 0.7060, 0.7577, 0.7181, 0.7172,\n",
      "        0.7343, 0.7430, 0.7281, 0.7477, 0.6994, 0.7549, 0.7231, 0.7225, 0.7543,\n",
      "        0.7626, 0.7238, 0.7547, 0.7398, 0.7847, 0.6561, 0.7694, 0.7516, 0.7511,\n",
      "        0.7749, 0.7447, 0.7249, 0.7933, 0.7743], grad_fn=<ViewBackward>)\n",
      "tensor([0.7524, 0.7728, 0.7295, 0.7384, 0.7677, 0.7584, 0.7782, 0.7528, 0.8312,\n",
      "        0.7682, 0.7788, 0.7525, 0.7228, 0.7266, 0.7663, 0.7716, 0.8106, 0.7776,\n",
      "        0.8179, 0.7548, 0.6729, 0.7910, 0.7890, 0.7342, 0.7979, 0.7390, 0.7802,\n",
      "        0.6634, 0.7945, 0.8199, 0.8073, 0.6802, 0.7222, 0.7438, 0.7580, 0.7652,\n",
      "        0.7316, 0.7799, 0.8012, 0.7246, 0.7582, 0.6029, 0.7658, 0.7313, 0.7638,\n",
      "        0.8155, 0.7823, 0.7997, 0.8010, 0.7667])\n",
      "\n",
      "3/140.0 loss: 0.5647588521242142 \n",
      "tensor([0.7441, 0.7678, 0.7525, 0.7639, 0.7154, 0.7452, 0.7625, 0.7739, 0.7218,\n",
      "        0.7873, 0.7355, 0.7901, 0.7808, 0.7384, 0.7687, 0.7610, 0.7580, 0.7427,\n",
      "        0.7412, 0.7197, 0.7295, 0.7019, 0.7303, 0.7268, 0.7671, 0.7958, 0.6919,\n",
      "        0.7427, 0.7467, 0.7710, 0.7945, 0.8004, 0.7807, 0.7367, 0.7411, 0.7223,\n",
      "        0.7319, 0.7337, 0.7598, 0.7494, 0.7613, 0.7761, 0.7404, 0.7439, 0.7596,\n",
      "        0.7529, 0.7395, 0.7669, 0.7210, 0.7713], grad_fn=<ViewBackward>)\n",
      "tensor([0.7744, 0.7695, 0.7693, 0.6692, 0.7241, 0.7889, 0.7649, 0.7905, 0.7620,\n",
      "        0.6611, 0.7369, 0.7758, 0.7876, 0.7705, 0.7032, 0.7694, 0.7566, 0.7728,\n",
      "        0.7605, 0.7618, 0.7266, 0.7279, 0.7863, 0.7986, 0.7683, 0.6270, 0.7695,\n",
      "        0.8017, 0.7852, 0.7285, 0.7758, 0.8002, 0.7499, 0.7968, 0.7517, 0.7599,\n",
      "        0.6319, 0.8203, 0.7845, 0.7478, 0.7427, 0.7526, 0.7034, 0.7397, 0.7478,\n",
      "        0.8265, 0.7699, 0.5379, 0.8330, 0.1854])\n",
      "\n",
      "4/140.0 loss: 0.5670819640159607 \n",
      "tensor([0.7705, 0.7287, 0.7530, 0.7008, 0.7627, 0.7805, 0.7486, 0.7507, 0.7960,\n",
      "        0.7599, 0.7309, 0.7566, 0.7633, 0.7467, 0.7803, 0.7119, 0.7793, 0.7029,\n",
      "        0.7147, 0.7346, 0.7490, 0.7528, 0.7276, 0.7326, 0.7634, 0.7183, 0.7520,\n",
      "        0.7288, 0.7706, 0.7456, 0.7316, 0.7665, 0.7405, 0.7509, 0.7458, 0.7698,\n",
      "        0.7219, 0.7275, 0.7173, 0.7255, 0.7686, 0.7692, 0.7133, 0.7638, 0.7278,\n",
      "        0.7589, 0.7057, 0.7671, 0.7596, 0.7686], grad_fn=<ViewBackward>)\n",
      "tensor([0.7882, 0.7492, 0.7852, 0.7910, 0.7564, 0.6975, 0.7330, 0.7769, 0.7620,\n",
      "        0.7475, 0.7987, 0.8201, 0.7751, 0.6806, 0.7939, 0.7382, 0.7162, 0.7770,\n",
      "        0.8007, 0.7932, 0.7651, 0.7426, 0.7321, 0.8116, 0.7435, 0.7913, 0.7966,\n",
      "        0.7805, 0.6510, 0.8106, 0.7506, 0.7203, 0.7318, 0.7263, 0.7378, 0.7714,\n",
      "        0.7164, 0.5768, 0.7749, 0.7613, 0.7486, 0.7521, 0.7323, 0.7926, 0.8037,\n",
      "        0.7688, 0.6847, 0.6064, 0.7253, 0.8054])\n",
      "\n",
      "5/140.0 loss: 0.5662612815697988 \n",
      "tensor([0.7717, 0.7424, 0.6919, 0.7593, 0.7513, 0.8104, 0.7446, 0.7526, 0.7554,\n",
      "        0.7558, 0.7586, 0.7130, 0.7409, 0.7473, 0.8178, 0.7317, 0.7543, 0.7106,\n",
      "        0.7462, 0.7662, 0.7652, 0.7272, 0.7217, 0.7068, 0.7484, 0.7376, 0.7607,\n",
      "        0.7369, 0.8010, 0.7413, 0.7800, 0.7572, 0.7442, 0.7388, 0.7289, 0.7229,\n",
      "        0.7970, 0.7133, 0.7484, 0.7588, 0.7382, 0.7249, 0.7052, 0.7399, 0.7729,\n",
      "        0.7732, 0.7526, 0.7067, 0.7706, 0.7598], grad_fn=<ViewBackward>)\n",
      "tensor([0.7359, 0.8120, 0.7633, 0.7933, 0.7833, 0.7504, 0.7526, 0.7933, 0.7846,\n",
      "        0.7958, 0.7430, 0.7816, 0.7966, 0.8306, 0.7685, 0.7333, 0.7805, 0.8043,\n",
      "        0.7688, 0.7715, 0.4854, 0.7748, 0.7766, 0.7656, 0.7884, 0.7703, 0.8072,\n",
      "        0.7753, 0.7553, 0.4025, 0.7966, 0.7724, 0.7285, 0.7616, 0.8252, 0.7719,\n",
      "        0.5641, 0.3512, 0.7656, 0.7503, 0.8295, 0.7782, 0.7458, 0.7997, 0.7548,\n",
      "        0.7701, 0.8205, 0.7695, 0.6852, 0.8008])\n",
      "\n",
      "6/140.0 loss: 0.5660724299294608 \n",
      "tensor([0.7679, 0.6826, 0.7070, 0.7612, 0.7957, 0.7028, 0.8022, 0.7401, 0.7683,\n",
      "        0.7511, 0.7381, 0.7212, 0.7819, 0.7884, 0.7733, 0.7366, 0.7241, 0.7695,\n",
      "        0.7663, 0.8008, 0.7665, 0.7004, 0.7567, 0.7756, 0.7624, 0.7610, 0.7871,\n",
      "        0.7654, 0.7133, 0.7420, 0.7148, 0.7517, 0.7695, 0.7696, 0.7068, 0.7333,\n",
      "        0.7761, 0.7691, 0.7071, 0.7609, 0.8227, 0.8094, 0.7148, 0.7652, 0.7647,\n",
      "        0.7890, 0.7439, 0.7757, 0.7586, 0.6759], grad_fn=<ViewBackward>)\n",
      "tensor([0.7897, 0.8641, 0.7807, 0.5459, 0.2546, 0.7747, 0.7560, 0.7736, 0.8641,\n",
      "        0.5682, 0.7767, 0.7849, 0.6642, 0.7426, 0.8052, 0.7511, 0.7450, 0.7685,\n",
      "        0.7773, 0.7955, 0.7306, 0.7899, 0.7877, 0.7501, 0.8120, 0.7358, 0.7314,\n",
      "        0.7697, 0.7188, 0.7643, 0.6586, 0.7728, 0.7305, 0.8529, 0.7605, 0.7716,\n",
      "        0.7772, 0.7828, 0.6676, 0.7749, 0.7964, 0.7590, 0.5065, 0.7938, 0.7758,\n",
      "        0.7356, 0.7762, 0.6535, 0.7274, 0.6311])\n",
      "\n",
      "7/140.0 loss: 0.5677695795893669 \n",
      "tensor([0.7607, 0.7530, 0.7483, 0.7812, 0.7642, 0.7915, 0.7477, 0.7833, 0.7603,\n",
      "        0.7604, 0.7530, 0.7537, 0.8041, 0.7749, 0.7861, 0.7479, 0.7363, 0.7508,\n",
      "        0.7282, 0.7664, 0.6994, 0.7712, 0.7788, 0.7950, 0.7159, 0.7616, 0.7474,\n",
      "        0.7353, 0.7662, 0.7284, 0.7445, 0.7475, 0.7432, 0.7037, 0.7240, 0.7671,\n",
      "        0.7534, 0.8028, 0.7351, 0.7595, 0.7584, 0.7334, 0.6968, 0.7836, 0.7364,\n",
      "        0.7593, 0.7827, 0.7777, 0.7799, 0.7673], grad_fn=<ViewBackward>)\n",
      "tensor([0.7135, 0.7711, 0.7595, 0.7744, 0.7502, 0.7845, 0.7324, 0.7175, 0.7474,\n",
      "        0.7130, 0.7686, 0.7582, 0.7846, 0.7291, 0.7742, 0.7641, 0.7123, 0.7617,\n",
      "        0.7789, 0.7279, 0.7852, 0.6813, 0.7634, 0.5409, 0.7951, 0.7811, 0.8283,\n",
      "        0.6123, 0.6111, 0.7395, 0.7481, 0.7809, 0.7733, 0.5614, 0.7798, 0.7471,\n",
      "        0.7059, 0.7250, 0.7567, 0.7577, 0.7598, 0.6788, 0.7485, 0.7618, 0.8088,\n",
      "        0.7758, 0.7654, 0.7669, 0.7365, 0.6999])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "8/140.0 loss: 0.5686462918917338 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-9d39541c8a33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mprobas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_clf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mloss_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/mli/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-2f553fd1659f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tokens, masks)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_all_encoded_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mdropout_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpooled_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mlinear_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropout_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/mli/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/mli/lib/python3.6/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, output_all_encoded_layers)\u001b[0m\n\u001b[1;32m    731\u001b[0m         encoded_layers = self.encoder(embedding_output,\n\u001b[1;32m    732\u001b[0m                                       \u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m                                       output_all_encoded_layers=output_all_encoded_layers)\n\u001b[0m\u001b[1;32m    734\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoded_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m         \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/mli/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/mli/lib/python3.6/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_all_encoded_layers)\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0mall_encoder_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer_module\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moutput_all_encoded_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m                 \u001b[0mall_encoder_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/mli/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/mli/lib/python3.6/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/mli/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/mli/lib/python3.6/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_tensor, attention_mask)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0mself_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/mli/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/mli/lib/python3.6/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0mmixed_query_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0mmixed_key_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m         \u001b[0mmixed_value_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/mli/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/mli/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/mli/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1372\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1373\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "EPOCHS = 3\n",
    "train_loss=0\n",
    "train_loss_curve=[]\n",
    "test_loss=0\n",
    "test_loss_curve=[]\n",
    "optimizer = torch.optim.Adam(bert_clf.parameters(), lr=3e-6)\n",
    "bert_clf.train()\n",
    "\n",
    "#Train our model and print result at each epoch\n",
    "for epoch_num in range(EPOCHS):\n",
    "    print(\"\\n======================================================\")\n",
    "    print('Epoch: ', epoch_num + 1)\n",
    "    print(\"======================================================\\n\")\n",
    "    bert_clf.train()\n",
    "    for step_num, batch_data in enumerate(train_dataloader):\n",
    "        token_ids, masks, labels = tuple(t for t in batch_data)\n",
    "        probas = bert_clf(token_ids, masks)\n",
    "        loss_func = nn.BCELoss()\n",
    "        print(probas.view(-1))\n",
    "        print(labels)\n",
    "        print()\n",
    "        batch_loss = loss_func(probas.view(-1), labels)\n",
    "        train_loss += batch_loss.item()\n",
    "        train_loss_curve.append(train_loss / (step_num + 1))\n",
    "        bert_clf.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        print(\"\\r\" + \"{0}/{1} loss: {2} \".format(step_num, len(train_data) / batch_size, train_loss / (step_num + 1)))\n",
    "    # test at each epoch \n",
    "    bert_clf.eval()\n",
    "    with torch.no_grad():\n",
    "        for step_num, batch_data in enumerate(validation_dataloader):\n",
    "            token_ids, masks, labels = tuple(t for t in batch_data)\n",
    "            logits = bert_clf(token_ids, masks)\n",
    "            loss_func = nn.BCELoss()\n",
    "            batch_loss = loss_func(probas.view(-1), labels)\n",
    "            test_loss += batch_loss.item()\n",
    "            test_loss_curve.append(test_loss / (step_num + 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(train_loss_curve)\n",
    "plt.title(\"Training error\")\n",
    "plt.figure()\n",
    "plt.plot(test_loss_curve, color='orange')\n",
    "plt.title(\"Test err_or\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_clf = torch.load(\"bert-1-epoch.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ym_a2rZEgj7i"
   },
   "source": [
    "### Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AmapXCc30YnO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One\n",
      "One\n",
      "One\n",
      "One\n",
      "One\n",
      "One\n",
      "One\n",
      "One\n",
      "One\n",
      "One\n",
      "One\n",
      "One\n",
      "One\n",
      "One\n",
      "One\n",
      "One\n",
      "One\n",
      "One\n",
      "One\n",
      "One\n",
      "[0.7686386, 0.7642594, 0.76705843, 0.77130836, 0.76574004, 0.76259714, 0.7608875, 0.76757175, 0.7643948, 0.76353705, 0.77080905, 0.7711928, 0.76398325, 0.76731104, 0.7639812, 0.77033734, 0.76824296, 0.76552063, 0.7680111, 0.768026, 0.76854986, 0.76210946, 0.7692506, 0.77017605, 0.769563, 0.76582456, 0.76697254, 0.7685399, 0.7667852, 0.7672276, 0.7653242, 0.76853013, 0.769239, 0.7652554, 0.7650749, 0.76666707, 0.7674311, 0.76113105, 0.7640891, 0.7658187, 0.76836723, 0.7684849, 0.7690999, 0.76755255, 0.7693482, 0.7614947, 0.7652667, 0.76913077, 0.764423, 0.7634952, 0.7685646, 0.7644568, 0.7646172, 0.76868004, 0.771374, 0.7663773, 0.7658981, 0.76408666, 0.7672378, 0.7693468, 0.7666945, 0.76679707, 0.7632659, 0.7644874, 0.76318073, 0.7644417, 0.76870894, 0.767874, 0.76206726, 0.7691766, 0.7660236, 0.7637485, 0.7680888, 0.77171934, 0.7676098, 0.7652823, 0.76742876, 0.7640579, 0.77030027, 0.76437795, 0.7681531, 0.767781, 0.76558775, 0.7677947, 0.7634873, 0.76758844, 0.7661262, 0.7666543, 0.7666884, 0.7652112, 0.7690254, 0.76743114, 0.76579005, 0.7650324, 0.7697028, 0.7647373, 0.7680576, 0.76562077, 0.761137, 0.7713653, 0.7700825, 0.7686981, 0.7697652, 0.77144796, 0.7637907, 0.7674612, 0.76650697, 0.76644564, 0.7657245, 0.7686163, 0.7684433, 0.7661512, 0.77219397, 0.77117044, 0.7659125, 0.76764137, 0.76454884, 0.76469153, 0.76852685, 0.7668039, 0.76866126, 0.7662012, 0.76476353, 0.7656885, 0.77087235, 0.76359046, 0.7650638, 0.7663644, 0.762618, 0.7665681, 0.76675653, 0.76653373, 0.7724913, 0.76742166, 0.76610476, 0.76439184, 0.7662559, 0.76518285, 0.76700824, 0.7658211, 0.7676322, 0.7673066, 0.76664346, 0.76792943, 0.7711564, 0.7697593, 0.7642724, 0.7644383, 0.77246463, 0.76352876, 0.76932055, 0.7649274, 0.7668773, 0.763928, 0.76999027, 0.769305, 0.76901376, 0.76558244, 0.7668149, 0.7681047, 0.76796377, 0.7665549, 0.7709989, 0.7687345, 0.77098876, 0.76620585, 0.77308714, 0.76551545, 0.76764834, 0.76967883, 0.76840866, 0.76877147, 0.77089316, 0.7661906, 0.76797307, 0.7653402, 0.7640242, 0.76435053, 0.7735417, 0.76434404, 0.7676053, 0.77027357, 0.7711006, 0.76777005, 0.77190274, 0.7663379, 0.76821697, 0.7644709, 0.7662464, 0.7694849, 0.76226455, 0.77087134, 0.7647274, 0.7621926, 0.7704788, 0.77031475, 0.7678141, 0.76562893, 0.7653938, 0.7615405, 0.76378036, 0.76974213, 0.76730967, 0.76510096, 0.7690618, 0.7630553, 0.76765305, 0.76929694, 0.76965404, 0.77174, 0.7649637, 0.76746714, 0.7645858, 0.76531905, 0.765539, 0.76803917, 0.7680939, 0.76692563, 0.7708071, 0.76575524, 0.7587149, 0.76884305, 0.76713854, 0.7723732, 0.7676506, 0.76739997, 0.76860946, 0.7681783, 0.767693, 0.76782554, 0.76643246, 0.76638824, 0.76923084, 0.77029943, 0.76708496, 0.769626, 0.7595814, 0.7693705, 0.76735276, 0.76823515, 0.76948124, 0.76930773, 0.76784414, 0.76316047, 0.76690626, 0.7706918, 0.76633334, 0.7670862, 0.76485205, 0.7618035, 0.76974034, 0.7685071, 0.767571, 0.764975, 0.76586455, 0.76462644, 0.7671112, 0.7719113, 0.768601, 0.76615304, 0.76778275, 0.76760453, 0.7662215, 0.76564556, 0.7680792, 0.7672762, 0.76901895, 0.7640825, 0.7736473, 0.76213264, 0.7661804, 0.7673838, 0.7675368, 0.7649021, 0.7682058, 0.76587915, 0.7666651, 0.7692291, 0.7662455, 0.76873857, 0.7683885, 0.76660925, 0.77003706, 0.7658334, 0.76378655, 0.7656685, 0.766493, 0.7682993, 0.7688508, 0.77022356, 0.76621497, 0.7683798, 0.7678065, 0.7692047, 0.7693814, 0.7633194, 0.76703674, 0.7686763, 0.76551586, 0.76881105, 0.76880497, 0.7672083, 0.7670114, 0.7617942, 0.76611614, 0.7687295, 0.768328, 0.76940805, 0.7616376, 0.76369774, 0.7704706, 0.76392305, 0.7724384, 0.7648771, 0.7670984, 0.7726711, 0.7671168, 0.76712507, 0.7672288, 0.7688316, 0.76697004, 0.75861233, 0.7704671, 0.76246864, 0.7689691, 0.7618389, 0.77136666, 0.76392025, 0.7660345, 0.77417463, 0.76410186, 0.76633376, 0.7653813, 0.76879853, 0.7677964, 0.76974905, 0.76723176, 0.7750026, 0.7682919, 0.7656011, 0.7668944, 0.76159185, 0.7726221, 0.7665348, 0.76345134, 0.76752406, 0.76843566, 0.7708871, 0.76795936, 0.7663919, 0.76969594, 0.76718485, 0.7665122, 0.7664381, 0.7690817, 0.76314443, 0.76912034, 0.77123517, 0.76414984, 0.7648397, 0.7625345, 0.7658341, 0.770456, 0.7678298, 0.76985955, 0.76823395, 0.7665185, 0.7710112, 0.7741451, 0.7640843, 0.7687961, 0.7648327, 0.76600707, 0.7658238, 0.77026826, 0.7653087, 0.7699266, 0.76680285, 0.76806265, 0.7699501, 0.7698455, 0.76659113, 0.7647826, 0.7693291, 0.76726127, 0.76515853, 0.7671202, 0.7659858, 0.77029055, 0.77068055, 0.7677363, 0.7686596, 0.76645774, 0.76703095, 0.7650974, 0.76721495, 0.76824796, 0.76973224, 0.7679387, 0.76572263, 0.76729167, 0.76575524, 0.76382905, 0.7706432, 0.7649147, 0.7657286, 0.7678824, 0.7633987, 0.7630493, 0.7678717, 0.76767725, 0.77112424, 0.76766616, 0.77211857, 0.7699546, 0.7659988, 0.76877004, 0.763955, 0.76788056, 0.77235866, 0.7618481, 0.771598, 0.7708988, 0.7685571, 0.7648521, 0.7654968, 0.76808614, 0.7626311, 0.77167153, 0.76736593, 0.7651935, 0.7693813, 0.7692713, 0.7723454, 0.7683669, 0.7656523, 0.7715828, 0.7670008, 0.77118796, 0.7656224, 0.7724341, 0.76387715, 0.7655811, 0.7654179, 0.76648843, 0.764429, 0.7689759, 0.7677051, 0.76828516, 0.7692373, 0.7667794, 0.7674025, 0.7699062, 0.77098674, 0.7652876, 0.7683796, 0.7635794, 0.76317906, 0.7643349, 0.76765466, 0.76838744, 0.76742715, 0.76546824, 0.7632241, 0.7652154, 0.7649268, 0.7705746, 0.76854664, 0.76653206, 0.76522166, 0.77036667, 0.7622451, 0.7662555, 0.769271, 0.7701486, 0.7646529, 0.7659407, 0.7669039, 0.7655607, 0.76283056, 0.7663683, 0.7606708, 0.76707697, 0.76615566, 0.76886487, 0.76440334, 0.77251977, 0.7686127, 0.7653462, 0.7705147, 0.7673192, 0.76732355, 0.7659532, 0.76472557, 0.7688536, 0.767356, 0.7640032, 0.7683575, 0.7609906, 0.76363933, 0.7686017, 0.76607245, 0.77008677, 0.7675839, 0.7685177, 0.77113765, 0.7701306, 0.7695255, 0.76680464, 0.7710854, 0.7661998, 0.7670287, 0.7680809, 0.765918, 0.7678767, 0.7682533, 0.76603186, 0.7699004, 0.7665022, 0.7654706, 0.76577866, 0.7665025, 0.7690879, 0.7667925, 0.7635302, 0.7640718, 0.7677405, 0.76645005, 0.7675746, 0.76484174, 0.7660247, 0.7722192, 0.769001, 0.7698962, 0.7694195, 0.7648985, 0.7672676, 0.76489055, 0.76669985, 0.7657135, 0.76999635, 0.763591, 0.76563054, 0.763353, 0.7718668, 0.76847756, 0.7675982, 0.76759356, 0.7671305, 0.771191, 0.7648483, 0.76738316, 0.7747463, 0.76695144, 0.7699809, 0.7673655, 0.76733536, 0.76408947, 0.76602036, 0.7685251, 0.76426613, 0.7642672, 0.7656379, 0.76457405, 0.76615083, 0.7667057, 0.76830864, 0.7695234, 0.76282513, 0.77007884, 0.7651366, 0.7640762, 0.76860195, 0.7630854, 0.76655155, 0.7715906, 0.7644302, 0.7690649, 0.7604423, 0.768127, 0.76792866, 0.7620828, 0.7677232, 0.76895046, 0.7693219, 0.7695793, 0.7604353, 0.7684896, 0.765996, 0.76647, 0.76736194, 0.765935, 0.7721471, 0.7667807, 0.767034, 0.7670013, 0.765689, 0.7653829, 0.77129567, 0.76710755, 0.7724607, 0.765212, 0.7669825, 0.7712592, 0.7700081, 0.7696795, 0.7667566, 0.77004004, 0.76543355, 0.7640698, 0.7680878, 0.76473343, 0.76498204, 0.7614409, 0.76753217, 0.7653314, 0.76852846, 0.76671845, 0.7649709, 0.76905, 0.76940155, 0.7672094, 0.76929027, 0.7674339, 0.760994, 0.76608425, 0.76667786, 0.7666808, 0.7706746, 0.7683654, 0.76781166, 0.7669872, 0.7658316, 0.76856995, 0.7639887, 0.77017677, 0.77273226, 0.76713026, 0.76537454, 0.7659086, 0.7721914, 0.76991963, 0.76511675, 0.76457614, 0.7664989, 0.76983297, 0.7704865, 0.7652724, 0.7669976, 0.7651633, 0.7652205, 0.7675357, 0.7676427, 0.7649753, 0.76867443, 0.76593846, 0.7639082, 0.76596266, 0.7645222, 0.76708734, 0.76956767, 0.7612453, 0.76650393, 0.7687389, 0.76245785, 0.7623437, 0.77119833, 0.7677468, 0.77078646, 0.76658374, 0.7666152, 0.77037805, 0.76345164, 0.7638699, 0.76951176, 0.7620587, 0.7685257, 0.7660509, 0.76715016, 0.77195203, 0.7630189, 0.76924926, 0.76699615, 0.76493454, 0.7623167, 0.76526845, 0.76436156, 0.76801807, 0.7700074, 0.7610155, 0.7650793, 0.767923, 0.76796097, 0.76186395, 0.76798975, 0.76428425, 0.7706213, 0.7627193, 0.76521546, 0.7657569, 0.7654844, 0.76393443, 0.76999325, 0.76680285, 0.76653117, 0.76807827, 0.7648855, 0.77080566, 0.7675332, 0.77134585, 0.7649981, 0.7695071, 0.76870763, 0.7672027, 0.77033573, 0.76570547, 0.76774615, 0.7693252, 0.76690024, 0.7701774, 0.7665718, 0.7644667, 0.76642025, 0.76616925, 0.7637244, 0.7678988, 0.7656854, 0.76462436, 0.7697327, 0.76291233, 0.76247203, 0.7656862, 0.76803434, 0.7722098, 0.7682199, 0.7673632, 0.7725816, 0.7672832, 0.76930785, 0.7726621, 0.76722354, 0.7690131, 0.7663911, 0.76611257, 0.76683205, 0.76787966, 0.76750094, 0.7700511, 0.7683528, 0.76756394, 0.767492, 0.76364887, 0.7701993, 0.77086353, 0.76904905, 0.7645575, 0.7647113, 0.76688635, 0.7644936, 0.7669702, 0.76702785, 0.7690223, 0.7690911, 0.7660082, 0.76420194, 0.7681018, 0.7666107, 0.76731384, 0.7682345, 0.770289, 0.76473844, 0.7665413, 0.7684454, 0.761934, 0.7677909, 0.7655352, 0.7683376, 0.7689599, 0.76777166, 0.7679369, 0.7656733, 0.7627128, 0.7684056, 0.767339, 0.7683257, 0.76385003, 0.763522, 0.76976043, 0.7641102, 0.76471424, 0.7697525, 0.76837987, 0.7666119, 0.7645643, 0.7660851, 0.76512635, 0.7709019, 0.7649738, 0.76767695, 0.7707774, 0.7675579, 0.7699368, 0.7674336, 0.76923645, 0.7657006, 0.7714731, 0.76762295, 0.7664417, 0.76734906, 0.7670531, 0.7685479, 0.76474375, 0.76841617, 0.76550555, 0.76716566, 0.76922697, 0.7642487, 0.7645446, 0.766829, 0.7631365, 0.7680265, 0.7631022, 0.7670413, 0.76516, 0.7686119, 0.7663806, 0.7669865, 0.76356095, 0.76526415, 0.76945937, 0.76873326, 0.77018, 0.76686054, 0.76951057, 0.7676348, 0.7665488, 0.76700824, 0.7650132, 0.767321, 0.76889306, 0.7677886, 0.76761645, 0.7729351, 0.766832, 0.76724494, 0.76434135, 0.7650042, 0.7651054, 0.76770926, 0.7646266, 0.7682769, 0.7692149, 0.76487166, 0.76551425, 0.76714027, 0.75874656, 0.76390487, 0.768742, 0.76840526, 0.76787513, 0.7651682, 0.7680128, 0.76114595, 0.76711315, 0.76554066, 0.7666573, 0.76478785, 0.76862323, 0.7704735, 0.7653349, 0.768422, 0.76179546, 0.7661035, 0.76942027, 0.76667386, 0.76618564, 0.7646663, 0.7661522, 0.7702443, 0.77194107, 0.76465213, 0.76541716, 0.76813143, 0.7672822, 0.7699036, 0.7657141, 0.771432, 0.7666776, 0.7660167, 0.7632605, 0.7647896, 0.76461864, 0.7711766, 0.765566, 0.76572365, 0.7667772, 0.7702585, 0.7717266, 0.77218777, 0.7671922, 0.7628485, 0.7657774, 0.772417, 0.76553905, 0.7651925, 0.7687734, 0.76597744, 0.7684685, 0.76983726, 0.7668233, 0.7664199, 0.76616037, 0.769456, 0.76762867, 0.77018905, 0.76971424, 0.76992613, 0.76776606, 0.7686072, 0.7697936, 0.77059853, 0.7661827, 0.76989836, 0.76784045, 0.76965255, 0.7727455, 0.7676065, 0.7648481, 0.7683148, 0.7701977, 0.768413, 0.768749, 0.7650012, 0.76844245, 0.76686156, 0.76776487, 0.7698387, 0.7627759, 0.7670468, 0.7687313, 0.7676078, 0.765294, 0.76703334, 0.7701382, 0.7655868, 0.76921076, 0.77143586, 0.76780015, 0.7624721, 0.7687706, 0.76904595, 0.7667037, 0.7680243, 0.76610506, 0.76945144, 0.76685685, 0.7633447, 0.7618702, 0.7678806, 0.7682784, 0.76779395, 0.768422, 0.7700933, 0.76384526, 0.76713675, 0.7695322, 0.76262665, 0.76622254, 0.7676613, 0.7639486, 0.77202547, 0.7682424, 0.7667223, 0.7663706, 0.76526034, 0.77118886, 0.7642712, 0.7698472, 0.7699522, 0.7651534, 0.771282, 0.77018946, 0.76654804, 0.76486266, 0.7649301, 0.76955026, 0.7682378, 0.76554656, 0.76333034, 0.7682086, 0.7659633, 0.7692067, 0.77271545, 0.76329756, 0.76733565, 0.7674946, 0.76751584, 0.7677159, 0.7699054, 0.76392245]\n"
     ]
    }
   ],
   "source": [
    "#get prediction for test set\n",
    "bert_clf.eval()\n",
    "all_logits = []\n",
    "with torch.no_grad():\n",
    "    for step_num, batch_data in enumerate(validation_dataloader):\n",
    "        token_ids, masks, labels = tuple(t for t in batch_data)\n",
    "        logits = bert_clf(token_ids, masks)\n",
    "        numpy_logits = logits.cpu().detach().numpy()\n",
    "        all_logits += list(numpy_logits[:, 0])\n",
    "        print('One')        \n",
    "print(all_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UtiYdBccs-EY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7686386\n",
      "0.2415179820904676\n",
      "0.7642594\n",
      "0.19376137136707072\n",
      "0.76705843\n",
      "0.22428566584476606\n",
      "0.77130836\n",
      "0.270632412966755\n",
      "0.76574004\n",
      "0.20990817714050358\n",
      "0.76259714\n",
      "0.17563399648645017\n",
      "0.7608875\n",
      "0.1569898666205276\n",
      "0.76757175\n",
      "0.22988351984906608\n",
      "0.7643948\n",
      "0.19523818561437345\n",
      "0.76353705\n",
      "0.18588394537100683\n",
      "0.77080905\n",
      "0.26518731043609733\n",
      "0.7711928\n",
      "0.26937205080763427\n",
      "0.76398325\n",
      "0.19074989231436312\n",
      "0.76731104\n",
      "0.22704039242050023\n",
      "0.7639812\n",
      "0.1907277921011552\n",
      "0.77033734\n",
      "0.2600431608088307\n",
      "0.76824296\n",
      "0.23720324046476549\n",
      "0.76552063\n",
      "0.20751550405761598\n",
      "0.7680111\n",
      "0.234674716071277\n",
      "0.768026\n",
      "0.23483721763898302\n",
      "0.76854986\n",
      "0.2405501227532163\n",
      "0.76210946\n",
      "0.17031564517860254\n",
      "0.7692506\n",
      "0.2481915964729744\n",
      "0.77017605\n",
      "0.2582842438399915\n",
      "0.769563\n",
      "0.2515989293446115\n",
      "0.76582456\n",
      "0.21082988603252595\n",
      "0.76697254\n",
      "0.22334900680851533\n",
      "0.7685399\n",
      "0.2404415717059898\n",
      "0.7667852\n",
      "0.22130603709932828\n",
      "0.7672276\n",
      "0.22613038364135285\n",
      "0.7653242\n",
      "0.20537308338899507\n",
      "0.76853013\n",
      "0.24033497067757637\n",
      "0.769239\n",
      "0.2480654952564354\n",
      "0.7652554\n",
      "0.2046229761524696\n",
      "0.7650749\n",
      "0.20265475716442616\n",
      "0.76666707\n",
      "0.2200177246705639\n",
      "0.7674311\n",
      "0.22834950504993046\n",
      "0.76113105\n",
      "0.15964579224309716\n",
      "0.7640891\n",
      "0.1919043034513397\n",
      "0.7658187\n",
      "0.2107661854179863\n",
      "0.76836723\n",
      "0.23855850353942465\n",
      "0.7684849\n",
      "0.23984161591802433\n",
      "0.7690999\n",
      "0.24654838062034123\n",
      "0.76755255\n",
      "0.22967421782986186\n",
      "0.7693482\n",
      "0.24925630674457722\n",
      "0.7614947\n",
      "0.16361148050136798\n",
      "0.7652667\n",
      "0.20474647734392448\n",
      "0.76913077\n",
      "0.2468850838686265\n",
      "0.764423\n",
      "0.19554563858047125\n",
      "0.7634952\n",
      "0.18542764096889108\n",
      "0.7685646\n",
      "0.24071067430210924\n",
      "0.7644568\n",
      "0.19591419213602634\n",
      "0.7646172\n",
      "0.1976633590108019\n",
      "0.76868004\n",
      "0.2419697364486879\n",
      "0.771374\n",
      "0.2713480698709265\n",
      "0.7663773\n",
      "0.21685739418183836\n",
      "0.7658981\n",
      "0.21163199377071784\n",
      "0.76408666\n",
      "0.19187765319423633\n",
      "0.7672378\n",
      "0.2262415347136617\n",
      "0.7693468\n",
      "0.24924070659407782\n",
      "0.7666945\n",
      "0.2203167275551401\n",
      "0.76679707\n",
      "0.22143538834722065\n",
      "0.7632659\n",
      "0.182927066845048\n",
      "0.7644874\n",
      "0.19624764535295647\n",
      "0.76318073\n",
      "0.18199820788404608\n",
      "0.7644417\n",
      "0.195749740549509\n",
      "0.76870894\n",
      "0.24228498949003452\n",
      "0.767874\n",
      "0.23317970164839252\n",
      "0.76206726\n",
      "0.16985544073886238\n",
      "0.7691766\n",
      "0.24738493869088707\n",
      "0.7660236\n",
      "0.21300025697079406\n",
      "0.7637485\n",
      "0.18819016761987584\n",
      "0.7680888\n",
      "0.23552232424842678\n",
      "0.77171934\n",
      "0.2751142062040568\n",
      "0.7676098\n",
      "0.23029822384984833\n",
      "0.7652823\n",
      "0.2049161289806083\n",
      "0.76742876\n",
      "0.22832415480536916\n",
      "0.7640579\n",
      "0.19156370016543\n",
      "0.77030027\n",
      "0.2596388569083796\n",
      "0.76437795\n",
      "0.1950542338397323\n",
      "0.7681531\n",
      "0.23622303100836994\n",
      "0.767781\n",
      "0.23216569186591407\n",
      "0.76558775\n",
      "0.2082474111185597\n",
      "0.7677947\n",
      "0.23231519330820127\n",
      "0.7634873\n",
      "0.1853411901348725\n",
      "0.76758844\n",
      "0.23006552160489413\n",
      "0.7661262\n",
      "0.21411956776914565\n",
      "0.7666543\n",
      "0.21987862332860786\n",
      "0.7666884\n",
      "0.22025042691551633\n",
      "0.7652112\n",
      "0.20414132150579078\n",
      "0.7690254\n",
      "0.2457358727818164\n",
      "0.76743114\n",
      "0.2283501550562015\n",
      "0.76579005\n",
      "0.21045353240172204\n",
      "0.7650324\n",
      "0.20219130269333263\n",
      "0.7697028\n",
      "0.2531231940496834\n",
      "0.7647373\n",
      "0.19897312164650316\n",
      "0.7680576\n",
      "0.2351817209625171\n",
      "0.76562077\n",
      "0.2086075145925932\n",
      "0.761137\n",
      "0.15971079287017886\n",
      "0.7713653\n",
      "0.2712531689553881\n",
      "0.7700825\n",
      "0.2572637339948045\n",
      "0.7686981\n",
      "0.2421666883487461\n",
      "0.7697652\n",
      "0.25380375061523175\n",
      "0.77144796\n",
      "0.27215472765301385\n",
      "0.7637907\n",
      "0.188650372059616\n",
      "0.7674612\n",
      "0.22867775821669412\n",
      "0.76650697\n",
      "0.21827180782714173\n",
      "0.76644564\n",
      "0.2176029513744684\n",
      "0.7657245\n",
      "0.20973852550381977\n",
      "0.7686163\n",
      "0.24127487974518047\n",
      "0.7684433\n",
      "0.23938791154099093\n",
      "0.7661512\n",
      "0.21439192039661847\n",
      "0.77219397\n",
      "0.28029020613859323\n",
      "0.77117044\n",
      "0.2691282984560779\n",
      "0.7659125\n",
      "0.21178864528198638\n",
      "0.76764137\n",
      "0.2306427271733824\n",
      "0.76454884\n",
      "0.19691780181817187\n",
      "0.76469153\n",
      "0.19847391683051363\n",
      "0.76852685\n",
      "0.24029922033268036\n",
      "0.7668039\n",
      "0.22151013906836603\n",
      "0.76866126\n",
      "0.24176498447337913\n",
      "0.7662012\n",
      "0.21493727565783693\n",
      "0.76476353\n",
      "0.19925912440566407\n",
      "0.7656885\n",
      "0.20934592171624544\n",
      "0.77087235\n",
      "0.26587761709570756\n",
      "0.76359046\n",
      "0.18646635098966158\n",
      "0.7650638\n",
      "0.20253385599805362\n",
      "0.7663644\n",
      "0.21671699282734025\n",
      "0.762618\n",
      "0.1758614986812379\n",
      "0.7665681\n",
      "0.21893871426100198\n",
      "0.76675653\n",
      "0.22099338408306402\n",
      "0.76653373\n",
      "0.21856366064274013\n",
      "0.7724913\n",
      "0.28353243741744194\n",
      "0.76742166\n",
      "0.22824680405914144\n",
      "0.76610476\n",
      "0.2138855655116494\n",
      "0.76439184\n",
      "0.1952056853008326\n",
      "0.7662559\n",
      "0.21553398141444902\n",
      "0.76518285\n",
      "0.20383191852088167\n",
      "0.76700824\n",
      "0.22373836056473628\n",
      "0.7658211\n",
      "0.21079218566881863\n",
      "0.7676322\n",
      "0.23054262620767574\n",
      "0.7673066\n",
      "0.22699229195645998\n",
      "0.76664346\n",
      "0.21976032218731945\n",
      "0.76792943\n",
      "0.2337842074802552\n",
      "0.7711564\n",
      "0.2689748969761645\n",
      "0.7697593\n",
      "0.2537393999944211\n",
      "0.7642724\n",
      "0.19390307273410912\n",
      "0.7644383\n",
      "0.19571204018580168\n",
      "0.77246463\n",
      "0.2832418846143856\n",
      "0.76352876\n",
      "0.18579359449936206\n",
      "0.76932055\n",
      "0.2489547038349169\n",
      "0.7649274\n",
      "0.2010459916441487\n",
      "0.7668773\n",
      "0.22231029678774483\n",
      "0.763928\n",
      "0.19014733650131355\n",
      "0.76999027\n",
      "0.2562581742938459\n",
      "0.769305\n",
      "0.2487850521982331\n",
      "0.76901376\n",
      "0.24560912155900638\n",
      "0.76558244\n",
      "0.20818956056045756\n",
      "0.7668149\n",
      "0.2216297402221965\n",
      "0.7681047\n",
      "0.23569522591646397\n",
      "0.76796377\n",
      "0.234158611092246\n",
      "0.7665549\n",
      "0.21879441286888124\n",
      "0.7709989\n",
      "0.26725758040865877\n",
      "0.7687345\n",
      "0.24256384218021587\n",
      "0.77098876\n",
      "0.26714707934261916\n",
      "0.76620585\n",
      "0.21498797614695953\n",
      "0.77308714\n",
      "0.29003055010682743\n",
      "0.76551545\n",
      "0.2074589535120559\n",
      "0.76764834\n",
      "0.23071877790706807\n",
      "0.76967883\n",
      "0.25286189152881455\n",
      "0.76840866\n",
      "0.23901025789764496\n",
      "0.76877147\n",
      "0.2429668460681249\n",
      "0.77089316\n",
      "0.26610446928422427\n",
      "0.7661906\n",
      "0.21482157454163087\n",
      "0.76797307\n",
      "0.23426001207049474\n",
      "0.7653402\n",
      "0.20554793507584534\n",
      "0.7640242\n",
      "0.19119644662241697\n",
      "0.76435053\n",
      "0.19475523095515435\n",
      "0.7735417\n",
      "0.29498749792809953\n",
      "0.76434404\n",
      "0.19468438027163515\n",
      "0.7676053\n",
      "0.23024947337953705\n",
      "0.77027357\n",
      "0.25934765409905225\n",
      "0.7711006\n",
      "0.26836649110667743\n",
      "0.76777005\n",
      "0.23204609071208182\n",
      "0.77190274\n",
      "0.2771142754993683\n",
      "0.7663379\n",
      "0.21642774003682597\n",
      "0.76821697\n",
      "0.2369198377306887\n",
      "0.7644709\n",
      "0.19606759361593973\n",
      "0.7662464\n",
      "0.21542998041111794\n",
      "0.7694849\n",
      "0.25074677112356625\n",
      "0.76226455\n",
      "0.17200696149527595\n",
      "0.77087134\n",
      "0.2658665669891036\n",
      "0.7647274\n",
      "0.19886522060554768\n",
      "0.7621926\n",
      "0.17122240392639654\n",
      "0.7704788\n",
      "0.2615856256894862\n",
      "0.77031475\n",
      "0.2597968084321902\n",
      "0.7678141\n",
      "0.23252644534621858\n",
      "0.76562893\n",
      "0.2086965654516959\n",
      "0.7653938\n",
      "0.2061322907133114\n",
      "0.7615405\n",
      "0.1641106853173575\n",
      "0.76378036\n",
      "0.18853727096849227\n",
      "0.76974213\n",
      "0.2535521981884248\n",
      "0.76730967\n",
      "0.22702544227627008\n",
      "0.76510096\n",
      "0.20293880990477575\n",
      "0.7690618\n",
      "0.24613302661328795\n",
      "0.7630553\n",
      "0.1806305946902409\n",
      "0.76765305\n",
      "0.23077012840246347\n",
      "0.76929694\n",
      "0.24869730135167245\n",
      "0.76965404\n",
      "0.25259148892015304\n",
      "0.77174\n",
      "0.27533975838003144\n",
      "0.7649637\n",
      "0.20144184546307642\n",
      "0.76746714\n",
      "0.22874275884377582\n",
      "0.7645858\n",
      "0.19732080570607913\n",
      "0.76531905\n",
      "0.20531718284970424\n",
      "0.765539\n",
      "0.20771570598902933\n",
      "0.76803917\n",
      "0.2349808690248345\n",
      "0.7680939\n",
      "0.23557757478144659\n",
      "0.76692563\n",
      "0.22283745187337978\n",
      "0.7708071\n",
      "0.26516586022916044\n",
      "0.76575524\n",
      "0.210073928739563\n",
      "0.7587149\n",
      "0.1332971380491479\n",
      "0.76884305\n",
      "0.2437475035993799\n",
      "0.76713854\n",
      "0.22515927427274818\n",
      "0.7723732\n",
      "0.2822447749949486\n",
      "0.7676506\n",
      "0.23074347814536011\n",
      "0.76739997\n",
      "0.22801020177656284\n",
      "0.76860946\n",
      "0.24120012902403687\n",
      "0.7681783\n",
      "0.2364979836609269\n",
      "0.767693\n",
      "0.23120563260391336\n",
      "0.76782554\n",
      "0.23265124655021552\n",
      "0.76643246\n",
      "0.2174592999886169\n",
      "0.76638824\n",
      "0.21697699533566883\n",
      "0.76923084\n",
      "0.24797644439733268\n",
      "0.77029943\n",
      "0.25962975682058875\n",
      "0.76708496\n",
      "0.22457491863528034\n",
      "0.769626\n",
      "0.25228598597286656\n",
      "0.7595814\n",
      "0.14274627920805472\n",
      "0.7693705\n",
      "0.24949940908986257\n",
      "0.76735276\n",
      "0.2274953968100739\n",
      "0.76823515\n",
      "0.23711808964328895\n",
      "0.76948124\n",
      "0.25070712074104584\n",
      "0.76930773\n",
      "0.24881495248668983\n",
      "0.76784414\n",
      "0.2328540485067112\n",
      "0.76316047\n",
      "0.18177720575196687\n",
      "0.76690626\n",
      "0.22262619983536247\n",
      "0.7706918\n",
      "0.26390874810139486\n",
      "0.76633334\n",
      "0.21637833956024366\n",
      "0.7670862\n",
      "0.22458856876696842\n",
      "0.76485205\n",
      "0.20022438371783124\n",
      "0.7618035\n",
      "0.16697916299048643\n",
      "0.76974034\n",
      "0.2535326980002992\n",
      "0.7685071\n",
      "0.24008406825703865\n",
      "0.767571\n",
      "0.22987506976754446\n",
      "0.764975\n",
      "0.20156534665453307\n",
      "0.76586455\n",
      "0.21126604024024687\n",
      "0.76462644\n",
      "0.1977641099827796\n",
      "0.7671112\n",
      "0.22486092139444125\n",
      "0.7719113\n",
      "0.27720787640236644\n",
      "0.768601\n",
      "0.24110782813358078\n",
      "0.76615304\n",
      "0.2144120705910133\n",
      "0.76778275\n",
      "0.23218454204776684\n",
      "0.76760453\n",
      "0.23024102329801543\n",
      "0.7662215\n",
      "0.2151589277961854\n",
      "0.76564556\n",
      "0.2088779172012547\n",
      "0.7680792\n",
      "0.23541767323882468\n",
      "0.7672762\n",
      "0.22666078875834117\n",
      "0.76901895\n",
      "0.24566567210456824\n",
      "0.7640825\n",
      "0.19183215275527843\n",
      "0.7736473\n",
      "0.29613930903999197\n",
      "0.76213264\n",
      "0.17056849761795156\n",
      "0.7661804\n",
      "0.21471042346932023\n",
      "0.7673838\n",
      "0.2278340500771705\n",
      "0.7675368\n",
      "0.22950261617436496\n",
      "0.7649021\n",
      "0.20077038898532074\n",
      "0.7682058\n",
      "0.23679828655804513\n",
      "0.76587915\n",
      "0.21142529177659775\n",
      "0.7666651\n",
      "0.21999627446362702\n",
      "0.7692291\n",
      "0.24795759421547814\n",
      "0.7662455\n",
      "0.21542023031705604\n",
      "0.76873857\n",
      "0.2426080426066317\n",
      "0.7683885\n",
      "0.23879055577810782\n",
      "0.76660925\n",
      "0.21938721858786892\n",
      "0.77003706\n",
      "0.2567684292164394\n",
      "0.7658334\n",
      "0.21092608696060822\n",
      "0.76378655\n",
      "0.1886048716206581\n",
      "0.7656685\n",
      "0.2091281696155196\n",
      "0.766493\n",
      "0.2181197063597704\n",
      "0.7682993\n",
      "0.23781749639069183\n",
      "0.7688508\n",
      "0.2438320044145854\n",
      "0.77022356\n",
      "0.25880229883783556\n",
      "0.76621497\n",
      "0.21508742710639517\n",
      "0.7683798\n",
      "0.2386956548625676\n",
      "0.7678065\n",
      "0.23244324454355336\n",
      "0.7692047\n",
      "0.2476910916444428\n",
      "0.7693814\n",
      "0.2496183602374238\n",
      "0.7633194\n",
      "0.18351012246997378\n",
      "0.76703674\n",
      "0.22404906356218746\n",
      "0.7686763\n",
      "0.24192878605362544\n",
      "0.76551586\n",
      "0.20746350355595133\n",
      "0.76881105\n",
      "0.24339845023194862\n",
      "0.76880497\n",
      "0.24333214959232485\n",
      "0.7672083\n",
      "0.22591978160960657\n",
      "0.7670114\n",
      "0.22377281089708845\n",
      "0.7617942\n",
      "0.1668777620122377\n",
      "0.76611614\n",
      "0.21400971670937707\n",
      "0.7687295\n",
      "0.2425092416534671\n",
      "0.768328\n",
      "0.23813079941322535\n",
      "0.76940805\n",
      "0.24990891304048013\n",
      "0.7616376\n",
      "0.16517019553879386\n",
      "0.76369774\n",
      "0.18763636227713754\n",
      "0.7704706\n",
      "0.26149657483038347\n",
      "0.76392305\n",
      "0.1900933859808358\n",
      "0.7724384\n",
      "0.28295588185522647\n",
      "0.7648771\n",
      "0.2004973863515751\n",
      "0.7670984\n",
      "0.2247218200524852\n",
      "0.7726711\n",
      "0.28549350633650583\n",
      "0.7671168\n",
      "0.22492202198389855\n",
      "0.76712507\n",
      "0.22501237285554154\n",
      "0.7672288\n",
      "0.22614338376676812\n",
      "0.7688316\n",
      "0.2436227023953812\n",
      "0.76697004\n",
      "0.22332170654514094\n",
      "0.75861233\n",
      "0.13217847725706733\n",
      "0.7704671\n",
      "0.2614582244604051\n",
      "0.76246864\n",
      "0.17423258296656385\n",
      "0.7689691\n",
      "0.24512226686216287\n",
      "0.7618389\n",
      "0.16736526671535223\n",
      "0.77136666\n",
      "0.27126811909961646\n",
      "0.76392025\n",
      "0.19006283568610627\n",
      "0.7660345\n",
      "0.2131192081183535\n",
      "0.77417463\n",
      "0.30188991451793434\n",
      "0.76410186\n",
      "0.19204340479329396\n",
      "0.76633376\n",
      "0.2163828896041391\n",
      "0.7653813\n",
      "0.20599578939643948\n",
      "0.76879853\n",
      "0.2432619489150767\n",
      "0.7677964\n",
      "0.2323333934837848\n",
      "0.76974905\n",
      "0.2536275989158394\n",
      "0.76723176\n",
      "0.22617588408030898\n",
      "0.7750026\n",
      "0.3109191516258907\n",
      "0.7682919\n",
      "0.23773689561310896\n",
      "0.7656011\n",
      "0.2083930125232225\n",
      "0.7668944\n",
      "0.2224968485874701\n",
      "0.76159185\n",
      "0.16467099072280433\n",
      "0.7726221\n",
      "0.28495920118189133\n",
      "0.7665348\n",
      "0.21857536075561512\n",
      "0.76345134\n",
      "0.18494923635356741\n",
      "0.76752406\n",
      "0.2293635148324089\n",
      "0.76843566\n",
      "0.2393047107383257\n",
      "0.7708871\n",
      "0.2660381686446005\n",
      "0.76795936\n",
      "0.23411051062820576\n",
      "0.7663919\n",
      "0.21701664571818924\n",
      "0.76969594\n",
      "0.2530484433285398\n",
      "0.76718485\n",
      "0.2256643291451752\n",
      "0.7665122\n",
      "0.21832900837897284\n",
      "0.7664381\n",
      "0.21752105058434523\n",
      "0.7690817\n",
      "0.24635012870774098\n",
      "0.76314443\n",
      "0.1816023540651166\n",
      "0.76912034\n",
      "0.24677133277123353\n",
      "0.77123517\n",
      "0.2698342052661875\n",
      "0.76414984\n",
      "0.1925666598413045\n",
      "0.7648397\n",
      "0.2000898324197724\n",
      "0.7625345\n",
      "0.1749508398958195\n",
      "0.7658341\n",
      "0.21093388703585703\n",
      "0.770456\n",
      "0.2613373232940326\n",
      "0.7678298\n",
      "0.23269739699544445\n",
      "0.76985955\n",
      "0.25483271054193857\n",
      "0.76823395\n",
      "0.2371050895178719\n",
      "0.7665185\n",
      "0.2183972590374097\n",
      "0.7710112\n",
      "0.2673914817004466\n",
      "0.7741451\n",
      "0.30156816141387743\n",
      "0.7640843\n",
      "0.19185165294340223\n",
      "0.7687961\n",
      "0.24323529865797333\n",
      "0.7648327\n",
      "0.20001313167981571\n",
      "0.76600707\n",
      "0.21282020523377732\n",
      "0.7658238\n",
      "0.2108214359510061\n",
      "0.77026826\n",
      "0.2592898035409501\n",
      "0.7653087\n",
      "0.2052040817585823\n",
      "0.7699266\n",
      "0.25556396759661126\n",
      "0.76680285\n",
      "0.22149843895549104\n",
      "0.76806265\n",
      "0.2352369714955369\n",
      "0.7699501\n",
      "0.25582007006731367\n",
      "0.7698455\n",
      "0.2546793090620252\n",
      "0.76659113\n",
      "0.2191896166815397\n",
      "0.7647826\n",
      "0.19946712641232622\n",
      "0.7693291\n",
      "0.24904765473164403\n",
      "0.76726127\n",
      "0.22649763718436589\n",
      "0.76515853\n",
      "0.20356671596238662\n",
      "0.7671202\n",
      "0.22495907234133483\n",
      "0.7659858\n",
      "0.21258815299509415\n",
      "0.77029055\n",
      "0.25953290588623723\n",
      "0.77068055\n",
      "0.26378589691620924\n",
      "0.7677363\n",
      "0.23167818716279776\n",
      "0.7686596\n",
      "0.2417467842977956\n",
      "0.76645774\n",
      "0.21773490264744488\n",
      "0.76703095\n",
      "0.22398601295391707\n",
      "0.7650974\n",
      "0.20289980952852638\n",
      "0.76721495\n",
      "0.22599258231193886\n",
      "0.76824796\n",
      "0.23725784099151603\n",
      "0.76973224\n",
      "0.2534442971474675\n",
      "0.7679387\n",
      "0.23388495845223112\n",
      "0.76572263\n",
      "0.20971837530942494\n",
      "0.76729167\n",
      "0.22682914038248292\n",
      "0.76575524\n",
      "0.210073928739563\n",
      "0.76382905\n",
      "0.1890683260917534\n",
      "0.7706432\n",
      "0.26337834298440654\n",
      "0.7649147\n",
      "0.2009075403084637\n",
      "0.7657286\n",
      "0.20978337593650664\n",
      "0.7678824\n",
      "0.23327135253257758\n",
      "0.7633987\n",
      "0.18437528081643428\n",
      "0.7630493\n",
      "0.18056494405688817\n",
      "0.7678717\n",
      "0.23315435140382945\n",
      "0.76767725\n",
      "0.23103403094841646\n",
      "0.77112424\n",
      "0.2686245435961929\n",
      "0.76766616\n",
      "0.23091312978204392\n",
      "0.77211857\n",
      "0.2794679482060065\n",
      "0.7699546\n",
      "0.25586947054389597\n",
      "0.7659988\n",
      "0.21272985436213254\n",
      "0.76877004\n",
      "0.2429512459176255\n",
      "0.763955\n",
      "0.1904417893419943\n",
      "0.76788056\n",
      "0.23325120233818275\n",
      "0.77235866\n",
      "0.28208617346486875\n",
      "0.7618481\n",
      "0.1674653676810589\n",
      "0.771598\n",
      "0.27379079343666746\n",
      "0.7708988\n",
      "0.2661662198799526\n",
      "0.7685571\n",
      "0.24062877351198608\n",
      "0.7648521\n",
      "0.20022503372410227\n",
      "0.7654968\n",
      "0.20725550154928918\n",
      "0.76808614\n",
      "0.2354930739662393\n",
      "0.7626311\n",
      "0.17600450006081836\n",
      "0.77167153\n",
      "0.27459290117485935\n",
      "0.76736593\n",
      "0.2276390481959254\n",
      "0.7651935\n",
      "0.20394826964335877\n",
      "0.7693813\n",
      "0.24961706022488173\n",
      "0.7692713\n",
      "0.2484177986552183\n",
      "0.7723454\n",
      "0.28194187207274624\n",
      "0.7683669\n",
      "0.23855460350180024\n",
      "0.7656523\n",
      "0.20895136790985802\n",
      "0.7715828\n",
      "0.27362504183760805\n",
      "0.7670008\n",
      "0.22365710978088238\n",
      "0.77118796\n",
      "0.2693194002996986\n",
      "0.7656224\n",
      "0.20862506476190568\n",
      "0.7724341\n",
      "0.2829090814037265\n",
      "0.76387715\n",
      "0.18959288115230422\n",
      "0.7655811\n",
      "0.20817461041622742\n",
      "0.7654179\n",
      "0.2063948932467241\n",
      "0.76648843\n",
      "0.21806965587691707\n",
      "0.764429\n",
      "0.19561063920755295\n",
      "0.7689759\n",
      "0.24519636757703722\n",
      "0.7677051\n",
      "0.23133758387688985\n",
      "0.76828516\n",
      "0.23766344490450741\n",
      "0.7692373\n",
      "0.24804664507458085\n",
      "0.7667794\n",
      "0.2212429864910579\n",
      "0.7674025\n",
      "0.22803815204620825\n",
      "0.7699062\n",
      "0.2553416654519918\n",
      "0.77098674\n",
      "0.26712497912941124\n",
      "0.7652876\n",
      "0.20497397953871221\n",
      "0.7683796\n",
      "0.2386937048437563\n",
      "0.7635794\n",
      "0.18634609982956007\n",
      "0.76317906\n",
      "0.18198000770846257\n",
      "0.7643349\n",
      "0.1945849293121995\n",
      "0.76765466\n",
      "0.23078767857177596\n",
      "0.76838744\n",
      "0.23877885566523283\n",
      "0.76742715\n",
      "0.22830660463605668\n",
      "0.76546824\n",
      "0.2069441485455652\n",
      "0.7632241\n",
      "0.18247141244920329\n",
      "0.7652154\n",
      "0.2041868219447487\n",
      "0.7649268\n",
      "0.2010394915814402\n",
      "0.7705746\n",
      "0.2626308357729634\n",
      "0.76854664\n",
      "0.2405150224145931\n",
      "0.76653206\n",
      "0.21854546046715662\n",
      "0.76522166\n",
      "0.20425507260318554\n",
      "0.77036667\n",
      "0.26036296389407276\n",
      "0.7622451\n",
      "0.17179505945098938\n",
      "0.7662555\n",
      "0.21552943137055358\n",
      "0.769271\n",
      "0.24841454862386492\n",
      "0.7701486\n",
      "0.2579845909491443\n",
      "0.7646529\n",
      "0.19805271276702285\n",
      "0.7659407\n",
      "0.21209674825435343\n",
      "0.7669039\n",
      "0.22260019958453015\n",
      "0.7655607\n",
      "0.20795230827160793\n",
      "0.76283056\n",
      "0.17817942104298012\n",
      "0.7663683\n",
      "0.21675989324121403\n",
      "0.7606708\n",
      "0.15462644381982749\n",
      "0.76707697\n",
      "0.22448781779499072\n",
      "0.76615566\n",
      "0.21444067086692975\n",
      "0.76886487\n",
      "0.2439854058944988\n",
      "0.76440334\n",
      "0.19533113651110057\n",
      "0.77251977\n",
      "0.2838431404148949\n",
      "0.7686127\n",
      "0.24123522936266184\n",
      "0.7653462\n",
      "0.20561358570919808\n",
      "0.7705147\n",
      "0.2619775794707895\n",
      "0.7673192\n",
      "0.22712944327960294\n",
      "0.76732355\n",
      "0.22717689373737215\n",
      "0.7659532\n",
      "0.2122325995649561\n",
      "0.76472557\n",
      "0.19884507041115285\n",
      "0.7688536\n",
      "0.24386255470931495\n",
      "0.767356\n",
      "0.2275304971486971\n",
      "0.7640032\n",
      "0.19096764441508718\n",
      "0.7683575\n",
      "0.23845255251728226\n",
      "0.7609906\n",
      "0.15811437746904566\n",
      "0.76363933\n",
      "0.18699935613173402\n",
      "0.7686017\n",
      "0.2411156282088296\n",
      "0.76607245\n",
      "0.2135332621128665\n",
      "0.77008677\n",
      "0.2573105344463045\n",
      "0.7675839\n",
      "0.23001612112831182\n",
      "0.7685177\n",
      "0.2401991193669737\n",
      "0.77113765\n",
      "0.26877079500712675\n",
      "0.7701306\n",
      "0.2577882890553571\n",
      "0.7695255\n",
      "0.25119007540026494\n",
      "0.76680464\n",
      "0.22151793914361484\n",
      "0.7710854\n",
      "0.268200739507618\n",
      "0.7661998\n",
      "0.2149223255136068\n",
      "0.7670287\n",
      "0.2239613127156268\n",
      "0.7680809\n",
      "0.2354358734144082\n",
      "0.765918\n",
      "0.21184909586517264\n",
      "0.7678767\n",
      "0.23320895193057822\n",
      "0.7682533\n",
      "0.2373163415558892\n",
      "0.76603186\n",
      "0.21309060784243705\n",
      "0.7699004\n",
      "0.25527796483745036\n",
      "0.7665022\n",
      "0.2182198073254753\n",
      "0.7654706\n",
      "0.2069701487963993\n",
      "0.76577866\n",
      "0.21032938120399614\n",
      "0.7665025\n",
      "0.21822305735683045\n",
      "0.7690879\n",
      "0.2464177293599068\n",
      "0.7667925\n",
      "0.22138533786436732\n",
      "0.7635302\n",
      "0.18580919464986323\n",
      "0.7640718\n",
      "0.19171580163280133\n",
      "0.7677405\n",
      "0.23172368760175566\n",
      "0.76645005\n",
      "0.21765105183850864\n",
      "0.7675746\n",
      "0.22991472015006487\n",
      "0.76484174\n",
      "0.20011193263298033\n",
      "0.7660247\n",
      "0.2130126070899383\n",
      "0.7722192\n",
      "0.2805651587911502\n",
      "0.769001\n",
      "0.2454700202170521\n",
      "0.7698962\n",
      "0.25523246439849245\n",
      "0.7694195\n",
      "0.25003371424447707\n",
      "0.7648985\n",
      "0.20073073860280033\n",
      "0.7672676\n",
      "0.226566537849072\n",
      "0.76489055\n",
      "0.20064428776878174\n",
      "0.76669985\n",
      "0.22037522811951504\n",
      "0.7657135\n",
      "0.2096189243499893\n",
      "0.76999635\n",
      "0.25632447493346966\n",
      "0.763591\n",
      "0.18647220104609907\n",
      "0.76563054\n",
      "0.2087141156210084\n",
      "0.763353\n",
      "0.18387672600671578\n",
      "0.7718668\n",
      "0.276722321718065\n",
      "0.76847756\n",
      "0.2397616651467125\n",
      "0.7675982\n",
      "0.23017212263330933\n",
      "0.76759356\n",
      "0.23012142214418496\n",
      "0.7671305\n",
      "0.22507152342618753\n",
      "0.771191\n",
      "0.26935255061951047\n",
      "0.7648483\n",
      "0.20018343332277055\n",
      "0.76738316\n",
      "0.22782690000819095\n",
      "0.7747463\n",
      "0.3081241246613651\n",
      "0.76695144\n",
      "0.22311890458864347\n",
      "0.7699809\n",
      "0.2561561233093279\n",
      "0.7673655\n",
      "0.2276344981520282\n",
      "0.76733536\n",
      "0.2273055949789935\n",
      "0.76408947\n",
      "0.1919082034889641\n",
      "0.76602036\n",
      "0.21296515663216908\n",
      "0.7685251\n",
      "0.2402803701508276\n",
      "0.76426613\n",
      "0.19383482207567404\n",
      "0.7642672\n",
      "0.19384652218854903\n",
      "0.7656379\n",
      "0.20879406639231846\n",
      "0.76457405\n",
      "0.19719275447072881\n",
      "0.76615083\n",
      "0.21438802035899407\n",
      "0.7667057\n",
      "0.22043892873405468\n",
      "0.76830864\n",
      "0.23791954737520982\n",
      "0.7695234\n",
      "0.25116667517451496\n",
      "0.76282513\n",
      "0.17812027047233592\n",
      "0.77007884\n",
      "0.2572240836122841\n",
      "0.7651366\n",
      "0.20332751365472568\n",
      "0.7640762\n",
      "0.19176325209057055\n",
      "0.76860195\n",
      "0.2411182282339137\n",
      "0.7630854\n",
      "0.18095884785700633\n",
      "0.76655155\n",
      "0.2187580125177142\n",
      "0.7715906\n",
      "0.27371019265908636\n",
      "0.7644302\n",
      "0.19562428933924103\n",
      "0.7690649\n",
      "0.24616682693937086\n",
      "0.7604423\n",
      "0.15213496978377528\n",
      "0.768127\n",
      "0.23593897826802213\n",
      "0.76792866\n",
      "0.23377575739873357\n",
      "0.7620828\n",
      "0.1700250923755462\n",
      "0.7677232\n",
      "0.23153518578321908\n",
      "0.76895046\n",
      "0.24491881489939615\n",
      "0.7693219\n",
      "0.24896965397914528\n",
      "0.7695793\n",
      "0.2517763810565441\n",
      "0.7604353\n",
      "0.15205826904381858\n",
      "0.7684896\n",
      "0.23989296641341795\n",
      "0.765996\n",
      "0.21269930406740478\n",
      "0.76647\n",
      "0.2178688039392327\n",
      "0.76736194\n",
      "0.2275954977757806\n",
      "0.765935\n",
      "0.21203434765235585\n",
      "0.7721471\n",
      "0.2797793012097305\n",
      "0.7667807\n",
      "0.22125663662274597\n",
      "0.767034\n",
      "0.22401916327372895\n",
      "0.7670013\n",
      "0.22366230983104884\n",
      "0.765689\n",
      "0.20935177177268294\n",
      "0.7653829\n",
      "0.20601333956575196\n",
      "0.77129567\n",
      "0.27049396163107\n",
      "0.76710755\n",
      "0.22482127101192084\n",
      "0.7724607\n",
      "0.2831989842005118\n",
      "0.765212\n",
      "0.2041497715873124\n",
      "0.7669825\n",
      "0.22345755785574184\n",
      "0.7712592\n",
      "0.2700961577933274\n",
      "0.7700081\n",
      "0.25645252616882175\n",
      "0.7696795\n",
      "0.25286904159779233\n",
      "0.7667566\n",
      "0.22099403408933505\n",
      "0.77004004\n",
      "0.256800929529982\n",
      "0.76543355\n",
      "0.2065658448959482\n",
      "0.7640698\n",
      "0.1916937014195934\n",
      "0.7680878\n",
      "0.23551127414182282\n",
      "0.76473343\n",
      "0.1989308712389004\n",
      "0.76498204\n",
      "0.20164204739448977\n",
      "0.7614409\n",
      "0.1630245248388178\n",
      "0.76753217\n",
      "0.2294519156852406\n",
      "0.7653314\n",
      "0.20545173414776485\n",
      "0.76852846\n",
      "0.24031677050199285\n",
      "0.76671845\n",
      "0.22057803007601073\n",
      "0.7649709\n",
      "0.2015204962218462\n",
      "0.76905\n",
      "0.24600432537166483\n",
      "0.76940155\n",
      "0.24983806235696093\n",
      "0.7672094\n",
      "0.2259321317287526\n",
      "0.76929027\n",
      "0.24862450064934016\n",
      "0.7674339\n",
      "0.22838005534466\n",
      "0.760994\n",
      "0.15815142782648195\n",
      "0.76608425\n",
      "0.21366196335448784\n",
      "0.76667786\n",
      "0.2201353758055813\n",
      "0.7666808\n",
      "0.2201672261128511\n",
      "0.7706746\n",
      "0.26372089628912754\n",
      "0.7683654\n",
      "0.23853835334502982\n",
      "0.76781166\n",
      "0.23249979508911522\n",
      "0.7669872\n",
      "0.22350890835113546\n",
      "0.7658316\n",
      "0.21090658677248264\n",
      "0.76856995\n",
      "0.2407691748664842\n",
      "0.7639887\n",
      "0.19080904288500733\n",
      "0.77017677\n",
      "0.2582920439152421\n",
      "0.77273226\n",
      "0.28616041277036786\n",
      "0.76713026\n",
      "0.2250689234011034\n",
      "0.76537454\n",
      "0.20592233868783794\n",
      "0.7659086\n",
      "0.21174639487438185\n",
      "0.7721914\n",
      "0.2802622558689496\n",
      "0.76991963\n",
      "0.2554879168629256\n",
      "0.76511675\n",
      "0.2031110615665419\n",
      "0.76457614\n",
      "0.19721550469020777\n",
      "0.7664989\n",
      "0.21818405698058108\n",
      "0.76983297\n",
      "0.25454280774515325\n",
      "0.7704865\n",
      "0.26166947649842065\n",
      "0.7652724\n",
      "0.2048082279396528\n",
      "0.7669976\n",
      "0.22362200944225918\n",
      "0.7651633\n",
      "0.20361871646405305\n",
      "0.7652205\n",
      "0.20424272248403952\n",
      "0.7675357\n",
      "0.22949026605521894\n",
      "0.7676427\n",
      "0.23065702731134152\n",
      "0.7649753\n",
      "0.20156859668588645\n",
      "0.76867443\n",
      "0.24190863585923061\n",
      "0.76593846\n",
      "0.21207204801606316\n",
      "0.7639082\n",
      "0.1899315344194008\n",
      "0.76596266\n",
      "0.21233595056201615\n",
      "0.7645222\n",
      "0.19662724901511552\n",
      "0.76708734\n",
      "0.22460091888611267\n",
      "0.76956767\n",
      "0.25164962983373407\n",
      "0.7612453\n",
      "0.16089185426425878\n",
      "0.76650393\n",
      "0.21823865750732985\n",
      "0.7687389\n",
      "0.2426119426442579\n",
      "0.76245785\n",
      "0.1741149318315447\n",
      "0.7623437\n",
      "0.17287016982292513\n",
      "0.77119833\n",
      "0.26943250139082053\n",
      "0.7677468\n",
      "0.23179258826646354\n",
      "0.77078646\n",
      "0.26494095805945683\n",
      "0.76658374\n",
      "0.21910901590395682\n",
      "0.7666152\n",
      "0.21945221921495062\n",
      "0.77037805\n",
      "0.26048711509180045\n",
      "0.76345164\n",
      "0.18495248638492257\n",
      "0.7638699\n",
      "0.1895135803872634\n",
      "0.76951176\n",
      "0.2510399239517067\n",
      "0.7620587\n",
      "0.16976183983586424\n",
      "0.7685257\n",
      "0.24028687021353434\n",
      "0.7660509\n",
      "0.21329795984282818\n",
      "0.76715016\n",
      "0.22528602549555643\n",
      "0.77195203\n",
      "0.27765183068533617\n",
      "0.7630189\n",
      "0.18023344085877113\n",
      "0.76924926\n",
      "0.24817729633501528\n",
      "0.76699615\n",
      "0.2236064092917598\n",
      "0.76493454\n",
      "0.2011239923966457\n",
      "0.7623167\n",
      "0.1725757169822444\n",
      "0.76526845\n",
      "0.20476532752577903\n",
      "0.76436156\n",
      "0.19487548211525585\n",
      "0.76801807\n",
      "0.23475076680496265\n",
      "0.7700074\n",
      "0.25644472609357116\n",
      "0.7610155\n",
      "0.1583854300839782\n",
      "0.7650793\n",
      "0.20270285762846818\n",
      "0.767923\n",
      "0.23371400680300525\n",
      "0.76796097\n",
      "0.23412806079751824\n",
      "0.76186395\n",
      "0.1676382693490961\n",
      "0.76798975\n",
      "0.23444201382632457\n",
      "0.76428425\n",
      "0.19403242398200327\n",
      "0.7706213\n",
      "0.26313979068301485\n",
      "0.7627193\n",
      "0.17696585933535935\n",
      "0.76521546\n",
      "0.20418747195101972\n",
      "0.7657569\n",
      "0.2100921289151465\n",
      "0.7654844\n",
      "0.20712030024495753\n",
      "0.76393443\n",
      "0.1902175371785617\n",
      "0.76999325\n",
      "0.2562906746073885\n",
      "0.76680285\n",
      "0.22149843895549104\n",
      "0.76653117\n",
      "0.21853571037309472\n",
      "0.76807827\n",
      "0.23540727313849175\n",
      "0.7648855\n",
      "0.20058903723576194\n",
      "0.77080566\n",
      "0.26515026007866105\n",
      "0.7675332\n",
      "0.22946296579184455\n",
      "0.77134585\n",
      "0.27104126691109975\n",
      "0.7649981\n",
      "0.20181689908134004\n",
      "0.7695071\n",
      "0.25098922346258234\n",
      "0.76870763\n",
      "0.24227068935207718\n",
      "0.7672027\n",
      "0.22585868102014928\n",
      "0.77033573\n",
      "0.26002561063951823\n",
      "0.76570547\n",
      "0.20953117350342865\n",
      "0.76774615\n",
      "0.23178543819748398\n",
      "0.7693252\n",
      "0.2490054043240395\n",
      "0.76690024\n",
      "0.22256054920200974\n",
      "0.7701774\n",
      "0.25829919398422163\n",
      "0.7665718\n",
      "0.2189790146497934\n",
      "0.7644667\n",
      "0.19602209317698183\n",
      "0.76642025\n",
      "0.21732604870309835\n",
      "0.76616925\n",
      "0.21458887229667667\n",
      "0.7637244\n",
      "0.18792691508019388\n",
      "0.7678988\n",
      "0.23345010425705404\n",
      "0.7656854\n",
      "0.20931212139016253\n",
      "0.76462436\n",
      "0.19774135976330065\n",
      "0.7697327\n",
      "0.253449497197634\n",
      "0.76291233\n",
      "0.17907122964654576\n",
      "0.76247203\n",
      "0.17426963332400014\n",
      "0.7656862\n",
      "0.2093212214779534\n",
      "0.76803434\n",
      "0.23492821851689705\n",
      "0.7722098\n",
      "0.2804631078066322\n",
      "0.7682199\n",
      "0.23695168803795852\n",
      "0.7673632\n",
      "0.2276091479074669\n",
      "0.7725816\n",
      "0.2845171969177347\n",
      "0.7672832\n",
      "0.22673683949202683\n",
      "0.76930785\n",
      "0.2488162524992319\n",
      "0.7726621\n",
      "0.28539535538961225\n",
      "0.76722354\n",
      "0.226086183214937\n",
      "0.7690131\n",
      "0.2456019714900286\n",
      "0.7663911\n",
      "0.21700819563666762\n",
      "0.76611257\n",
      "0.2139707163331277\n",
      "0.76683205\n",
      "0.2218169420281928\n",
      "0.76787966\n",
      "0.23324145224412085\n",
      "0.76750094\n",
      "0.2291113123993309\n",
      "0.7700511\n",
      "0.25692183069635455\n",
      "0.7683528\n",
      "0.23840120202188686\n",
      "0.76756394\n",
      "0.22979836902758777\n",
      "0.767492\n",
      "0.22901381145870836\n",
      "0.76364887\n",
      "0.1871033571350651\n",
      "0.7701993\n",
      "0.25853774628561155\n",
      "0.77086353\n",
      "0.26578141616762707\n",
      "0.76904905\n",
      "0.2459939252713319\n",
      "0.7645575\n",
      "0.19701205272744104\n",
      "0.7647113\n",
      "0.19868971891242637\n",
      "0.76688635\n",
      "0.22240909774090944\n",
      "0.7644936\n",
      "0.1963152460051223\n",
      "0.7669702\n",
      "0.22332365656395226\n",
      "0.76702785\n",
      "0.22395221262783593\n",
      "0.7690223\n",
      "0.2457020724557335\n",
      "0.7690911\n",
      "0.24645282969853177\n",
      "0.7660082\n",
      "0.21283255535292156\n",
      "0.76420194\n",
      "0.19313476532200013\n",
      "0.7681018\n",
      "0.23566402561546518\n",
      "0.7666107\n",
      "0.2194028187383683\n",
      "0.76731384\n",
      "0.22707094271522799\n",
      "0.7682345\n",
      "0.2371109395743094\n",
      "0.770289\n",
      "0.2595160057231958\n",
      "0.76473844\n",
      "0.19898547176564918\n",
      "0.7665413\n",
      "0.21864621143913432\n",
      "0.7684454\n",
      "0.23941066176046988\n",
      "0.761934\n",
      "0.16840202671730964\n",
      "0.7677909\n",
      "0.23227359290686955\n",
      "0.7655352\n",
      "0.20767410558769583\n",
      "0.7683376\n",
      "0.23823545042282745\n",
      "0.7689599\n",
      "0.24502151589018517\n",
      "0.76777166\n",
      "0.2320636408813943\n",
      "0.7679369\n",
      "0.23386545826410732\n",
      "0.7656733\n",
      "0.20918017011718604\n",
      "0.7627128\n",
      "0.17689500865184016\n",
      "0.7684056\n",
      "0.23897710757783308\n",
      "0.767339\n",
      "0.2273452453615139\n",
      "0.7683257\n",
      "0.23810544916866405\n",
      "0.76385003\n",
      "0.1892971282990814\n",
      "0.763522\n",
      "0.18572014379076052\n",
      "0.76976043\n",
      "0.2537517501135653\n",
      "0.7641102\n",
      "0.19213440567120976\n",
      "0.76471424\n",
      "0.1987215692196962\n",
      "0.7697525\n",
      "0.25366529927954673\n",
      "0.76837987\n",
      "0.23869630486883864\n",
      "0.7666119\n",
      "0.21941581886378358\n",
      "0.7645643\n",
      "0.19708615344231362\n",
      "0.7660851\n",
      "0.2136710634422787\n",
      "0.76512635\n",
      "0.20321571257614401\n",
      "0.7709019\n",
      "0.2662000202060337\n",
      "0.7649738\n",
      "0.20155234652911602\n",
      "0.76767695\n",
      "0.2310307809170613\n",
      "0.7707774\n",
      "0.2648421571062922\n",
      "0.7675579\n",
      "0.22973271839423504\n",
      "0.7699368\n",
      "0.2556751186689219\n",
      "0.7674336\n",
      "0.22837680531330484\n",
      "0.76923645\n",
      "0.24803754498678998\n",
      "0.7657006\n",
      "0.20947787298922194\n",
      "0.7714731\n",
      "0.27242903029929977\n",
      "0.76762295\n",
      "0.2304418752356998\n",
      "0.7664417\n",
      "0.2175600509605946\n",
      "0.76734906\n",
      "0.22745509642128248\n",
      "0.7670531\n",
      "0.22422781528666214\n",
      "0.7685479\n",
      "0.2405286725462794\n",
      "0.76474375\n",
      "0.1990433223237531\n",
      "0.76841617\n",
      "0.23909215868776812\n",
      "0.76550555\n",
      "0.20735105247109864\n",
      "0.76716566\n",
      "0.22545502712597099\n",
      "0.76922697\n",
      "0.24793419398972993\n",
      "0.7642487\n",
      "0.19364502024459362\n",
      "0.7645446\n",
      "0.19687165137294294\n",
      "0.766829\n",
      "0.22178379170838092\n",
      "0.7631365\n",
      "0.181515903231098\n",
      "0.7680265\n",
      "0.23484241768914949\n",
      "0.7631022\n",
      "0.18114149961910542\n",
      "0.7670413\n",
      "0.2240991140450408\n",
      "0.76516\n",
      "0.20358296611915705\n",
      "0.7686119\n",
      "0.24122677928114022\n",
      "0.7663806\n",
      "0.21689379453300361\n",
      "0.7669865\n",
      "0.22350110827588665\n",
      "0.76356095\n",
      "0.18614459788560467\n",
      "0.76526415\n",
      "0.20471852707427907\n",
      "0.76945937\n",
      "0.2504685684396559\n",
      "0.76873326\n",
      "0.24255019204852957\n",
      "0.77018\n",
      "0.25832714425386527\n",
      "0.76686054\n",
      "0.22212764502564397\n",
      "0.76951057\n",
      "0.25102692382628966\n",
      "0.7676348\n",
      "0.23057122648359218\n",
      "0.7665488\n",
      "0.21872811222925748\n",
      "0.76700824\n",
      "0.22373836056473628\n",
      "0.7650132\n",
      "0.20198200067412841\n",
      "0.767321\n",
      "0.22714894346772674\n",
      "0.76889306\n",
      "0.2442928588605966\n",
      "0.7677886\n",
      "0.23224824266230826\n",
      "0.76761645\n",
      "0.23037102455218061\n",
      "0.7729351\n",
      "0.2883723841099659\n",
      "0.766832\n",
      "0.22181629202192177\n",
      "0.76724494\n",
      "0.22631953546616046\n",
      "0.76434135\n",
      "0.19465512998944767\n",
      "0.7650042\n",
      "0.20188384972723483\n",
      "0.7651054\n",
      "0.20298756037508703\n",
      "0.76770926\n",
      "0.23138308431584598\n",
      "0.7646266\n",
      "0.19776606000159092\n",
      "0.7682769\n",
      "0.23757309403286264\n",
      "0.7692149\n",
      "0.24780289272302447\n",
      "0.76487166\n",
      "0.2004382357809309\n",
      "0.76551425\n",
      "0.20744595338663885\n",
      "0.76714027\n",
      "0.22517812445460095\n",
      "0.75874656\n",
      "0.13364229137895478\n",
      "0.76390487\n",
      "0.18989513406823555\n",
      "0.768742\n",
      "0.2426457429703408\n",
      "0.76840526\n",
      "0.23897320754020868\n",
      "0.76787513\n",
      "0.23319205176753854\n",
      "0.7651682\n",
      "0.20367201697825976\n",
      "0.7680128\n",
      "0.23469356625313154\n",
      "0.76114595\n",
      "0.1598082938108032\n",
      "0.76711315\n",
      "0.22488237160137814\n",
      "0.76554066\n",
      "0.20773390616461285\n",
      "0.7666573\n",
      "0.2199111236421487\n",
      "0.76478785\n",
      "0.19952432696415912\n",
      "0.76862323\n",
      "0.24135028047259688\n",
      "0.7704735\n",
      "0.26152777513138226\n",
      "0.7653349\n",
      "0.2054900845177432\n",
      "0.768422\n",
      "0.23915585930230954\n",
      "0.76179546\n",
      "0.16689141214392578\n",
      "0.7661035\n",
      "0.21387191537996308\n",
      "0.76942027\n",
      "0.2500421643259969\n",
      "0.76667386\n",
      "0.22009182538543648\n",
      "0.76618564\n",
      "0.21476762402115313\n",
      "0.7646663\n",
      "0.19819896417795668\n",
      "0.7661522\n",
      "0.21440297050322243\n",
      "0.7702443\n",
      "0.25902850102008124\n",
      "0.77194107\n",
      "0.2775322295315057\n",
      "0.76465213\n",
      "0.19804426268550301\n",
      "0.76541716\n",
      "0.20638709317147352\n",
      "0.76813143\n",
      "0.23598707873206237\n",
      "0.7672822\n",
      "0.22672578938542287\n",
      "0.7699036\n",
      "0.2553130651760753\n",
      "0.7657141\n",
      "0.20962542441269783\n",
      "0.771432\n",
      "0.2719805259724346\n",
      "0.7666776\n",
      "0.22013277578049895\n",
      "0.7660167\n",
      "0.21292550624964868\n",
      "0.7632605\n",
      "0.1828679162744038\n",
      "0.7647896\n",
      "0.19954317714601189\n",
      "0.76461864\n",
      "0.1976789591613013\n",
      "0.7711766\n",
      "0.2691952491019727\n",
      "0.765566\n",
      "0.20801015882971008\n",
      "0.76572365\n",
      "0.2097294254160289\n",
      "0.7667772\n",
      "0.22121893625903866\n",
      "0.7702585\n",
      "0.2591832025125349\n",
      "0.7717266\n",
      "0.27519350696909584\n",
      "0.77218777\n",
      "0.2802226054864292\n",
      "0.7671922\n",
      "0.22574427991648527\n",
      "0.7628485\n",
      "0.17837507293049804\n",
      "0.7657774\n",
      "0.21031573107230805\n",
      "0.772417\n",
      "0.28272252960400124\n",
      "0.76553905\n",
      "0.20771635599530036\n",
      "0.7651925\n",
      "0.2039372195367548\n",
      "0.7687734\n",
      "0.24298764626879077\n",
      "0.76597744\n",
      "0.21249715211717835\n",
      "0.7684685\n",
      "0.23966286419354788\n",
      "0.76983726\n",
      "0.2545896081966532\n",
      "0.7668233\n",
      "0.22172139110638156\n",
      "0.7664199\n",
      "0.21732214866547395\n",
      "0.76616037\n",
      "0.21449202136232515\n",
      "0.769456\n",
      "0.2504321680884889\n",
      "0.76762867\n",
      "0.2305042758376974\n",
      "0.77018905\n",
      "0.2584259452070299\n",
      "0.76971424\n",
      "0.25324799525368036\n",
      "0.76992613\n",
      "0.2555587675464448\n",
      "0.76776606\n",
      "0.232002540291937\n",
      "0.7686072\n",
      "0.24117542878574483\n",
      "0.7697936\n",
      "0.2541138036064119\n",
      "0.77059853\n",
      "0.26289148828756304\n",
      "0.7661827\n",
      "0.21473577371388153\n",
      "0.76989836\n",
      "0.25525586462424243\n",
      "0.76784045\n",
      "0.23281374811791977\n",
      "0.76965255\n",
      "0.2525752387633826\n",
      "0.7727455\n",
      "0.2863047141624886\n",
      "0.7676065\n",
      "0.23026247350495233\n",
      "0.7648481\n",
      "0.20018148330395746\n",
      "0.7683148\n",
      "0.2379864980211046\n",
      "0.7701977\n",
      "0.25852019611629906\n",
      "0.768413\n",
      "0.23905770835541418\n",
      "0.768749\n",
      "0.24272179370402647\n",
      "0.7650012\n",
      "0.20185069940742295\n",
      "0.76844245\n",
      "0.23937881145320006\n",
      "0.76686156\n",
      "0.22213869513224793\n",
      "0.76776487\n",
      "0.23198954016652173\n",
      "0.7698387\n",
      "0.2546052083471526\n",
      "0.7627759\n",
      "0.17758336529263907\n",
      "0.7670468\n",
      "0.22415891462195603\n",
      "0.7687313\n",
      "0.24252874184159268\n",
      "0.7676078\n",
      "0.23027677364291144\n",
      "0.765294\n",
      "0.20504418021596038\n",
      "0.76703334\n",
      "0.22401201320475117\n",
      "0.7701382\n",
      "0.25787148985802055\n",
      "0.7655868\n",
      "0.20823701101822678\n",
      "0.76921076\n",
      "0.24775739228406657\n",
      "0.77143586\n",
      "0.27202277638003736\n",
      "0.76780015\n",
      "0.23237434387884726\n",
      "0.7624721\n",
      "0.17427028333027117\n",
      "0.7687706\n",
      "0.242957095974063\n",
      "0.76904595\n",
      "0.245960124945249\n",
      "0.7667037\n",
      "0.2204174785271178\n",
      "0.7680243\n",
      "0.2348190174633995\n",
      "0.76610506\n",
      "0.21388881554300454\n",
      "0.76945144\n",
      "0.25038211760563556\n",
      "0.76685685\n",
      "0.22208734463685254\n",
      "0.7633447\n",
      "0.183786375135071\n",
      "0.7618702\n",
      "0.16770652000753294\n",
      "0.7678806\n",
      "0.23325185234445378\n",
      "0.7682784\n",
      "0.2375899941959041\n",
      "0.76779395\n",
      "0.23230674322668143\n",
      "0.768422\n",
      "0.23915585930230954\n",
      "0.7700933\n",
      "0.2573820351360947\n",
      "0.76384526\n",
      "0.18924512779741498\n",
      "0.76713675\n",
      "0.2251397740846226\n",
      "0.7695322\n",
      "0.25126287610259723\n",
      "0.76262665\n",
      "0.1759557495905053\n",
      "0.76622254\n",
      "0.21516997790278936\n",
      "0.7676613\n",
      "0.2308598292678372\n",
      "0.7639486\n",
      "0.19037223867101716\n",
      "0.77202547\n",
      "0.278452638410986\n",
      "0.7682424\n",
      "0.23719739040832977\n",
      "0.7667223\n",
      "0.22062028048361348\n",
      "0.7663706\n",
      "0.21678459347950607\n",
      "0.76526034\n",
      "0.20467692667294735\n",
      "0.77118886\n",
      "0.2693291503937605\n",
      "0.7642712\n",
      "0.19389007260869384\n",
      "0.7698472\n",
      "0.25469815924387973\n",
      "0.7699522\n",
      "0.2558428202867926\n",
      "0.7651534\n",
      "0.2035108154230958\n",
      "0.771282\n",
      "0.27034511019505203\n",
      "0.77018946\n",
      "0.2584304952509271\n",
      "0.76654804\n",
      "0.21871966214773586\n",
      "0.76486266\n",
      "0.2003400848340373\n",
      "0.7649301\n",
      "0.20107589193260544\n",
      "0.76955026\n",
      "0.25145982800265543\n",
      "0.7682378\n",
      "0.23714733992547643\n",
      "0.76554656\n",
      "0.20779825678542352\n",
      "0.76333034\n",
      "0.18362972362380425\n",
      "0.7682086\n",
      "0.23682883685277467\n",
      "0.7659633\n",
      "0.21234310063099393\n",
      "0.7692067\n",
      "0.24771319185765073\n",
      "0.77271545\n",
      "0.28597711100199597\n",
      "0.76329756\n",
      "0.18327222017485312\n",
      "0.76733565\n",
      "0.22730884501034865\n",
      "0.7674946\n",
      "0.2290424117346248\n",
      "0.76751584\n",
      "0.22927381396703694\n",
      "0.7677159\n",
      "0.23145523501190723\n",
      "0.7699054\n",
      "0.2553325653641991\n",
      "0.76392245\n",
      "0.19008688591812728\n",
      "0.19467564016137248\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAUWUlEQVR4nO3df5BlZX3n8fcn/Fi3ACNkOgjMwBilqCKsINs7YOlmUXEcRgST0gSSIG7QSVJYK1W61ihZMRp32Rg1tSEVagIUqISYRFESBmHisiHu+oOeyYDDDwOhMDPjyDQOAv4oU6Pf/aPP7F6a2z+m7525PTzvV9WtPuc5zz3Pt3umP33uc889J1WFJKkNPzXqAiRJ+4+hL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfmkGS5UkqycHd+m1JLt4P434gyaf29Thqk6GvA16SR5P8MMn3kjyW5Pokhw97nKo6p6pumGc9Zw97fGkYDH09V7yhqg4HTgfGgd/p3Zgp/n9X8/wl0HNKVW0HbgNOSfK/knw4yf8GfgD8XJKfTnJtkh1Jtif5vSQHASQ5KMkfJHk8ySPA63v33e3vbT3rb0/yQJKnk9yf5PQknwSOB/66e+Xxnq7vmUn+T5LvJrknyVk9+3lRkr/r9rMBWLKPf0xqmKGv55Qky4DVwD90TRcBa4AjgG8C1wO7gZcALwNWAnuC/O3AuV37OPCmWcZ5M/AB4C3A84HzgO9U1UXAP9O98qiq309yHHAr8HvAUcC7gc8kGet292fARqbC/kPAPn/fQO0y9PVc8bkk3wW+BPwd8F+79uur6r6q2s1U4K4GLquq71fVTuDjwAVd318G/rCqtlbVLuC/zTLe24Dfr6q7a8rDVfXNGfr+OrC+qtZX1U+qagMwAaxOcjzw74D/UlU/qqq7gL9e8E9BmsPBoy5AGpI3VtXf9jYkAdja03QCcAiwo9sGUwc+e/ocO63/TCEOsAz4p3nWdgLw5iRv6Gk7BLizG/OJqvr+tHGXzXPf0l4x9PVc13sZ2a3Aj4Al3ZH/dDt4ZtgeP8t+twIvnseYe/p+sqrePr1jkhOAI5Mc1hP8x/fZhzQUTu+oGVW1A7gD+GiS5yf5qSQvTvIfui5/AfynJEuTHAmsnWV31wDvTvJvuzODXtIFOMBjwM/19P0U8IYkr+veLH5ekrOSLO2mhCaA301yaJJXAm9A2kcMfbXmLcChwP3AE8BfAcd02/4UuB24B9gEfHamnVTVXwIfZupN2KeBzzH1ngFMvRfwO92ZOu+uqq3A+cD7gEmmjvz/M///9+9XgTOAXcAVwCeG8Y1K/cSbqEhSOzzSl6SGGPqS1BBDX5IaYuhLUkMW5Xn6S5YsqeXLl4+6DEk6YGzcuPHxqhqbq9+iDP3ly5czMTEx6jIk6YCRZLZPkP8/Tu9IUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0JekhswZ+kmWJbmzuwfofUne2bUflWRDkoe6r0fO8PyLuz4PJfE2cJI0QvM50t8NvKuqTgbOBC5NcjJT1xr/YlWdCHyRPtceT3IUU5eKPQNYAVwx0x8HSdK+N2foV9WOqtrULT8NPAAcx9T1wW/out0AvLHP018HbKiqXVX1BLABWDWMwiVJe2+v5vSTLAdeBnwVOLq7ExHAt4Gj+zzlOJ55z9FtXVu/fa9JMpFkYnJycm/KkiTN07xDP8nhwGeAy6rqqd5tNXUnloHuxlJV66pqvKrGx8bmvHyEJGkB5hX6SQ5hKvBvrKo9t5B7LMkx3fZjgJ19nrqdZ95oemnXJkkagfmcvRPgWuCBqvpYz6ZbgD1n41wMfL7P028HViY5snsDd2XXJkkagfkc6b8CuAh4dZLN3WM1cCXw2iQPAWd36yQZT3INQFXtAj4E3N09Pti1SZJGYFHeGH18fLy8tLIkzV+SjVU1Plc/P5ErSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWrIwXN1SHIdcC6ws6pO6do+DZzUdXkB8N2qOq3Pcx8FngZ+DOyezwX+JUn7zpyhD1wPXAV8Yk9DVf3KnuUkHwWenOX5r6qqxxdaoCRpeOYM/aq6K8nyftu6m6b/MvDq4ZYlSdoXBp3T//fAY1X10AzbC7gjycYkawYcS5I0oPlM78zmQuCmWba/sqq2J/lZYEOSB6vqrn4duz8KawCOP/74AcuSJPWz4CP9JAcDvwR8eqY+VbW9+7oTuBlYMUvfdVU1XlXjY2NjCy1LkjSLQaZ3zgYerKpt/TYmOSzJEXuWgZXAlgHGkyQNaM7QT3IT8GXgpCTbklzSbbqAaVM7SY5Nsr5bPRr4UpJ7gK8Bt1bVF4ZXuiRpb83n7J0LZ2h/a5+2bwGru+VHgFMHrE+SNER+IlcasuVrbx11CdKMDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkPmc7vE65LsTLKlp+0DSbYn2dw9Vs/w3FVJvpHk4SRrh1m4JGnvzedI/3pgVZ/2j1fVad1j/fSNSQ4C/hg4BzgZuDDJyYMUK0kazJyhX1V3AbsWsO8VwMNV9UhV/Qvw58D5C9iPJGlIBpnTf0eSe7vpnyP7bD8O2Nqzvq1r6yvJmiQTSSYmJycHKEuSNJOFhv6fAC8GTgN2AB8dtJCqWldV41U1PjY2NujuJEl9LCj0q+qxqvpxVf0E+FOmpnKm2w4s61lf2rVJkkZkQaGf5Jie1V8EtvTpdjdwYpIXJTkUuAC4ZSHjSZKG4+C5OiS5CTgLWJJkG3AFcFaS04ACHgV+s+t7LHBNVa2uqt1J3gHcDhwEXFdV9+2T70KSNC9zhn5VXdin+doZ+n4LWN2zvh541umckqTR8BO5ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1JA5Qz/JdUl2JtnS0/aRJA8muTfJzUleMMNzH03y9SSbk0wMs3BJ0t6bz5H+9cCqaW0bgFOq6qXAPwLvneX5r6qq06pqfGElSpKGZc7Qr6q7gF3T2u6oqt3d6leApfugNknSkA1jTv83gNtm2FbAHUk2Jlkz206SrEkykWRicnJyCGVJkqYbKPSTXA7sBm6cocsrq+p04Bzg0iS/MNO+qmpdVY1X1fjY2NggZUmSZrDg0E/yVuBc4Neqqvr1qart3dedwM3AioWOJ0ka3IJCP8kq4D3AeVX1gxn6HJbkiD3LwEpgS7++kqT9Yz6nbN4EfBk4Kcm2JJcAVwFHABu60zGv7voem2R999SjgS8luQf4GnBrVX1hn3wXkqR5OXiuDlV1YZ/ma2fo+y1gdbf8CHDqQNVJkobKT+RKUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQ+YV+kmuS7IzyZaetqOSbEjyUPf1yBmee3HX56EkFw+rcEnS3pvvkf71wKppbWuBL1bVicAXu/VnSHIUcAVwBrACuGKmPw6SpH1vXqFfVXcBu6Y1nw/c0C3fALyxz1NfB2yoql1V9QSwgWf/8ZAk7SeDzOkfXVU7uuVvA0f36XMcsLVnfVvX9ixJ1iSZSDIxOTk5QFmSpJkM5Y3cqiqgBtzHuqoar6rxsbGxYZQlSZpmkNB/LMkxAN3XnX36bAeW9awv7dokSSMwSOjfAuw5G+di4PN9+twOrExyZPcG7squTZI0AvM9ZfMm4MvASUm2JbkEuBJ4bZKHgLO7dZKMJ7kGoKp2AR8C7u4eH+zaJEkjcPB8OlXVhTNsek2fvhPA23rWrwOuW1B1kqSh8hO5ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfSlIVq+9tZRlyDNytCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IasuDQT3JSks09j6eSXDatz1lJnuzp8/7BS5YkLdS8bpfYT1V9AzgNIMlBwHbg5j5d/76qzl3oOJKk4RnW9M5rgH+qqm8OaX+SpH1gWKF/AXDTDNtenuSeJLcl+fmZdpBkTZKJJBOTk5NDKkuS1Gvg0E9yKHAe8Jd9Nm8CTqiqU4E/Aj43036qal1VjVfV+NjY2KBlSZL6GMaR/jnApqp6bPqGqnqqqr7XLa8HDkmyZAhjSpIWYBihfyEzTO0keWGSdMsruvG+M4QxJUkLsOCzdwCSHAa8FvjNnrbfAqiqq4E3Ab+dZDfwQ+CCqqpBxpQkLdxAoV9V3wd+Zlrb1T3LVwFXDTKGdCBavvZWHr3y9aMuQ3oWP5ErSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDRk49JM8muTrSTYnmeizPUn+R5KHk9yb5PRBx5QkLcxAt0vs8aqqenyGbecAJ3aPM4A/6b5Kkvaz/TG9cz7wiZryFeAFSY7ZD+NKkqYZRugXcEeSjUnW9Nl+HLC1Z31b1/YMSdYkmUgyMTk5OYSyJEnTDSP0X1lVpzM1jXNpkl9YyE6qal1VjVfV+NjY2BDKkiRNN3DoV9X27utO4GZgxbQu24FlPetLuzZJ0n42UOgnOSzJEXuWgZXAlmndbgHe0p3FcybwZFXtGGRcSdLCDHr2ztHAzUn27OvPquoLSX4LoKquBtYDq4GHgR8A/3HAMSVJCzRQ6FfVI8Cpfdqv7lku4NJBxpEkDYefyJWkhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGLDj0kyxLcmeS+5Pcl+SdffqcleTJJJu7x/sHK1eSNIhBbpe4G3hXVW3qbo6+McmGqrp/Wr+/r6pzBxhHkjQkCz7Sr6odVbWpW34aeAA4bliFSZKGbyhz+kmWAy8Dvtpn88uT3JPktiQ/P8s+1iSZSDIxOTk5jLIkSdMMHPpJDgc+A1xWVU9N27wJOKGqTgX+CPjcTPupqnVVNV5V42NjY4OWJUnqY6DQT3IIU4F/Y1V9dvr2qnqqqr7XLa8HDkmyZJAxJUkLN8jZOwGuBR6oqo/N0OeFXT+SrOjG+85Cx5QkDWaQs3deAVwEfD3J5q7tfcDxAFV1NfAm4LeT7AZ+CFxQVTXAmJKkASw49KvqS0Dm6HMVcNVCx5AOZMvX3sqjV75+1GVIz+AnciWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGvjQky9feOuoSpDkZ+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNWTQG6OvSvKNJA8nWdtn+79K8ulu+1eTLB9kPEnSYAa5MfpBwB8D5wAnAxcmOXlat0uAJ6rqJcDHgf++0PEkSYMb5Eh/BfBwVT1SVf8C/Dlw/rQ+5wM3dMt/Bbwmyaz31ZUk7TsLvjE6cBywtWd9G3DGTH2qaneSJ4GfAR6fvrMka4A13eqPkmwZoLb9YQl9vo9FyDqHa6/qzOhe2z4nf54jdCDUedJ8Og0S+kNVVeuAdQBJJqpqfMQlzepAqBGsc9isc7isc3iSTMyn3yDTO9uBZT3rS7u2vn2SHAz8NPCdAcaUJA1gkNC/GzgxyYuSHApcANwyrc8twMXd8puA/1lVNcCYkqQBLHh6p5ujfwdwO3AQcF1V3Zfkg8BEVd0CXAt8MsnDwC6m/jDMx7qF1rUfHQg1gnUOm3UOl3UOz7xqjAfektQOP5ErSQ0x9CWpIYs69JO8K0klWTLqWvpJ8qEk9ybZnOSOJMeOuqZ+knwkyYNdrTcnecGoa+onyZuT3JfkJ0kW1elxc11yZLFIcl2SnYv5cy5JliW5M8n93b/3O0ddUz9Jnpfka0nu6er83VHXNJskByX5hyR/M1u/RRv6SZYBK4F/HnUts/hIVb20qk4D/gZ4/6gLmsEG4JSqeinwj8B7R1zPTLYAvwTcNepCes3zkiOLxfXAqlEXMYfdwLuq6mTgTODSRfrz/BHw6qo6FTgNWJXkzBHXNJt3Ag/M1WnRhj5T1+p5D7Bo32muqqd6Vg9jkdZaVXdU1e5u9StMfaZi0amqB6rqG6Ouo4/5XHJkUaiqu5g6U27RqqodVbWpW36aqaA6brRVPVtN+V63ekj3WJS/40mWAq8Hrpmr76IM/STnA9ur6p5R1zKXJB9OshX4NRbvkX6v3wBuG3URB5h+lxxZdCF1IOquvPsy4KujraS/bspkM7AT2FBVi7JO4A+ZOkj+yVwdR3YZhiR/C7ywz6bLgfcxNbUzcrPVWVWfr6rLgcuTvBd4B3DFfi2wM1edXZ/LmXppfeP+rK3XfOpUG5IcDnwGuGzaq+ZFo6p+DJzWvQ92c5JTqmpRvV+S5FxgZ1VtTHLWXP1HFvpVdXa/9iT/BngRcE93Qc6lwKYkK6rq2/uxRGDmOvu4EVjPiEJ/rjqTvBU4F3jNKD8VvRc/z8VkPpcc0V5IcghTgX9jVX121PXMpaq+m+ROpt4vWVShD7wCOC/JauB5wPOTfKqqfr1f50U3vVNVX6+qn62q5VW1nKmX0qePIvDnkuTEntXzgQdHVctskqxi6qXfeVX1g1HXcwCazyVHNE/d5dWvBR6oqo+Nup6ZJBnbc6Zbkn8NvJZF+DteVe+tqqVdXl7A1OVu+gY+LMLQP8BcmWRLknuZmo5alKeeAVcBRwAbutNLrx51Qf0k+cUk24CXA7cmuX3UNcHUJUeYmrq7nak3Hf+iqu4bbVX9JbkJ+DJwUpJtSS4ZdU19vAK4CHh19/9xc3eUutgcA9zZ/X7fzdSc/qynQx4IvAyDJDXEI31JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhryfwHD3dXiAER7HwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluation on test set\n",
    "outputs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    prediction = []\n",
    "    true_score = []\n",
    "    \n",
    "    for i in range(len(X_val)):\n",
    "        prediction.append((all_logits[i]*(max_t-min_t))+min_t)\n",
    "        true_score.append(val_scores[i].item())\n",
    "\n",
    "from scipy.stats.stats import pearsonr\n",
    "pearson = pearsonr(true_score, prediction)\n",
    "print(pearson[0])\n",
    "\n",
    "plt.figure()\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-4,4])\n",
    "plt.hist(prediction, bins=200)\n",
    "plt.title(\"Predicted\")\n",
    "\n",
    "plt.figure()\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-4,4])\n",
    "plt.hist(true_score, bins=200)\n",
    "plt.title(\"Actual\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8HHY2juogt5o"
   },
   "source": [
    "### Prediction of unknow dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X18aIv-2a7E4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mt: 1000 Training src: 1000\n"
     ]
    }
   ],
   "source": [
    "#import set\n",
    "de_testf_src = get_embeddings(\"./test.ende.src\",nlp_en,'en')\n",
    "de_testf_mt = get_embeddings(\"./test.ende.mt\",nlp_de,'de')\n",
    "print(f\"Training mt: {len(de_testf_mt)} Training src: {len(de_testf_src)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NF5RfSc_b9YM"
   },
   "outputs": [],
   "source": [
    "#preprocess set\n",
    "X_testf_en = de_testf_src\n",
    "X_testf_de = de_testf_mt\n",
    "X_testf = []\n",
    "\n",
    "for i in range(len(X_testf_en)):\n",
    "    X_testf.append(X_testf_en[i]+X_testf_de[i])\n",
    "\n",
    "X_testf[0]\n",
    "    \n",
    "testf_ids = pad_sequences(X_testf, maxlen=max_train, dtype=\"long\", \n",
    "                          value=0, truncating=\"post\", padding=\"post\")\n",
    "# Scores\n",
    "testf_inputs = torch.tensor(testf_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0P80m9LJcnsH"
   },
   "outputs": [],
   "source": [
    "#create corresponding mask\n",
    "testf_masks = []\n",
    "\n",
    "# For each sentence...\n",
    "for sent in testf_ids:\n",
    "    \n",
    "    # Create the attention mask.\n",
    "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
    "    #   - If a token ID is > 0, then it's a real token, set the mask to 1\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]\n",
    "                \n",
    "    \n",
    "    # Store the attention mask for this sentence.\n",
    "    testf_masks.append(att_mask)\n",
    "testf_masks = torch.tensor(testf_masks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uN_0yDLGdCHA"
   },
   "outputs": [],
   "source": [
    "# Create batch\n",
    "testf_data = TensorDataset(testf_inputs, testf_masks, validation_labels)\n",
    "testf_sampler = SequentialSampler(testf_data)\n",
    "testf_dataloader = DataLoader(testf_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kfTfMfpuliQs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One\n",
      "One\n",
      "One\n",
      "One\n",
      "One\n",
      "One\n",
      "One\n",
      "One\n",
      "One\n",
      "One\n",
      "One\n",
      "One\n",
      "One\n",
      "One\n",
      "One\n",
      "One\n",
      "One\n",
      "One\n",
      "One\n",
      "One\n",
      "[0.7709172, 0.7688301, 0.76549625, 0.76826584, 0.7640199, 0.7680972, 0.76828545, 0.7674191, 0.76349896, 0.7677786, 0.7666277, 0.76734495, 0.7673382, 0.7659352, 0.76668626, 0.76307607, 0.76500905, 0.76411897, 0.7630839, 0.76648223, 0.76448053, 0.75721085, 0.7663126, 0.7676438, 0.7656683, 0.7662965, 0.76507336, 0.7653925, 0.7694168, 0.76666844, 0.76423883, 0.7664289, 0.7670185, 0.767317, 0.76740146, 0.7674211, 0.76852185, 0.76467544, 0.7686099, 0.76764905, 0.76693994, 0.76742584, 0.77140784, 0.77029145, 0.76643294, 0.7622806, 0.76577353, 0.7662801, 0.76609194, 0.7656719, 0.76574486, 0.7686186, 0.76894236, 0.77120507, 0.76682204, 0.77195406, 0.77188724, 0.7644618, 0.77236205, 0.7678551, 0.76808375, 0.76542, 0.7629479, 0.7638355, 0.7640579, 0.76932, 0.77012765, 0.76303893, 0.76694393, 0.7674215, 0.7693637, 0.7644453, 0.7683847, 0.7670948, 0.76917464, 0.77048165, 0.7643614, 0.7678806, 0.76814, 0.7665214, 0.76723325, 0.7675159, 0.76851946, 0.7672031, 0.76594085, 0.7646808, 0.76780593, 0.76828814, 0.76995873, 0.7669619, 0.76850665, 0.76728857, 0.76887995, 0.76438564, 0.76862353, 0.7632841, 0.77001643, 0.7708762, 0.7684547, 0.7675997, 0.75916517, 0.76239777, 0.7701436, 0.7688197, 0.76782763, 0.7639788, 0.76997405, 0.76834697, 0.7682727, 0.7670887, 0.76501113, 0.762277, 0.76702553, 0.76737297, 0.7685918, 0.7667332, 0.76751363, 0.76371115, 0.763699, 0.76242316, 0.76204497, 0.7629365, 0.7638104, 0.762406, 0.7676444, 0.7666874, 0.770604, 0.7645367, 0.7654259, 0.76852584, 0.771653, 0.7682476, 0.7628316, 0.7615333, 0.7671908, 0.7683849, 0.7693314, 0.7644859, 0.76795137, 0.7677754, 0.7653229, 0.7687109, 0.76606923, 0.7680879, 0.7671759, 0.7652138, 0.76737297, 0.76928765, 0.76750803, 0.76477915, 0.7670285, 0.76448226, 0.7686245, 0.7651053, 0.764701, 0.7700327, 0.7728969, 0.7687799, 0.7697605, 0.76392066, 0.7704595, 0.75960153, 0.7644446, 0.7699698, 0.7665916, 0.7658286, 0.77229184, 0.7667456, 0.76665545, 0.7656041, 0.76767665, 0.7651728, 0.7679078, 0.7632401, 0.7701557, 0.7646425, 0.7679098, 0.7658511, 0.76787, 0.767887, 0.7687417, 0.7642138, 0.76654404, 0.76327074, 0.7688258, 0.7628003, 0.7680012, 0.76876324, 0.7671011, 0.76997787, 0.7645654, 0.76195735, 0.7669556, 0.767341, 0.76996833, 0.76852566, 0.76710075, 0.76547277, 0.76746094, 0.7678121, 0.7691787, 0.76149803, 0.77322656, 0.7641783, 0.76607543, 0.76544595, 0.7624481, 0.7662199, 0.768876, 0.76781803, 0.7684269, 0.76395696, 0.76356035, 0.7652373, 0.770163, 0.7652828, 0.7653755, 0.76924056, 0.7663648, 0.76286674, 0.76723695, 0.7687369, 0.7684007, 0.7687329, 0.7658183, 0.7694019, 0.772847, 0.7667487, 0.76795083, 0.7692281, 0.7620098, 0.7700204, 0.76287967, 0.7654816, 0.7664616, 0.7695714, 0.7625365, 0.76666504, 0.77052414, 0.7682315, 0.76760817, 0.7737423, 0.7689187, 0.76505274, 0.76810414, 0.76927227, 0.76887333, 0.7680762, 0.76868373, 0.7605986, 0.76978433, 0.76458776, 0.76597, 0.7673957, 0.7655319, 0.7650832, 0.7628129, 0.76919645, 0.770331, 0.76768947, 0.76712126, 0.76440847, 0.7674835, 0.76376194, 0.7683088, 0.76596767, 0.7646023, 0.7688698, 0.7653706, 0.77040946, 0.7661417, 0.77206683, 0.768475, 0.7704544, 0.7626921, 0.7632771, 0.76687425, 0.7641488, 0.76859295, 0.76930517, 0.7682637, 0.7735504, 0.7663952, 0.76253, 0.767992, 0.7684177, 0.7649762, 0.76993656, 0.76737875, 0.7723773, 0.7694379, 0.76850593, 0.7639242, 0.7679353, 0.76656276, 0.7701083, 0.7636663, 0.7715567, 0.7677629, 0.76400036, 0.7663122, 0.76882684, 0.76897764, 0.7668084, 0.7651718, 0.76306605, 0.7682054, 0.76472443, 0.7682037, 0.76465696, 0.7669885, 0.7656572, 0.7732586, 0.768633, 0.7703652, 0.7649338, 0.767442, 0.76972044, 0.76484936, 0.768348, 0.77147895, 0.77340794, 0.77225506, 0.7662012, 0.7681572, 0.7685564, 0.7685164, 0.7669524, 0.7683118, 0.76243657, 0.7674135, 0.7663322, 0.77268, 0.761545, 0.7648295, 0.7647518, 0.7665912, 0.76738435, 0.7662355, 0.7640732, 0.77080345, 0.7679704, 0.76302093, 0.76784647, 0.7704421, 0.76376504, 0.76692814, 0.7682218, 0.76795816, 0.76756114, 0.76161796, 0.7728012, 0.76726264, 0.76967955, 0.76649696, 0.7700987, 0.7710483, 0.76488817, 0.7652227, 0.76660764, 0.76776385, 0.7721814, 0.7707257, 0.7684178, 0.7677118, 0.76374745, 0.77028036, 0.76957166, 0.76868635, 0.76288784, 0.767433, 0.76493937, 0.76286095, 0.76640475, 0.7721752, 0.7693128, 0.7707491, 0.76778233, 0.76559436, 0.7656609, 0.77307636, 0.76262057, 0.7697833, 0.7669948, 0.76538444, 0.76914644, 0.7703852, 0.76119107, 0.76916534, 0.76870096, 0.76507086, 0.76680124, 0.76825434, 0.7686585, 0.76678395, 0.7670981, 0.7666898, 0.7664462, 0.768082, 0.7689861, 0.7644245, 0.7672411, 0.76594454, 0.76362675, 0.76541704, 0.76747924, 0.76710343, 0.7659938, 0.76857895, 0.7671369, 0.7686473, 0.765001, 0.7649403, 0.76371914, 0.7668469, 0.76441276, 0.7688783, 0.7667751, 0.7674286, 0.7669563, 0.7666044, 0.76863796, 0.76908296, 0.7688223, 0.7703459, 0.7645272, 0.7689847, 0.765047, 0.7682517, 0.7683244, 0.7690614, 0.7673833, 0.7678357, 0.7681839, 0.7628165, 0.7666107, 0.76783043, 0.7715063, 0.76837724, 0.7661481, 0.76536816, 0.76828367, 0.7654154, 0.7689487, 0.76642364, 0.76863015, 0.7656384, 0.7711751, 0.7665166, 0.765977, 0.76440763, 0.7696375, 0.7651064, 0.7636207, 0.76677155, 0.7657334, 0.768181, 0.76646763, 0.767781, 0.76871026, 0.7671108, 0.76954275, 0.7689769, 0.76752216, 0.77180994, 0.76415503, 0.76297194, 0.7708668, 0.7634132, 0.7710465, 0.76947033, 0.76731956, 0.7632442, 0.76093453, 0.76713014, 0.7637046, 0.76148033, 0.76423275, 0.7714393, 0.763029, 0.77020776, 0.7682641, 0.7709594, 0.7663671, 0.7634834, 0.76719517, 0.7697167, 0.7673813, 0.7676797, 0.76471525, 0.7714269, 0.76706266, 0.77349097, 0.7684485, 0.76987606, 0.765642, 0.7640786, 0.7691876, 0.76303434, 0.7626674, 0.7660727, 0.77125114, 0.76838267, 0.7675222, 0.76406336, 0.76881987, 0.76875037, 0.76608825, 0.76941335, 0.764023, 0.7711581, 0.7692579, 0.7712031, 0.7667671, 0.7688343, 0.7691154, 0.76551867, 0.7696613, 0.76705396, 0.7676396, 0.7652538, 0.76518804, 0.76711226, 0.76880443, 0.76948094, 0.76324767, 0.764158, 0.768548, 0.7723975, 0.7678667, 0.76342064, 0.7660555, 0.7654693, 0.7644694, 0.76694435, 0.76228726, 0.764392, 0.7712819, 0.76765406, 0.76950884, 0.7675468, 0.76779413, 0.7691411, 0.7633206, 0.76241463, 0.76870155, 0.7693474, 0.76877964, 0.7656335, 0.7657809, 0.768525, 0.7721492, 0.77056134, 0.7666554, 0.76311165, 0.76638305, 0.76187235, 0.7671145, 0.7665519, 0.7673666, 0.76527196, 0.76837033, 0.76709396, 0.7640736, 0.7624035, 0.7689796, 0.76427084, 0.76975596, 0.770427, 0.7688753, 0.7684431, 0.7647336, 0.7671407, 0.76339436, 0.77045006, 0.77479815, 0.7672156, 0.770221, 0.7704308, 0.7704875, 0.7644869, 0.7658585, 0.7671355, 0.7666303, 0.7696082, 0.77151173, 0.76665926, 0.767567, 0.76845145, 0.76615024, 0.7669865, 0.7643982, 0.7676463, 0.7692729, 0.76873857, 0.7667317, 0.76713884, 0.76931834, 0.7688814, 0.76447386, 0.7680643, 0.76677966, 0.76382667, 0.76728517, 0.76777524, 0.77344525, 0.76879436, 0.7658573, 0.76548845, 0.7706886, 0.7691846, 0.76555425, 0.7656268, 0.7644773, 0.76621294, 0.7670607, 0.77126867, 0.7708199, 0.7718327, 0.7669405, 0.7685887, 0.7689248, 0.7677438, 0.7669838, 0.7679264, 0.7646594, 0.7635994, 0.7672955, 0.76793534, 0.7698601, 0.76631504, 0.7651779, 0.7648581, 0.77098286, 0.767459, 0.7694032, 0.7627144, 0.7690933, 0.7712466, 0.7680106, 0.767776, 0.76736706, 0.76652074, 0.7644616, 0.76818204, 0.770936, 0.76499915, 0.7704132, 0.76620734, 0.7696908, 0.7715109, 0.7677713, 0.76945907, 0.7688883, 0.7649051, 0.76658374, 0.7659683, 0.76448756, 0.7696983, 0.768158, 0.76757014, 0.7667749, 0.7642431, 0.7669462, 0.769586, 0.76794016, 0.7673935, 0.76941377, 0.76417744, 0.7654507, 0.7628738, 0.76542675, 0.77064806, 0.7689131, 0.7635111, 0.76338613, 0.7682726, 0.7700877, 0.76834154, 0.7724048, 0.76375407, 0.7687093, 0.76735806, 0.7710817, 0.7652297, 0.766603, 0.76903737, 0.7701039, 0.7618077, 0.7630597, 0.76420665, 0.7629794, 0.7683796, 0.76867133, 0.76844555, 0.7612282, 0.7705632, 0.764016, 0.76755965, 0.7664197, 0.76954824, 0.76424295, 0.771402, 0.7617383, 0.7646194, 0.7662281, 0.7694644, 0.7711605, 0.764976, 0.76923406, 0.76598203, 0.76819783, 0.7666588, 0.76953894, 0.7691734, 0.76677126, 0.7663864, 0.76914924, 0.7678072, 0.7652749, 0.7673157, 0.76497763, 0.7689482, 0.7703773, 0.76701003, 0.7652265, 0.765659, 0.76558274, 0.7673309, 0.7669039, 0.7664677, 0.76570314, 0.7691494, 0.7649162, 0.7643124, 0.7723506, 0.763774, 0.76843786, 0.7615946, 0.76657164, 0.76465094, 0.7597592, 0.76814365, 0.7654326, 0.7660366, 0.76230603, 0.76647896, 0.7695653, 0.7690309, 0.7692903, 0.76902413, 0.76862633, 0.7664524, 0.7695005, 0.7672291, 0.7662389, 0.76517564, 0.76597536, 0.7647258, 0.7665528, 0.768295, 0.7667167, 0.7659532, 0.7633738, 0.7713557, 0.76288444, 0.760926, 0.768359, 0.76752883, 0.7653876, 0.7663584, 0.7636857, 0.76440793, 0.7730836, 0.7692562, 0.7688945, 0.76806444, 0.76717275, 0.76586336, 0.76735693, 0.7673268, 0.7657266, 0.7670313, 0.76902246, 0.7697951, 0.7650535, 0.76700443, 0.7649594, 0.7688584, 0.7720637, 0.76701355, 0.7664289, 0.76971996, 0.76669294, 0.7665496, 0.76692414, 0.76701677, 0.76832896, 0.7653007, 0.7689398, 0.76627773, 0.7660115, 0.7664638, 0.7709948, 0.76381665, 0.76606506, 0.7725159, 0.7620847, 0.77024096, 0.76742953, 0.76516366, 0.7646042, 0.7646939, 0.7694529, 0.7654964, 0.76348054, 0.76837325, 0.76817334, 0.7662576, 0.7702424, 0.7624043, 0.76568455, 0.7726489, 0.7685348, 0.7644627, 0.76451886, 0.765308, 0.76741874, 0.7707421, 0.7672171, 0.76794463, 0.76703626, 0.76709855, 0.7686651, 0.76496315, 0.770117, 0.7657071, 0.76600057, 0.76860464, 0.7685887, 0.76677746, 0.7691649, 0.76820487, 0.76840013, 0.7636792, 0.7684658, 0.7649136, 0.76838225, 0.7651671, 0.7712911, 0.76702195, 0.7626272, 0.7692679, 0.765484, 0.7682097, 0.7662978, 0.76626045, 0.76440483, 0.7686728, 0.76876915, 0.7673059, 0.76467246, 0.7673559, 0.7665189, 0.7641243, 0.76976657, 0.76897043, 0.7688848, 0.76928324, 0.76838046, 0.76709765, 0.7666575, 0.7693137, 0.76454073, 0.76820695, 0.7683941, 0.76546776, 0.7700288, 0.76921403, 0.7708691, 0.7605004, 0.7677861, 0.76528597, 0.7680707, 0.76825774, 0.76801276, 0.7728075, 0.7708493, 0.76706755, 0.7665573, 0.76564646, 0.76509565, 0.76681894, 0.76823217, 0.77152973, 0.7679404, 0.7688417, 0.7677523, 0.7668048, 0.77024776, 0.76772994, 0.7672743, 0.7684185, 0.76621747, 0.7622553, 0.7700786, 0.76151067, 0.76626104, 0.7620312, 0.7636034, 0.7662994, 0.7683039, 0.7706722, 0.7636097, 0.76386267, 0.76923025, 0.7694246, 0.76976883, 0.7642419, 0.76883626, 0.7670337, 0.76477015, 0.7689623, 0.7684291, 0.7641514, 0.7634987, 0.76345164, 0.7668255, 0.7714562, 0.76990724, 0.7643642, 0.7670923, 0.765639, 0.7654793, 0.76399314, 0.7653112, 0.77118295, 0.76542723, 0.76750517, 0.7677688, 0.76880497, 0.7640264, 0.77013445, 0.7676737, 0.76316744, 0.76893693, 0.76935124, 0.7682813, 0.7699658, 0.76778865, 0.7679955, 0.76810956, 0.7711327, 0.76603967, 0.7624786, 0.77051544, 0.7645448, 0.7653022, 0.7662576, 0.7698297, 0.7684724, 0.76204026, 0.76546854, 0.7643749, 0.7709499, 0.7636025, 0.7644442, 0.7650901, 0.76484936, 0.76981217, 0.76769626, 0.7662556, 0.7659268, 0.7657643, 0.7641855, 0.7676344, 0.76595247, 0.7676326, 0.7678727, 0.77020293, 0.76707107, 0.7645886, 0.7684548, 0.77033865, 0.766327, 0.7693877, 0.7676794, 0.76816756, 0.7664564, 0.76593226, 0.76781625, 0.762725, 0.7684995, 0.7655317, 0.76685494, 0.765464, 0.769676, 0.76582766, 0.7658046, 0.7672793]\n"
     ]
    }
   ],
   "source": [
    "#predict results\n",
    "bert_clf.eval()\n",
    "allf_logits = []\n",
    "with torch.no_grad():\n",
    "    for step_num, batch_data in enumerate(testf_dataloader):\n",
    "        token_ids, masks, labels = tuple(t for t in batch_data)\n",
    "        logits = bert_clf(token_ids, masks)\n",
    "        numpy_logits = logits.cpu().detach().numpy()\n",
    "        allf_logits += list(numpy_logits[:, 0])\n",
    "        print('One')        \n",
    "print(allf_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O9GnO2yHd22n"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Predicted')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#display results\n",
    "with torch.no_grad():\n",
    "    predictionf = []\n",
    "    \n",
    "    for i in range(len(X_val)):\n",
    "        predictionf.append((allf_logits[i]*(max_t-min_t))+min_t)\n",
    "\n",
    "plt.figure()\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-4,4])\n",
    "plt.hist(predictionf, bins=200)\n",
    "plt.title(\"Predicted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GzI7rSa8an6O"
   },
   "outputs": [],
   "source": [
    "# save predictions\n",
    "\n",
    "with open('predictions.txt', 'w') as f:\n",
    "    for item in predictionf:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "bert.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
