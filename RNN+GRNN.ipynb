{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "omEQHVdOS61G"
   },
   "source": [
    "# NLP Coursework\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "from os.path import exists\n",
    "import io\n",
    "import time\n",
    "import random\n",
    "\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import download\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "import sklearn\n",
    "\n",
    "from google.colab import files\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lweXud1Wpemd"
   },
   "source": [
    "## English-German"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yu6s3YOf_C93"
   },
   "source": [
    "### Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 457
    },
    "colab_type": "code",
    "id": "scs7ICZrPFcs",
    "outputId": "4bb9d133-95ab-4d7c-a1fe-289ec607cfe4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-02-28 14:54:01--  https://competitions.codalab.org/my/datasets/download/c748d2c0-d6be-4e36-9f12-ca0e88819c4d\n",
      "Resolving competitions.codalab.org (competitions.codalab.org)... 129.175.22.230\n",
      "Connecting to competitions.codalab.org (competitions.codalab.org)|129.175.22.230|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 FOUND\n",
      "Location: https://newcodalab.lri.fr/prod-private/dataset_data_file/None/104ea/en-de.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=802ee837936fb9601dc76074cb0b638913adb062b372f36cc295626ebb13e8eb&X-Amz-Date=20200228T145402Z&X-Amz-Credential=AZIAIOSAODNN7EX123LE%2F20200228%2Fnewcodalab%2Fs3%2Faws4_request [following]\n",
      "--2020-02-28 14:54:02--  https://newcodalab.lri.fr/prod-private/dataset_data_file/None/104ea/en-de.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=802ee837936fb9601dc76074cb0b638913adb062b372f36cc295626ebb13e8eb&X-Amz-Date=20200228T145402Z&X-Amz-Credential=AZIAIOSAODNN7EX123LE%2F20200228%2Fnewcodalab%2Fs3%2Faws4_request\n",
      "Resolving newcodalab.lri.fr (newcodalab.lri.fr)... 129.175.15.11\n",
      "Connecting to newcodalab.lri.fr (newcodalab.lri.fr)|129.175.15.11|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 864010 (844K) [application/zip]\n",
      "Saving to: ‘ende_data.zip’\n",
      "\n",
      "ende_data.zip       100%[===================>] 843.76K   542KB/s    in 1.6s    \n",
      "\n",
      "2020-02-28 14:54:05 (542 KB/s) - ‘ende_data.zip’ saved [864010/864010]\n",
      "\n",
      "Archive:  ende_data.zip\n",
      "  inflating: dev.ende.mt             \n",
      "  inflating: dev.ende.scores         \n",
      "  inflating: dev.ende.src            \n",
      "  inflating: test.ende.mt            \n",
      "  inflating: test.ende.src           \n",
      "  inflating: train.ende.mt           \n",
      "  inflating: train.ende.scores       \n",
      "  inflating: train.ende.src          \n"
     ]
    }
   ],
   "source": [
    "# Download and unzip the data\n",
    "if not exists('ende_data.zip'):\n",
    "    !wget -O ende_data.zip https://competitions.codalab.org/my/datasets/download/c748d2c0-d6be-4e36-9f12-ca0e88819c4d\n",
    "    !unzip ende_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "id": "jPy_iwHnOSAZ",
    "outputId": "ab0895d6-ef4e-405e-f69c-5626c367a742"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---EN-DE---\n",
      "\n",
      "Source:  José Ortega y Gasset visited Husserl at Freiburg in 1934.\n",
      "\n",
      "Translation:  1934 besuchte José Ortega y Gasset Husserl in Freiburg.\n",
      "\n",
      "Score:  1.1016968715664406\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the files\n",
    "\n",
    "\n",
    "#English-German\n",
    "print(\"---EN-DE---\")\n",
    "print()\n",
    "\n",
    "with open(\"./train.ende.src\", encoding=\"utf8\") as ende_src:\n",
    "  print(\"Source: \", ende_src.readline())\n",
    "with open(\"./train.ende.mt\", encoding=\"utf8\") as ende_mt:\n",
    "  print(\"Translation: \", ende_mt.readline())\n",
    "with open(\"./train.ende.scores\", encoding=\"utf8\") as ende_scores:\n",
    "  print(\"Score: \", ende_scores.readline())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wiFHVnfH_Jpv"
   },
   "source": [
    "### Computing Sentence Embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g05fv5GiSyQ4"
   },
   "source": [
    "For this baseline model, we will simply use pre-trained GloVe embeddings via the Spacy module and compute the vector for each word and take the global mean for each sentence. We will do the same for both source and translation sentences. \n",
    "\n",
    "This is a very simplistic approach so feel free to be more creative and play around with how the sentence embeddings are computed for example ;)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 554
    },
    "colab_type": "code",
    "id": "96bRtBbuZLJe",
    "outputId": "1551658e-dbd0-4259-dc5b-b289d0d1c759"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_md==2.1.0\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.1.0/en_core_web_md-2.1.0.tar.gz (95.4MB)\n",
      "\u001b[K     |████████████████████████████████| 95.4MB 672kB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: en-core-web-md\n",
      "  Building wheel for en-core-web-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for en-core-web-md: filename=en_core_web_md-2.1.0-cp36-none-any.whl size=97126236 sha256=aef923bf337385d8ccae4736967f04076cb124306bf6ecfcb190dac7c5468998\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-87mc3a9v/wheels/c1/2c/5f/fd7f3ec336bf97b0809c86264d2831c5dfb00fc2e239d1bb01\n",
      "Successfully built en-core-web-md\n",
      "Installing collected packages: en-core-web-md\n",
      "Successfully installed en-core-web-md-2.1.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_md')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/usr/local/lib/python3.6/dist-packages/en_core_web_md -->\n",
      "/usr/local/lib/python3.6/dist-packages/spacy/data/en300\n",
      "You can now load the model via spacy.load('en300')\n",
      "Collecting de_core_news_md==2.1.0\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_md-2.1.0/de_core_news_md-2.1.0.tar.gz (220.8MB)\n",
      "\u001b[K     |████████████████████████████████| 220.8MB 628kB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: de-core-news-md\n",
      "  Building wheel for de-core-news-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for de-core-news-md: filename=de_core_news_md-2.1.0-cp36-none-any.whl size=224546880 sha256=7f86c5431cfb667a60ce2e9244500c51eeb4e785d6911aaa5904a4628f847bd1\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-jhs3d47m/wheels/44/34/f1/31d4b0fa32008c09695ccb180865f196ecd9d512c146f99749\n",
      "Successfully built de-core-news-md\n",
      "Installing collected packages: de-core-news-md\n",
      "Successfully installed de-core-news-md-2.1.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('de_core_news_md')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/usr/local/lib/python3.6/dist-packages/de_core_news_md -->\n",
      "/usr/local/lib/python3.6/dist-packages/spacy/data/de300\n",
      "You can now load the model via spacy.load('de300')\n"
     ]
    }
   ],
   "source": [
    "#Downloading spacy models for english and german\n",
    "\n",
    "!spacy download en_core_web_md\n",
    "!spacy link en_core_web_md en300\n",
    "\n",
    "!spacy download de_core_news_md\n",
    "!spacy link de_core_news_md de300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Om6kQX5bX2mB"
   },
   "source": [
    "We can now write our functions that will return the average embeddings for a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nhT2I6WYavY4"
   },
   "source": [
    "#### Pre-processing with Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "1yebpcR_IaA9",
    "outputId": "c62f96a3-18c8-4100-e799-7dccfd59cfed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "nlp_de = spacy.load('de300')\n",
    "nlp_en = spacy.load('en300')\n",
    "\n",
    "#downloading stopwords from the nltk package\n",
    "download('stopwords') #stopwords dictionary, run once\n",
    "\n",
    "stop_words_en = set(stopwords.words('english'))\n",
    "stop_words_de = set(stopwords.words('german'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g_NGZh_pIaBD"
   },
   "outputs": [],
   "source": [
    "file_de = open(\"./train.ende.src\") \n",
    "file_en = open(\"./train.ende.mt\") \n",
    "\n",
    "lines_de = file_de.readlines() \n",
    "lines_en = file_en.readlines() \n",
    "\n",
    "max_length = 0\n",
    "lengths_de = []\n",
    "lengths_en = []\n",
    "\n",
    "for i in range(len(lines_de)):\n",
    "    text_de = lines_en[i].lower()\n",
    "    text_en = lines_de[i].lower()\n",
    "\n",
    "    l = [token.lemma_ for token in nlp_en.tokenizer(text_en)]\n",
    "    #l = [word for word in l if word not in stop_words_en.union(set([\"\\n\", \".\", \",\"]))]\n",
    "    l = [word for word in l if word not in stop_words_en]\n",
    "    lengths_en.append(len(l))\n",
    "    if len(l) > max_length: \n",
    "        max_length = len(l)\n",
    "        \n",
    "    l = [token.lemma_ for token in nlp_de.tokenizer(text_de)]\n",
    "    #l = [word for word in l if word not in stop_words_de.union(set([\"\\n\", \".\", \",\"]))]\n",
    "    l = [word for word in l if word not in stop_words_de]\n",
    "    lengths_de.append(len(l))\n",
    "    if len(l) > max_length: \n",
    "        max_length = len(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "19gsNCgnW8ZT"
   },
   "outputs": [],
   "source": [
    "def get_sentence_emb(line, nlp, lang):\n",
    "    if lang == 'en':\n",
    "        text = line.lower()\n",
    "        l = [token.lemma_ for token in nlp.tokenizer(text)]\n",
    "        l = ' '.join([word for word in l if word not in stop_words_en.union(set([\"\\n\", \".\", \",\"]))])\n",
    "\n",
    "    elif lang == 'de':\n",
    "        text = line.lower()\n",
    "        l = [token.lemma_ for token in nlp.tokenizer(text)]\n",
    "        l= ' '.join([word for word in l if word not in stop_words_de.union(set([\"\\n\", \".\", \",\"]))])\n",
    "\n",
    "    sen = nlp(l)\n",
    "    sen_vectorised = []\n",
    "    \n",
    "    for words in sen:\n",
    "        sen_vectorised.append(words.vector.tolist())  \n",
    "        \n",
    "    return sen_vectorised\n",
    "\n",
    "\n",
    "def get_embeddings(f,nlp,lang):\n",
    "    file = open(f) \n",
    "    lines = file.readlines() \n",
    "    sentences_vectors =[]\n",
    "\n",
    "    for l in lines:\n",
    "        vec = get_sentence_emb(l,nlp,lang)\n",
    "\n",
    "        if vec is not None:\n",
    "            sentences_vectors.append(vec)\n",
    "        else:\n",
    "            print(\"didn't work :\", l)\n",
    "            sentences_vectors.append(0)\n",
    "\n",
    "    return sentences_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NUKMgbo2sreI"
   },
   "source": [
    "#### Getting Training and Validation Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZXqZamIKs30T"
   },
   "source": [
    "We will now run the code fo the English-German translations and getting our training and validation sets ready for the regression task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LwoUIDj0otbf"
   },
   "outputs": [],
   "source": [
    "# EN-DE files\n",
    "de_train_src = get_embeddings(\"./train.ende.src\",nlp_en,'en')\n",
    "de_train_mt = get_embeddings(\"./train.ende.mt\",nlp_de,'de')\n",
    "\n",
    "f_train_scores = open(\"./train.ende.scores\",'r')\n",
    "de_train_scores = f_train_scores.readlines()\n",
    "\n",
    "de_val_src = get_embeddings(\"./dev.ende.src\",nlp_en,'en')\n",
    "de_val_mt = get_embeddings(\"./dev.ende.mt\",nlp_de,'de')\n",
    "f_val_scores = open(\"./dev.ende.scores\",'r')\n",
    "de_val_scores = f_val_scores.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Px7ikaGoy9r0"
   },
   "outputs": [],
   "source": [
    "# Transformation into Torch tensor \n",
    "X_train_en = de_train_src\n",
    "X_train_de = de_train_mt\n",
    "X_train = []\n",
    "\n",
    "for i in range(len(X_train_en)):\n",
    "    X_train.append([X_train_en[i], X_train_de[i]])\n",
    "    \n",
    "X_val_en = de_val_src\n",
    "X_val_de = de_val_mt\n",
    "X_val = []\n",
    "\n",
    "for i in range(len(X_val_en)):\n",
    "    X_val.append([X_val_en[i], X_val_de[i]])\n",
    "\n",
    "# Scores\n",
    "train_scores = np.array(de_train_scores).astype(float)\n",
    "y_train_de = train_scores\n",
    "train_scores_tensor = torch.from_numpy(train_scores)\n",
    "\n",
    "val_scores = np.array(de_val_scores).astype(float)\n",
    "y_val_de = val_scores\n",
    "val_scores_tensor = torch.from_numpy(val_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        dim1 = 20\n",
    "        dim2 = 20\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.i2h_1 = nn.Linear(input_size + hidden_size, dim1)\n",
    "        self.i2h_2 = nn.Linear(dim1, dim2)\n",
    "        self.i2h_3 = nn.Linear(dim2, hidden_size)\n",
    "        \n",
    "        self.i2o_1 = nn.Linear(input_size + hidden_size, dim1)\n",
    "        self.i2o_2 = nn.Linear(dim1, dim2)\n",
    "        self.i2o_3 = nn.Linear(dim2, output_size)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 0)\n",
    "        hidden = F.relu(self.i2h_1(combined))\n",
    "        hidden = F.relu(self.i2h_2(hidden))\n",
    "        hidden = self.i2h_3(hidden)\n",
    "        \n",
    "        output = F.relu(self.i2o_1(combined))\n",
    "        output = F.relu(self.i2o_2(output))\n",
    "        output = self.i2o_3(output)\n",
    "        output = self.sigmoid(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "n_hidden = 128\n",
    "dim_word_vector = 300\n",
    "n_categories = 1 \n",
    "rnn = RNN(dim_word_vector, n_hidden, n_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gated RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z646fe4JY4GN"
   },
   "outputs": [],
   "source": [
    "class GRUNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2):\n",
    "        super(GRUNet, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        out, h = self.gru(x, h)\n",
    "        out = self.fc(self.relu(out[:,-1]))\n",
    "        return out, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "GPU = True\n",
    "device_idx = 0\n",
    "if GPU:\n",
    "    device = torch.device(\"cuda:\" + str(device_idx) if torch.cuda.is_available() else \"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Loss\n",
    "def RMSELoss(yhat,y):\n",
    "    return torch.sqrt(torch.mean((yhat-y)**2))\n",
    "\n",
    "def PearsonLoss(pred_vs_score):\n",
    "  pred_vs_score = np.array(pred_vs_score)\n",
    "  pearson = pearsonr(pred_vs_score[:,0], pred_vs_score[:,1])\n",
    "  pearson = torch.Tensor(pearson)\n",
    "  return pearson[0]\n",
    "\n",
    "criterion = RMSELoss\n",
    "# criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################### NORMALISATION   ####################################################################################\n",
    "min_ = train_scores_tensor.min()\n",
    "max_ = train_scores_tensor.max()\n",
    "norm_train_scores_tensor = (train_scores_tensor - min_)/(max_- min_)\n",
    "\n",
    "def shuffle_train(X, scores):\n",
    "    a, b = sklearn.utils.shuffle(X, list(scores.numpy()))\n",
    "    return a, torch.FloatTensor(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YtPQdgUcjQoC"
   },
   "outputs": [],
   "source": [
    "####################################################### HYPER PARAMETERS AND MODEL BUILDING  ##################################################################\n",
    "n_hidden = 128\n",
    "num_layers = 2\n",
    "batch_size = 20\n",
    "num_epochs = 17\n",
    "drop_out = 0.2\n",
    "learning_rate = 0.001\n",
    "num_train_samples = 700\n",
    "\n",
    "rnn = GRUNet(input_dim=300, hidden_dim=n_hidden, output_dim=1, n_layers = num_layers, drop_prob = drop_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FHCgQyTpxWMA"
   },
   "outputs": [],
   "source": [
    "def initHidden():\n",
    "    return torch.zeros(n_hidden*num_layers).view(num_layers, 1, n_hidden)\n",
    "\n",
    "def sentence_to_tensors(index, dataset_to_use):\n",
    "    sample = dataset_to_use[index]\n",
    "    sample_en = sample[0]\n",
    "    sample_de = sample[1]\n",
    "    sample_en_tensor = torch.FloatTensor(sample_en)\n",
    "    sample_de_tensor = torch.FloatTensor(sample_de)\n",
    "    return sample_en_tensor, sample_de_tensor\n",
    "\n",
    "######################################################################### TRAIN  ####################################################################################\n",
    "def forward_line(en_tensor, de_tensor, norm_line_score):\n",
    "    hidden = initHidden()\n",
    "    for i in range(en_tensor.size()[0]):\n",
    "        output, hidden = rnn(en_tensor[i,:].view(1,1,-1), hidden)\n",
    "    for i in range(de_tensor.size()[0]):\n",
    "        output, hidden = rnn(de_tensor[i,:].view(1,1,-1), hidden)\n",
    "    output = output.cuda()\n",
    "    loss = criterion(output, norm_line_score)\n",
    "    return output, norm_line_score, loss\n",
    "\n",
    "def train():\n",
    "    rnn.train()\n",
    "    total_loss = 0\n",
    "    loss = []\n",
    "    ##### Shufling og the dataset and the label #####\n",
    "    X_train_shuffle , norm_train_scores_tensor_shuffle = shuffle_train(X_train, norm_train_scores_tensor )\n",
    "    #################################################\n",
    "    for i in range(num_train_samples):\n",
    "        en_tensor_line, de_tensor_line = sentence_to_tensors(i, X_train_shuffle)\n",
    "        norm_line_score = torch.full((1,), norm_train_scores_tensor_shuffle[i].item(), device=device)\n",
    "        output, line_score, line_loss = forward_line(en_tensor_line, de_tensor_line, norm_line_score)\n",
    "        loss.append(line_loss) \n",
    "        \n",
    "        # Back-propagation\n",
    "        if i % batch_size == 0 and i != 0:\n",
    "          rnn.zero_grad()\n",
    "          mean_batch_loss = sum(loss)/batch_size\n",
    "          mean_batch_loss.backward() \n",
    "          for p in rnn.parameters():\n",
    "            p.data.add_(-learning_rate, p.grad.data)\n",
    "        \n",
    "          total_loss += sum(loss).item()\n",
    "          print([str(i) + '/' + str(num_train_samples)])\n",
    "          print('Mean batch loss = ' + str(sum(loss).item()/batch_size))\n",
    "          loss = []\n",
    "                    \n",
    "    return total_loss / num_train_samples\n",
    "\n",
    "######################################################################### TEST  ####################################################################################\n",
    "def test_line(en_tensor, de_tensor):\n",
    "    with torch.no_grad():\n",
    "        hidden = initHidden()\n",
    "        for i in range(en_tensor.size()[0]):\n",
    "            output, hidden = rnn(en_tensor[i,:].view(1,1,-1), hidden)\n",
    "        for i in range(de_tensor.size()[0]):\n",
    "            output, hidden = rnn(de_tensor[i,:].view(1,1,-1), hidden)\n",
    "        return output.cuda()\n",
    "\n",
    "def test():\n",
    "    rnn.eval()\n",
    "    total_loss = 0\n",
    "    num_train = 1000\n",
    "    pred_vs_score = []\n",
    "    for i in range(num_train):\n",
    "        en_tensor_line, de_tensor_line = sentence_to_tensors(i, X_val)\n",
    "        line_score = torch.full((1,), val_scores_tensor[i].item(), device=device)\n",
    "        output = test_line(en_tensor_line, de_tensor_line)\n",
    "        output = output *(max_ - min_) + min_ \n",
    "        pred_vs_score.append([output.item(), line_score.item()])\n",
    "        loss = criterion(output, line_score)\n",
    "        total_loss += loss.item()\n",
    "    p = PearsonLoss(pred_vs_score)\n",
    "    print('Pearson on validation = ' + str(p.item()))\n",
    "    return total_loss / num_train , p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "CanMIfJQxWKA",
    "outputId": "9096612b-ea38-4629-b4f4-bcfa5e87b283"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20/700']\n",
      "Mean batch loss = 0.7563954353332519\n",
      "['40/700']\n",
      "Mean batch loss = 0.7017841815948487\n",
      "['60/700']\n",
      "Mean batch loss = 0.6843565464019775\n",
      "['80/700']\n",
      "Mean batch loss = 0.7075612545013428\n",
      "['100/700']\n",
      "Mean batch loss = 0.6894464015960693\n",
      "['120/700']\n",
      "Mean batch loss = 0.6841238975524903\n",
      "['140/700']\n",
      "Mean batch loss = 0.6874476432800293\n",
      "['160/700']\n",
      "Mean batch loss = 0.6888272285461425\n",
      "['180/700']\n",
      "Mean batch loss = 0.6818910121917725\n",
      "['200/700']\n",
      "Mean batch loss = 0.6930150032043457\n",
      "['220/700']\n",
      "Mean batch loss = 0.6562315464019776\n",
      "['240/700']\n",
      "Mean batch loss = 0.6814474582672119\n",
      "['260/700']\n",
      "Mean batch loss = 0.6686973094940185\n",
      "['280/700']\n",
      "Mean batch loss = 0.661841630935669\n",
      "['300/700']\n",
      "Mean batch loss = 0.6762514591217041\n",
      "['320/700']\n",
      "Mean batch loss = 0.6394819736480712\n",
      "['340/700']\n",
      "Mean batch loss = 0.6412061214447021\n",
      "['360/700']\n",
      "Mean batch loss = 0.6697610855102539\n",
      "['380/700']\n",
      "Mean batch loss = 0.6807321548461914\n",
      "['400/700']\n",
      "Mean batch loss = 0.6628926753997803\n",
      "['420/700']\n",
      "Mean batch loss = 0.6733636379241943\n",
      "['440/700']\n",
      "Mean batch loss = 0.6504978656768798\n",
      "['460/700']\n",
      "Mean batch loss = 0.6680403232574463\n",
      "['480/700']\n",
      "Mean batch loss = 0.6602311134338379\n",
      "['500/700']\n",
      "Mean batch loss = 0.6645330905914306\n",
      "['520/700']\n",
      "Mean batch loss = 0.616535997390747\n",
      "['540/700']\n",
      "Mean batch loss = 0.6347466945648194\n",
      "['560/700']\n",
      "Mean batch loss = 0.6273497581481934\n",
      "['580/700']\n",
      "Mean batch loss = 0.6612743377685547\n",
      "['600/700']\n",
      "Mean batch loss = 0.6597868442535401\n",
      "['620/700']\n",
      "Mean batch loss = 0.6645454883575439\n",
      "['640/700']\n",
      "Mean batch loss = 0.6603816032409668\n",
      "['660/700']\n",
      "Mean batch loss = 0.6546692371368408\n",
      "['680/700']\n",
      "Mean batch loss = 0.6326045036315918\n",
      "Pearson on validation = -0.024794617667794228\n",
      "Epoch 0: 0.6497700718470982 // 6.956674631118775 (29s)\n",
      "['20/700']\n",
      "Mean batch loss = 0.6851646423339843\n",
      "['40/700']\n",
      "Mean batch loss = 0.6268198013305664\n",
      "['60/700']\n",
      "Mean batch loss = 0.6523571968078613\n",
      "['80/700']\n",
      "Mean batch loss = 0.6319043159484863\n",
      "['100/700']\n",
      "Mean batch loss = 0.6178918838500976\n",
      "['120/700']\n",
      "Mean batch loss = 0.6440662384033203\n",
      "['140/700']\n",
      "Mean batch loss = 0.6306598663330079\n",
      "['160/700']\n",
      "Mean batch loss = 0.6334511756896972\n",
      "['180/700']\n",
      "Mean batch loss = 0.6214161872863769\n",
      "['200/700']\n",
      "Mean batch loss = 0.6366003036499024\n",
      "['220/700']\n",
      "Mean batch loss = 0.6319411277770997\n",
      "['240/700']\n",
      "Mean batch loss = 0.6137836933135986\n",
      "['260/700']\n",
      "Mean batch loss = 0.5756266593933106\n",
      "['280/700']\n",
      "Mean batch loss = 0.6030235767364502\n",
      "['300/700']\n",
      "Mean batch loss = 0.5874292373657226\n",
      "['320/700']\n",
      "Mean batch loss = 0.6099835872650147\n",
      "['340/700']\n",
      "Mean batch loss = 0.6133627891540527\n",
      "['360/700']\n",
      "Mean batch loss = 0.6104119300842286\n",
      "['380/700']\n",
      "Mean batch loss = 0.6110722064971924\n",
      "['400/700']\n",
      "Mean batch loss = 0.6329163074493408\n",
      "['420/700']\n",
      "Mean batch loss = 0.6204140186309814\n",
      "['440/700']\n",
      "Mean batch loss = 0.6146873950958252\n",
      "['460/700']\n",
      "Mean batch loss = 0.6103341579437256\n",
      "['480/700']\n",
      "Mean batch loss = 0.5735375881195068\n",
      "['500/700']\n",
      "Mean batch loss = 0.5967160224914551\n",
      "['520/700']\n",
      "Mean batch loss = 0.6146932601928711\n",
      "['540/700']\n",
      "Mean batch loss = 0.5938406467437745\n",
      "['560/700']\n",
      "Mean batch loss = 0.5983627796173095\n",
      "['580/700']\n",
      "Mean batch loss = 0.5900798320770264\n",
      "['600/700']\n",
      "Mean batch loss = 0.5907768726348877\n",
      "['620/700']\n",
      "Mean batch loss = 0.6125837802886963\n",
      "['640/700']\n",
      "Mean batch loss = 0.5858358383178711\n",
      "['660/700']\n",
      "Mean batch loss = 0.5860493183135986\n",
      "['680/700']\n",
      "Mean batch loss = 0.5880678653717041\n",
      "Pearson on validation = -0.03660452738404274\n",
      "Epoch 1: 0.5955960600716728 // 6.341184042215347 (20s)\n",
      "['20/700']\n",
      "Mean batch loss = 0.6085041999816895\n",
      "['40/700']\n",
      "Mean batch loss = 0.5579441070556641\n",
      "['60/700']\n",
      "Mean batch loss = 0.577669095993042\n",
      "['80/700']\n",
      "Mean batch loss = 0.5830564498901367\n",
      "['100/700']\n",
      "Mean batch loss = 0.5661751747131347\n",
      "['120/700']\n",
      "Mean batch loss = 0.5855491638183594\n",
      "['140/700']\n",
      "Mean batch loss = 0.5522617816925048\n",
      "['160/700']\n",
      "Mean batch loss = 0.583457612991333\n",
      "['180/700']\n",
      "Mean batch loss = 0.5370641708374023\n",
      "['200/700']\n",
      "Mean batch loss = 0.56558198928833\n",
      "['220/700']\n",
      "Mean batch loss = 0.5705883502960205\n",
      "['240/700']\n",
      "Mean batch loss = 0.5694365501403809\n",
      "['260/700']\n",
      "Mean batch loss = 0.5538646221160889\n",
      "['280/700']\n",
      "Mean batch loss = 0.5401577949523926\n",
      "['300/700']\n",
      "Mean batch loss = 0.5359262943267822\n",
      "['320/700']\n",
      "Mean batch loss = 0.5709155082702637\n",
      "['340/700']\n",
      "Mean batch loss = 0.5387330055236816\n",
      "['360/700']\n",
      "Mean batch loss = 0.548780870437622\n",
      "['380/700']\n",
      "Mean batch loss = 0.5609271049499511\n",
      "['400/700']\n",
      "Mean batch loss = 0.5664535522460937\n",
      "['420/700']\n",
      "Mean batch loss = 0.5137651920318603\n",
      "['440/700']\n",
      "Mean batch loss = 0.5283012866973877\n",
      "['460/700']\n",
      "Mean batch loss = 0.5509902477264405\n",
      "['480/700']\n",
      "Mean batch loss = 0.5570971012115479\n",
      "['500/700']\n",
      "Mean batch loss = 0.5236910343170166\n",
      "['520/700']\n",
      "Mean batch loss = 0.5336123943328858\n",
      "['540/700']\n",
      "Mean batch loss = 0.5412302017211914\n",
      "['560/700']\n",
      "Mean batch loss = 0.5459951877593994\n",
      "['580/700']\n",
      "Mean batch loss = 0.5245865821838379\n",
      "['600/700']\n",
      "Mean batch loss = 0.5379642009735107\n",
      "['620/700']\n",
      "Mean batch loss = 0.5293355941772461\n",
      "['640/700']\n",
      "Mean batch loss = 0.5118668556213379\n",
      "['660/700']\n",
      "Mean batch loss = 0.5023102760314941\n",
      "['680/700']\n",
      "Mean batch loss = 0.49846410751342773\n",
      "Pearson on validation = -0.04762527346611023\n",
      "Epoch 2: 0.5334930760519845 // 5.734320713281631 (20s)\n",
      "['20/700']\n",
      "Mean batch loss = 0.5668267250061035\n",
      "['40/700']\n",
      "Mean batch loss = 0.5340590000152587\n",
      "['60/700']\n",
      "Mean batch loss = 0.5262238502502441\n",
      "['80/700']\n",
      "Mean batch loss = 0.5233766078948975\n",
      "['100/700']\n",
      "Mean batch loss = 0.5028469562530518\n",
      "['120/700']\n",
      "Mean batch loss = 0.4943286895751953\n",
      "['140/700']\n",
      "Mean batch loss = 0.5218420028686523\n",
      "['160/700']\n",
      "Mean batch loss = 0.531653642654419\n",
      "['180/700']\n",
      "Mean batch loss = 0.524812650680542\n",
      "['200/700']\n",
      "Mean batch loss = 0.5229899406433105\n",
      "['220/700']\n",
      "Mean batch loss = 0.5379698753356934\n",
      "['240/700']\n",
      "Mean batch loss = 0.49902052879333497\n",
      "['260/700']\n",
      "Mean batch loss = 0.5079662322998046\n",
      "['280/700']\n",
      "Mean batch loss = 0.4937140464782715\n",
      "['300/700']\n",
      "Mean batch loss = 0.4919143199920654\n",
      "['320/700']\n",
      "Mean batch loss = 0.5258818626403808\n",
      "['340/700']\n",
      "Mean batch loss = 0.5139533996582031\n",
      "['360/700']\n",
      "Mean batch loss = 0.5011018753051758\n",
      "['380/700']\n",
      "Mean batch loss = 0.48942198753356936\n",
      "['400/700']\n",
      "Mean batch loss = 0.454042911529541\n",
      "['420/700']\n",
      "Mean batch loss = 0.5061775207519531\n",
      "['440/700']\n",
      "Mean batch loss = 0.49439253807067873\n",
      "['460/700']\n",
      "Mean batch loss = 0.49735264778137206\n",
      "['480/700']\n",
      "Mean batch loss = 0.48317432403564453\n",
      "['500/700']\n",
      "Mean batch loss = 0.5105286598205566\n",
      "['520/700']\n",
      "Mean batch loss = 0.4980918884277344\n",
      "['540/700']\n",
      "Mean batch loss = 0.467564058303833\n",
      "['560/700']\n",
      "Mean batch loss = 0.47257146835327146\n",
      "['580/700']\n",
      "Mean batch loss = 0.48847160339355467\n",
      "['600/700']\n",
      "Mean batch loss = 0.4911336898803711\n",
      "['620/700']\n",
      "Mean batch loss = 0.45626282691955566\n",
      "['640/700']\n",
      "Mean batch loss = 0.47187085151672364\n",
      "['660/700']\n",
      "Mean batch loss = 0.48519034385681153\n",
      "['680/700']\n",
      "Mean batch loss = 0.47000622749328613\n",
      "Pearson on validation = -0.05766589939594269\n",
      "Epoch 3: 0.48733530725751606 // 5.1148325278759 (19s)\n",
      "['20/700']\n",
      "Mean batch loss = 0.4590958595275879\n",
      "['40/700']\n",
      "Mean batch loss = 0.48819599151611326\n",
      "['60/700']\n",
      "Mean batch loss = 0.4715552806854248\n",
      "['80/700']\n",
      "Mean batch loss = 0.47243223190307615\n",
      "['100/700']\n",
      "Mean batch loss = 0.45020279884338377\n",
      "['120/700']\n",
      "Mean batch loss = 0.4621079444885254\n",
      "['140/700']\n",
      "Mean batch loss = 0.49218454360961916\n",
      "['160/700']\n",
      "Mean batch loss = 0.45389652252197266\n",
      "['180/700']\n",
      "Mean batch loss = 0.4763175010681152\n",
      "['200/700']\n",
      "Mean batch loss = 0.45537819862365725\n",
      "['220/700']\n",
      "Mean batch loss = 0.4729275703430176\n",
      "['240/700']\n",
      "Mean batch loss = 0.4756946563720703\n",
      "['260/700']\n",
      "Mean batch loss = 0.4463002681732178\n",
      "['280/700']\n",
      "Mean batch loss = 0.4615300178527832\n",
      "['300/700']\n",
      "Mean batch loss = 0.41499757766723633\n",
      "['320/700']\n",
      "Mean batch loss = 0.4617812156677246\n",
      "['340/700']\n",
      "Mean batch loss = 0.43320045471191404\n",
      "['360/700']\n",
      "Mean batch loss = 0.4420783519744873\n",
      "['380/700']\n",
      "Mean batch loss = 0.4721070766448975\n",
      "['400/700']\n",
      "Mean batch loss = 0.41439056396484375\n",
      "['420/700']\n",
      "Mean batch loss = 0.42844290733337403\n",
      "['440/700']\n",
      "Mean batch loss = 0.4392336368560791\n",
      "['460/700']\n",
      "Mean batch loss = 0.440472412109375\n",
      "['480/700']\n",
      "Mean batch loss = 0.42720975875854494\n",
      "['500/700']\n",
      "Mean batch loss = 0.4430577278137207\n",
      "['520/700']\n",
      "Mean batch loss = 0.423721981048584\n",
      "['540/700']\n",
      "Mean batch loss = 0.4223503112792969\n",
      "['560/700']\n",
      "Mean batch loss = 0.42041945457458496\n",
      "['580/700']\n",
      "Mean batch loss = 0.4203506946563721\n",
      "['600/700']\n",
      "Mean batch loss = 0.42159547805786135\n",
      "['620/700']\n",
      "Mean batch loss = 0.4236637592315674\n",
      "['640/700']\n",
      "Mean batch loss = 0.4412808418273926\n",
      "['660/700']\n",
      "Mean batch loss = 0.4289868354797363\n",
      "['680/700']\n",
      "Mean batch loss = 0.4015210151672363\n",
      "Pearson on validation = -0.06512607634067535\n",
      "Epoch 4: 0.43310518401009696 // 4.4854088082313535 (20s)\n",
      "['20/700']\n",
      "Mean batch loss = 0.4451779842376709\n",
      "['40/700']\n",
      "Mean batch loss = 0.40959844589233396\n",
      "['60/700']\n",
      "Mean batch loss = 0.4175443649291992\n",
      "['80/700']\n",
      "Mean batch loss = 0.40425825119018555\n",
      "['100/700']\n",
      "Mean batch loss = 0.4122473239898682\n",
      "['120/700']\n",
      "Mean batch loss = 0.4071939468383789\n",
      "['140/700']\n",
      "Mean batch loss = 0.3848224639892578\n",
      "['160/700']\n",
      "Mean batch loss = 0.40236659049987794\n",
      "['180/700']\n",
      "Mean batch loss = 0.3791849374771118\n",
      "['200/700']\n",
      "Mean batch loss = 0.3735352993011475\n",
      "['220/700']\n",
      "Mean batch loss = 0.40201077461242674\n",
      "['240/700']\n",
      "Mean batch loss = 0.4127970695495605\n",
      "['260/700']\n",
      "Mean batch loss = 0.40917372703552246\n",
      "['280/700']\n",
      "Mean batch loss = 0.39396958351135253\n",
      "['300/700']\n",
      "Mean batch loss = 0.4022841930389404\n",
      "['320/700']\n",
      "Mean batch loss = 0.3786498785018921\n",
      "['340/700']\n",
      "Mean batch loss = 0.40032243728637695\n",
      "['360/700']\n",
      "Mean batch loss = 0.38133327960968016\n",
      "['380/700']\n",
      "Mean batch loss = 0.38068039417266847\n",
      "['400/700']\n",
      "Mean batch loss = 0.3590701103210449\n",
      "['420/700']\n",
      "Mean batch loss = 0.3729349851608276\n",
      "['440/700']\n",
      "Mean batch loss = 0.40695843696594236\n",
      "['460/700']\n",
      "Mean batch loss = 0.387892746925354\n",
      "['480/700']\n",
      "Mean batch loss = 0.3637770891189575\n",
      "['500/700']\n",
      "Mean batch loss = 0.35244662761688234\n",
      "['520/700']\n",
      "Mean batch loss = 0.36478731632232664\n",
      "['540/700']\n",
      "Mean batch loss = 0.3583985805511475\n",
      "['560/700']\n",
      "Mean batch loss = 0.3557835578918457\n",
      "['580/700']\n",
      "Mean batch loss = 0.3786763668060303\n",
      "['600/700']\n",
      "Mean batch loss = 0.353377366065979\n",
      "['620/700']\n",
      "Mean batch loss = 0.36642088890075686\n",
      "['640/700']\n",
      "Mean batch loss = 0.3338383913040161\n",
      "['660/700']\n",
      "Mean batch loss = 0.36812093257904055\n",
      "['680/700']\n",
      "Mean batch loss = 0.3699279069900513\n",
      "Pearson on validation = -0.07153082638978958\n",
      "Epoch 5: 0.3739874928338187 // 3.8418085156679154 (20s)\n",
      "['20/700']\n",
      "Mean batch loss = 0.3733027696609497\n",
      "['40/700']\n",
      "Mean batch loss = 0.3313500642776489\n",
      "['60/700']\n",
      "Mean batch loss = 0.3522865056991577\n",
      "['80/700']\n",
      "Mean batch loss = 0.36469807624816897\n",
      "['100/700']\n",
      "Mean batch loss = 0.306115460395813\n",
      "['120/700']\n",
      "Mean batch loss = 0.33460159301757814\n",
      "['140/700']\n",
      "Mean batch loss = 0.33160037994384767\n",
      "['160/700']\n",
      "Mean batch loss = 0.34206514358520507\n",
      "['180/700']\n",
      "Mean batch loss = 0.3368978500366211\n",
      "['200/700']\n",
      "Mean batch loss = 0.3197345733642578\n",
      "['220/700']\n",
      "Mean batch loss = 0.3362764835357666\n",
      "['240/700']\n",
      "Mean batch loss = 0.30281212329864504\n",
      "['260/700']\n",
      "Mean batch loss = 0.34091687202453613\n",
      "['280/700']\n",
      "Mean batch loss = 0.3295294284820557\n",
      "['300/700']\n",
      "Mean batch loss = 0.3360611915588379\n",
      "['320/700']\n",
      "Mean batch loss = 0.3280025005340576\n",
      "['340/700']\n",
      "Mean batch loss = 0.33654749393463135\n",
      "['360/700']\n",
      "Mean batch loss = 0.3103705167770386\n",
      "['380/700']\n",
      "Mean batch loss = 0.3401570558547974\n",
      "['400/700']\n",
      "Mean batch loss = 0.3125976324081421\n",
      "['420/700']\n",
      "Mean batch loss = 0.3273890018463135\n",
      "['440/700']\n",
      "Mean batch loss = 0.32648842334747313\n",
      "['460/700']\n",
      "Mean batch loss = 0.3401259660720825\n",
      "['480/700']\n",
      "Mean batch loss = 0.30187010765075684\n",
      "['500/700']\n",
      "Mean batch loss = 0.2940457820892334\n",
      "['520/700']\n",
      "Mean batch loss = 0.3250737190246582\n",
      "['540/700']\n",
      "Mean batch loss = 0.30765547752380373\n",
      "['560/700']\n",
      "Mean batch loss = 0.3244529485702515\n",
      "['580/700']\n",
      "Mean batch loss = 0.3043546676635742\n",
      "['600/700']\n",
      "Mean batch loss = 0.27364866733551024\n",
      "['620/700']\n",
      "Mean batch loss = 0.2851196527481079\n",
      "['640/700']\n",
      "Mean batch loss = 0.3084902286529541\n",
      "['660/700']\n",
      "Mean batch loss = 0.309800910949707\n",
      "['680/700']\n",
      "Mean batch loss = 0.2835453748703003\n",
      "Pearson on validation = -0.07647213339805603\n",
      "Epoch 6: 0.3136567040852138 // 3.185153475999832 (19s)\n",
      "['20/700']\n",
      "Mean batch loss = 0.28951339721679686\n",
      "['40/700']\n",
      "Mean batch loss = 0.28845274448394775\n",
      "['60/700']\n",
      "Mean batch loss = 0.2926074743270874\n",
      "['80/700']\n",
      "Mean batch loss = 0.2997269630432129\n",
      "['100/700']\n",
      "Mean batch loss = 0.2853466749191284\n",
      "['120/700']\n",
      "Mean batch loss = 0.2703463077545166\n",
      "['140/700']\n",
      "Mean batch loss = 0.2859311580657959\n",
      "['160/700']\n",
      "Mean batch loss = 0.284447455406189\n",
      "['180/700']\n",
      "Mean batch loss = 0.27641561031341555\n",
      "['200/700']\n",
      "Mean batch loss = 0.2856779098510742\n",
      "['220/700']\n",
      "Mean batch loss = 0.2625377893447876\n",
      "['240/700']\n",
      "Mean batch loss = 0.2860224485397339\n",
      "['260/700']\n",
      "Mean batch loss = 0.26679975986480714\n",
      "['280/700']\n",
      "Mean batch loss = 0.2794848203659058\n",
      "['300/700']\n",
      "Mean batch loss = 0.2735734939575195\n",
      "['320/700']\n",
      "Mean batch loss = 0.27346742153167725\n",
      "['340/700']\n",
      "Mean batch loss = 0.27668602466583253\n",
      "['360/700']\n",
      "Mean batch loss = 0.2554752349853516\n",
      "['380/700']\n",
      "Mean batch loss = 0.25577316284179685\n",
      "['400/700']\n",
      "Mean batch loss = 0.2852674961090088\n",
      "['420/700']\n",
      "Mean batch loss = 0.279215407371521\n",
      "['440/700']\n",
      "Mean batch loss = 0.2702966213226318\n",
      "['460/700']\n",
      "Mean batch loss = 0.24690253734588624\n",
      "['480/700']\n",
      "Mean batch loss = 0.26858367919921877\n",
      "['500/700']\n",
      "Mean batch loss = 0.27602009773254393\n",
      "['520/700']\n",
      "Mean batch loss = 0.2654555082321167\n",
      "['540/700']\n",
      "Mean batch loss = 0.2434925079345703\n",
      "['560/700']\n",
      "Mean batch loss = 0.23286786079406738\n",
      "['580/700']\n",
      "Mean batch loss = 0.24025211334228516\n",
      "['600/700']\n",
      "Mean batch loss = 0.21940145492553711\n",
      "['620/700']\n",
      "Mean batch loss = 0.2372504234313965\n",
      "['640/700']\n",
      "Mean batch loss = 0.23732750415802\n",
      "['660/700']\n",
      "Mean batch loss = 0.23652248382568358\n",
      "['680/700']\n",
      "Mean batch loss = 0.24856209754943848\n",
      "Pearson on validation = -0.08012949675321579\n",
      "Epoch 7: 0.2593058184215001 // 2.5271828808784487 (20s)\n",
      "['20/700']\n",
      "Mean batch loss = 0.2440349817276001\n",
      "['40/700']\n",
      "Mean batch loss = 0.2348869562149048\n",
      "['60/700']\n",
      "Mean batch loss = 0.21091618537902831\n",
      "['80/700']\n",
      "Mean batch loss = 0.22665436267852784\n",
      "['100/700']\n",
      "Mean batch loss = 0.21510848999023438\n",
      "['120/700']\n",
      "Mean batch loss = 0.22268927097320557\n",
      "['140/700']\n",
      "Mean batch loss = 0.23020617961883544\n",
      "['160/700']\n",
      "Mean batch loss = 0.22338225841522216\n",
      "['180/700']\n",
      "Mean batch loss = 0.20786659717559813\n",
      "['200/700']\n",
      "Mean batch loss = 0.1941827654838562\n",
      "['220/700']\n",
      "Mean batch loss = 0.22719645500183105\n",
      "['240/700']\n",
      "Mean batch loss = 0.19470996856689454\n",
      "['260/700']\n",
      "Mean batch loss = 0.20178086757659913\n",
      "['280/700']\n",
      "Mean batch loss = 0.17795470952987671\n",
      "['300/700']\n",
      "Mean batch loss = 0.20040040016174315\n",
      "['320/700']\n",
      "Mean batch loss = 0.21020331382751464\n",
      "['340/700']\n",
      "Mean batch loss = 0.1847725987434387\n",
      "['360/700']\n",
      "Mean batch loss = 0.1881201982498169\n",
      "['380/700']\n",
      "Mean batch loss = 0.20959532260894775\n",
      "['400/700']\n",
      "Mean batch loss = 0.1901899218559265\n",
      "['420/700']\n",
      "Mean batch loss = 0.2277221202850342\n",
      "['440/700']\n",
      "Mean batch loss = 0.19784029722213745\n",
      "['460/700']\n",
      "Mean batch loss = 0.16498878002166747\n",
      "['480/700']\n",
      "Mean batch loss = 0.20241374969482423\n",
      "['500/700']\n",
      "Mean batch loss = 0.18081297874450683\n",
      "['520/700']\n",
      "Mean batch loss = 0.173950457572937\n",
      "['540/700']\n",
      "Mean batch loss = 0.1702414870262146\n",
      "['560/700']\n",
      "Mean batch loss = 0.20633926391601562\n",
      "['580/700']\n",
      "Mean batch loss = 0.18542759418487548\n",
      "['600/700']\n",
      "Mean batch loss = 0.17655937671661376\n",
      "['620/700']\n",
      "Mean batch loss = 0.15868887901306153\n",
      "['640/700']\n",
      "Mean batch loss = 0.16512811183929443\n",
      "['660/700']\n",
      "Mean batch loss = 0.18047279119491577\n",
      "['680/700']\n",
      "Mean batch loss = 0.170135498046875\n",
      "Pearson on validation = -0.08329619467258453\n",
      "Epoch 8: 0.1930163768359593 // 1.8860450096726418 (19s)\n",
      "['20/700']\n",
      "Mean batch loss = 0.21475906372070314\n",
      "['40/700']\n",
      "Mean batch loss = 0.184660804271698\n",
      "['60/700']\n",
      "Mean batch loss = 0.15107709169387817\n",
      "['80/700']\n",
      "Mean batch loss = 0.14208325147628784\n",
      "['100/700']\n",
      "Mean batch loss = 0.1893514037132263\n",
      "['120/700']\n",
      "Mean batch loss = 0.16056458950042723\n",
      "['140/700']\n",
      "Mean batch loss = 0.1836233377456665\n",
      "['160/700']\n",
      "Mean batch loss = 0.16887741088867186\n",
      "['180/700']\n",
      "Mean batch loss = 0.13984928131103516\n",
      "['200/700']\n",
      "Mean batch loss = 0.16099214553833008\n",
      "['220/700']\n",
      "Mean batch loss = 0.14902801513671876\n",
      "['240/700']\n",
      "Mean batch loss = 0.16846251487731934\n",
      "['260/700']\n",
      "Mean batch loss = 0.14182618856430054\n",
      "['280/700']\n",
      "Mean batch loss = 0.17238720655441284\n",
      "['300/700']\n",
      "Mean batch loss = 0.15931880474090576\n",
      "['320/700']\n",
      "Mean batch loss = 0.16024258136749267\n",
      "['340/700']\n",
      "Mean batch loss = 0.14293909072875977\n",
      "['360/700']\n",
      "Mean batch loss = 0.1359827399253845\n",
      "['380/700']\n",
      "Mean batch loss = 0.12032769918441773\n",
      "['400/700']\n",
      "Mean batch loss = 0.1260688543319702\n",
      "['420/700']\n",
      "Mean batch loss = 0.12149864435195923\n",
      "['440/700']\n",
      "Mean batch loss = 0.11194604635238647\n",
      "['460/700']\n",
      "Mean batch loss = 0.14555160999298095\n",
      "['480/700']\n",
      "Mean batch loss = 0.13930444717407225\n",
      "['500/700']\n",
      "Mean batch loss = 0.1335153818130493\n",
      "['520/700']\n",
      "Mean batch loss = 0.13838590383529664\n",
      "['540/700']\n",
      "Mean batch loss = 0.15690076351165771\n",
      "['560/700']\n",
      "Mean batch loss = 0.1453643798828125\n",
      "['580/700']\n",
      "Mean batch loss = 0.13914769887924194\n",
      "['600/700']\n",
      "Mean batch loss = 0.10026123523712158\n",
      "['620/700']\n",
      "Mean batch loss = 0.12149491310119628\n",
      "['640/700']\n",
      "Mean batch loss = 0.13213509321212769\n",
      "['660/700']\n",
      "Mean batch loss = 0.1277146577835083\n",
      "['680/700']\n",
      "Mean batch loss = 0.13787510395050048\n",
      "Pearson on validation = -0.08598385751247406\n",
      "Epoch 9: 0.1435290844099862 // 1.2849567275643348 (20s)\n",
      "['20/700']\n",
      "Mean batch loss = 0.11007857322692871\n",
      "['40/700']\n",
      "Mean batch loss = 0.11969850063323975\n",
      "['60/700']\n",
      "Mean batch loss = 0.1262555480003357\n",
      "['80/700']\n",
      "Mean batch loss = 0.10561639070510864\n",
      "['100/700']\n",
      "Mean batch loss = 0.09092274904251099\n",
      "['120/700']\n",
      "Mean batch loss = 0.10118709802627564\n",
      "['140/700']\n",
      "Mean batch loss = 0.1302419662475586\n",
      "['160/700']\n",
      "Mean batch loss = 0.09857566952705384\n",
      "['180/700']\n",
      "Mean batch loss = 0.08840302228927613\n",
      "['200/700']\n",
      "Mean batch loss = 0.10111265182495117\n",
      "['220/700']\n",
      "Mean batch loss = 0.11659539937973022\n",
      "['240/700']\n",
      "Mean batch loss = 0.10491487979888917\n",
      "['260/700']\n",
      "Mean batch loss = 0.08697571754455566\n",
      "['280/700']\n",
      "Mean batch loss = 0.10162079334259033\n",
      "['300/700']\n",
      "Mean batch loss = 0.09280426502227783\n",
      "['320/700']\n",
      "Mean batch loss = 0.10142409801483154\n",
      "['340/700']\n",
      "Mean batch loss = 0.09267296195030213\n",
      "['360/700']\n",
      "Mean batch loss = 0.09129181504249573\n",
      "['380/700']\n",
      "Mean batch loss = 0.08781778812408447\n",
      "['400/700']\n",
      "Mean batch loss = 0.09365734457969666\n",
      "['420/700']\n",
      "Mean batch loss = 0.07862472534179688\n",
      "['440/700']\n",
      "Mean batch loss = 0.07532474994659424\n",
      "['460/700']\n",
      "Mean batch loss = 0.0681804060935974\n",
      "['480/700']\n",
      "Mean batch loss = 0.08147432804107665\n",
      "['500/700']\n",
      "Mean batch loss = 0.10268436670303345\n",
      "['520/700']\n",
      "Mean batch loss = 0.08536652326583863\n",
      "['540/700']\n",
      "Mean batch loss = 0.06924595832824706\n",
      "['560/700']\n",
      "Mean batch loss = 0.08637058734893799\n",
      "['580/700']\n",
      "Mean batch loss = 0.07127529382705688\n",
      "['600/700']\n",
      "Mean batch loss = 0.08661834001541138\n",
      "['620/700']\n",
      "Mean batch loss = 0.11022682189941406\n",
      "['640/700']\n",
      "Mean batch loss = 0.0831487774848938\n",
      "['660/700']\n",
      "Mean batch loss = 0.07833402156829834\n",
      "['680/700']\n",
      "Mean batch loss = 0.07810014486312866\n",
      "Pearson on validation = -0.08769600838422775\n",
      "Epoch 10: 0.09133835077285767 // 0.8386855151839554 (20s)\n",
      "['20/700']\n",
      "Mean batch loss = 0.07506484985351562\n",
      "['40/700']\n",
      "Mean batch loss = 0.0677131474018097\n",
      "['60/700']\n",
      "Mean batch loss = 0.06581509113311768\n",
      "['80/700']\n",
      "Mean batch loss = 0.060984498262405394\n",
      "['100/700']\n",
      "Mean batch loss = 0.06212608814239502\n",
      "['120/700']\n",
      "Mean batch loss = 0.06976521611213685\n",
      "['140/700']\n",
      "Mean batch loss = 0.07332034111022949\n",
      "['160/700']\n",
      "Mean batch loss = 0.07556599974632264\n",
      "['180/700']\n",
      "Mean batch loss = 0.06605461835861207\n",
      "['200/700']\n",
      "Mean batch loss = 0.07930068969726563\n",
      "['220/700']\n",
      "Mean batch loss = 0.06633845567703248\n",
      "['240/700']\n",
      "Mean batch loss = 0.08274334669113159\n",
      "['260/700']\n",
      "Mean batch loss = 0.08348209261894227\n",
      "['280/700']\n",
      "Mean batch loss = 0.0875815987586975\n",
      "['300/700']\n",
      "Mean batch loss = 0.0716053605079651\n",
      "['320/700']\n",
      "Mean batch loss = 0.054114580154418945\n",
      "['340/700']\n",
      "Mean batch loss = 0.06303479075431824\n",
      "['360/700']\n",
      "Mean batch loss = 0.06243264079093933\n",
      "['380/700']\n",
      "Mean batch loss = 0.09378972053527831\n",
      "['400/700']\n",
      "Mean batch loss = 0.060241091251373294\n",
      "['420/700']\n",
      "Mean batch loss = 0.07041308879852295\n",
      "['440/700']\n",
      "Mean batch loss = 0.04632433950901031\n",
      "['460/700']\n",
      "Mean batch loss = 0.05838103294372558\n",
      "['480/700']\n",
      "Mean batch loss = 0.051804757118225096\n",
      "['500/700']\n",
      "Mean batch loss = 0.0565202534198761\n",
      "['520/700']\n",
      "Mean batch loss = 0.09505846500396728\n",
      "['540/700']\n",
      "Mean batch loss = 0.0538409948348999\n",
      "['560/700']\n",
      "Mean batch loss = 0.05100509524345398\n",
      "['580/700']\n",
      "Mean batch loss = 0.07476633787155151\n",
      "['600/700']\n",
      "Mean batch loss = 0.047553449869155884\n",
      "['620/700']\n",
      "Mean batch loss = 0.08186399936676025\n",
      "['640/700']\n",
      "Mean batch loss = 0.053551900386810306\n",
      "['660/700']\n",
      "Mean batch loss = 0.05367146730422974\n",
      "['680/700']\n",
      "Mean batch loss = 0.040082541108131406\n",
      "Pearson on validation = -0.08827320486307144\n",
      "Epoch 11: 0.06445462686674935 // 0.6484848009366542 (19s)\n",
      "['20/700']\n",
      "Mean batch loss = 0.08784152865409851\n",
      "['40/700']\n",
      "Mean batch loss = 0.048276287317276\n",
      "['60/700']\n",
      "Mean batch loss = 0.06158919334411621\n",
      "['80/700']\n",
      "Mean batch loss = 0.05382589697837829\n",
      "['100/700']\n",
      "Mean batch loss = 0.0789265751838684\n",
      "['120/700']\n",
      "Mean batch loss = 0.07016339302062988\n",
      "['140/700']\n",
      "Mean batch loss = 0.04635885953903198\n",
      "['160/700']\n",
      "Mean batch loss = 0.04931064248085022\n",
      "['180/700']\n",
      "Mean batch loss = 0.06133666038513184\n",
      "['200/700']\n",
      "Mean batch loss = 0.07500252723693848\n",
      "['220/700']\n",
      "Mean batch loss = 0.11587505340576172\n",
      "['240/700']\n",
      "Mean batch loss = 0.04523223638534546\n",
      "['260/700']\n",
      "Mean batch loss = 0.03765156865119934\n",
      "['280/700']\n",
      "Mean batch loss = 0.07734680771827698\n",
      "['300/700']\n",
      "Mean batch loss = 0.0723389983177185\n",
      "['320/700']\n",
      "Mean batch loss = 0.06263503432273865\n",
      "['340/700']\n",
      "Mean batch loss = 0.04764722883701324\n",
      "['360/700']\n",
      "Mean batch loss = 0.05393393635749817\n",
      "['380/700']\n",
      "Mean batch loss = 0.03864046931266785\n",
      "['400/700']\n",
      "Mean batch loss = 0.083817458152771\n",
      "['420/700']\n",
      "Mean batch loss = 0.0705777108669281\n",
      "['440/700']\n",
      "Mean batch loss = 0.06131149530410766\n",
      "['460/700']\n",
      "Mean batch loss = 0.04941664040088654\n",
      "['480/700']\n",
      "Mean batch loss = 0.07425267696380615\n",
      "['500/700']\n",
      "Mean batch loss = 0.04371971189975739\n",
      "['520/700']\n",
      "Mean batch loss = 0.0467336118221283\n",
      "['540/700']\n",
      "Mean batch loss = 0.05587029457092285\n",
      "['560/700']\n",
      "Mean batch loss = 0.043978795409202576\n",
      "['580/700']\n",
      "Mean batch loss = 0.05031719207763672\n",
      "['600/700']\n",
      "Mean batch loss = 0.03799557089805603\n",
      "['620/700']\n",
      "Mean batch loss = 0.04566788375377655\n",
      "['640/700']\n",
      "Mean batch loss = 0.043633052706718446\n",
      "['660/700']\n",
      "Mean batch loss = 0.039479497075080874\n",
      "['680/700']\n",
      "Mean batch loss = 0.04678441286087036\n",
      "Pearson on validation = -0.088139608502388\n",
      "Epoch 12: 0.05649968292031969 // 0.6173278683382086 (20s)\n",
      "['20/700']\n",
      "Mean batch loss = 0.053929686546325684\n",
      "['40/700']\n",
      "Mean batch loss = 0.0760883092880249\n",
      "['60/700']\n",
      "Mean batch loss = 0.04534725248813629\n",
      "['80/700']\n",
      "Mean batch loss = 0.03800508081912994\n",
      "['100/700']\n",
      "Mean batch loss = 0.05366384983062744\n",
      "['120/700']\n",
      "Mean batch loss = 0.045029830932617185\n",
      "['140/700']\n",
      "Mean batch loss = 0.07241746187210082\n",
      "['160/700']\n",
      "Mean batch loss = 0.03869695961475372\n",
      "['180/700']\n",
      "Mean batch loss = 0.06183753609657287\n",
      "['200/700']\n",
      "Mean batch loss = 0.05735688209533692\n",
      "['220/700']\n",
      "Mean batch loss = 0.055500376224517825\n",
      "['240/700']\n",
      "Mean batch loss = 0.0644254207611084\n",
      "['260/700']\n",
      "Mean batch loss = 0.04562737047672272\n",
      "['280/700']\n",
      "Mean batch loss = 0.040799316763877866\n",
      "['300/700']\n",
      "Mean batch loss = 0.05420238971710205\n",
      "['320/700']\n",
      "Mean batch loss = 0.03761815726757049\n",
      "['340/700']\n",
      "Mean batch loss = 0.041226187348365785\n",
      "['360/700']\n",
      "Mean batch loss = 0.04275111258029938\n",
      "['380/700']\n",
      "Mean batch loss = 0.05404605865478516\n",
      "['400/700']\n",
      "Mean batch loss = 0.06046097278594971\n",
      "['420/700']\n",
      "Mean batch loss = 0.03745003640651703\n",
      "['440/700']\n",
      "Mean batch loss = 0.07562231421470642\n",
      "['460/700']\n",
      "Mean batch loss = 0.046842840313911435\n",
      "['480/700']\n",
      "Mean batch loss = 0.07376042604446412\n",
      "['500/700']\n",
      "Mean batch loss = 0.04775715470314026\n",
      "['520/700']\n",
      "Mean batch loss = 0.04582177698612213\n",
      "['540/700']\n",
      "Mean batch loss = 0.0323505163192749\n",
      "['560/700']\n",
      "Mean batch loss = 0.038980713486671446\n",
      "['580/700']\n",
      "Mean batch loss = 0.041378527879714966\n",
      "['600/700']\n",
      "Mean batch loss = 0.03734804093837738\n",
      "['620/700']\n",
      "Mean batch loss = 0.032507866621017456\n",
      "['640/700']\n",
      "Mean batch loss = 0.05284773111343384\n",
      "['660/700']\n",
      "Mean batch loss = 0.03705167472362518\n",
      "['680/700']\n",
      "Mean batch loss = 0.04383613169193268\n",
      "Pearson on validation = -0.08776386082172394\n",
      "Epoch 13: 0.04807388467448098 // 0.6103829883374273 (20s)\n",
      "['20/700']\n",
      "Mean batch loss = 0.08331490755081176\n",
      "['40/700']\n",
      "Mean batch loss = 0.05393891334533692\n",
      "['60/700']\n",
      "Mean batch loss = 0.067229962348938\n",
      "['80/700']\n",
      "Mean batch loss = 0.044376987218856814\n",
      "['100/700']\n",
      "Mean batch loss = 0.05033985376358032\n",
      "['120/700']\n",
      "Mean batch loss = 0.037889882922172546\n",
      "['140/700']\n",
      "Mean batch loss = 0.11410646438598633\n",
      "['160/700']\n",
      "Mean batch loss = 0.03464264571666718\n",
      "['180/700']\n",
      "Mean batch loss = 0.05106117129325867\n",
      "['200/700']\n",
      "Mean batch loss = 0.08610601425170898\n",
      "['220/700']\n",
      "Mean batch loss = 0.0659021019935608\n",
      "['240/700']\n",
      "Mean batch loss = 0.04994767904281616\n",
      "['260/700']\n",
      "Mean batch loss = 0.033947494626045224\n",
      "['280/700']\n",
      "Mean batch loss = 0.04828895330429077\n",
      "['300/700']\n",
      "Mean batch loss = 0.042552444338798526\n",
      "['320/700']\n",
      "Mean batch loss = 0.034685438871383666\n",
      "['340/700']\n",
      "Mean batch loss = 0.04537600874900818\n",
      "['360/700']\n",
      "Mean batch loss = 0.04268372654914856\n",
      "['380/700']\n",
      "Mean batch loss = 0.061292266845703124\n",
      "['400/700']\n",
      "Mean batch loss = 0.0488756000995636\n",
      "['420/700']\n",
      "Mean batch loss = 0.08955247402191162\n",
      "['440/700']\n",
      "Mean batch loss = 0.05572904348373413\n",
      "['460/700']\n",
      "Mean batch loss = 0.08200815916061402\n",
      "['480/700']\n",
      "Mean batch loss = 0.05052863359451294\n",
      "['500/700']\n",
      "Mean batch loss = 0.06661519408226013\n",
      "['520/700']\n",
      "Mean batch loss = 0.053642237186431886\n",
      "['540/700']\n",
      "Mean batch loss = 0.0773000717163086\n",
      "['560/700']\n",
      "Mean batch loss = 0.04183445870876312\n",
      "['580/700']\n",
      "Mean batch loss = 0.04496655762195587\n",
      "['600/700']\n",
      "Mean batch loss = 0.04974757134914398\n",
      "['620/700']\n",
      "Mean batch loss = 0.046412518620491026\n",
      "['640/700']\n",
      "Mean batch loss = 0.06770859956741333\n",
      "['660/700']\n",
      "Mean batch loss = 0.0620801568031311\n",
      "['680/700']\n",
      "Mean batch loss = 0.06110908389091492\n",
      "Pearson on validation = -0.08695264160633087\n",
      "Epoch 14: 0.05559409362929208 // 0.6066621848456561 (19s)\n",
      "['20/700']\n",
      "Mean batch loss = 0.0373056560754776\n",
      "['40/700']\n",
      "Mean batch loss = 0.05774111747741699\n",
      "['60/700']\n",
      "Mean batch loss = 0.0508175253868103\n",
      "['80/700']\n",
      "Mean batch loss = 0.05659838914871216\n",
      "['100/700']\n",
      "Mean batch loss = 0.0435651570558548\n",
      "['120/700']\n",
      "Mean batch loss = 0.04299967586994171\n",
      "['140/700']\n",
      "Mean batch loss = 0.05894145369529724\n",
      "['160/700']\n",
      "Mean batch loss = 0.03901718556880951\n",
      "['180/700']\n",
      "Mean batch loss = 0.08324247598648071\n",
      "['200/700']\n",
      "Mean batch loss = 0.034459999203681944\n",
      "['220/700']\n",
      "Mean batch loss = 0.049885261058807376\n",
      "['240/700']\n",
      "Mean batch loss = 0.061846566200256345\n",
      "['260/700']\n",
      "Mean batch loss = 0.0527427077293396\n",
      "['280/700']\n",
      "Mean batch loss = 0.039734256267547605\n",
      "['300/700']\n",
      "Mean batch loss = 0.03636086881160736\n",
      "['320/700']\n",
      "Mean batch loss = 0.0576663613319397\n",
      "['340/700']\n",
      "Mean batch loss = 0.05027186870574951\n",
      "['360/700']\n",
      "Mean batch loss = 0.03679579794406891\n",
      "['380/700']\n",
      "Mean batch loss = 0.07272622585296631\n",
      "['400/700']\n",
      "Mean batch loss = 0.09874240159988404\n",
      "['420/700']\n",
      "Mean batch loss = 0.04856120347976685\n",
      "['440/700']\n",
      "Mean batch loss = 0.0438627690076828\n",
      "['460/700']\n",
      "Mean batch loss = 0.0583882212638855\n",
      "['480/700']\n",
      "Mean batch loss = 0.04619100093841553\n",
      "['500/700']\n",
      "Mean batch loss = 0.047522449493408205\n",
      "['520/700']\n",
      "Mean batch loss = 0.05103607773780823\n",
      "['540/700']\n",
      "Mean batch loss = 0.033567997813224795\n",
      "['560/700']\n",
      "Mean batch loss = 0.054803085327148435\n",
      "['580/700']\n",
      "Mean batch loss = 0.04867359697818756\n",
      "['600/700']\n",
      "Mean batch loss = 0.03882015347480774\n",
      "['620/700']\n",
      "Mean batch loss = 0.04078886210918427\n",
      "['640/700']\n",
      "Mean batch loss = 0.04725436270236969\n",
      "['660/700']\n",
      "Mean batch loss = 0.04217865765094757\n",
      "['680/700']\n",
      "Mean batch loss = 0.04187742471694946\n",
      "Pearson on validation = -0.08612900227308273\n",
      "Epoch 15: 0.04871390896184104 // 0.6019161432813853 (19s)\n",
      "['20/700']\n",
      "Mean batch loss = 0.08212002515792846\n",
      "['40/700']\n",
      "Mean batch loss = 0.08541357517242432\n",
      "['60/700']\n",
      "Mean batch loss = 0.06758447885513305\n",
      "['80/700']\n",
      "Mean batch loss = 0.055430471897125244\n",
      "['100/700']\n",
      "Mean batch loss = 0.03724709153175354\n",
      "['120/700']\n",
      "Mean batch loss = 0.06063501834869385\n",
      "['140/700']\n",
      "Mean batch loss = 0.04033251106739044\n",
      "['160/700']\n",
      "Mean batch loss = 0.05220478177070618\n",
      "['180/700']\n",
      "Mean batch loss = 0.056483328342437744\n",
      "['200/700']\n",
      "Mean batch loss = 0.07156823873519898\n",
      "['220/700']\n",
      "Mean batch loss = 0.05782748460769653\n",
      "['240/700']\n",
      "Mean batch loss = 0.03833949565887451\n",
      "['260/700']\n",
      "Mean batch loss = 0.05186314582824707\n",
      "['280/700']\n",
      "Mean batch loss = 0.05781900882720947\n",
      "['300/700']\n",
      "Mean batch loss = 0.04290066361427307\n",
      "['320/700']\n",
      "Mean batch loss = 0.0524806022644043\n",
      "['340/700']\n",
      "Mean batch loss = 0.047004294395446775\n",
      "['360/700']\n",
      "Mean batch loss = 0.0870544970035553\n",
      "['380/700']\n",
      "Mean batch loss = 0.033059191703796384\n",
      "['400/700']\n",
      "Mean batch loss = 0.08037109971046448\n",
      "['420/700']\n",
      "Mean batch loss = 0.04036097526550293\n",
      "['440/700']\n",
      "Mean batch loss = 0.05588191747665405\n",
      "['460/700']\n",
      "Mean batch loss = 0.0415829211473465\n",
      "['480/700']\n",
      "Mean batch loss = 0.0965528666973114\n",
      "['500/700']\n",
      "Mean batch loss = 0.07588663101196289\n",
      "['520/700']\n",
      "Mean batch loss = 0.05163251757621765\n",
      "['540/700']\n",
      "Mean batch loss = 0.04054384231567383\n",
      "['560/700']\n",
      "Mean batch loss = 0.04954950213432312\n",
      "['580/700']\n",
      "Mean batch loss = 0.061896657943725585\n",
      "['600/700']\n",
      "Mean batch loss = 0.07008748650550842\n",
      "['620/700']\n",
      "Mean batch loss = 0.04835812449455261\n",
      "['640/700']\n",
      "Mean batch loss = 0.05222781896591187\n",
      "['660/700']\n",
      "Mean batch loss = 0.05981703400611878\n",
      "['680/700']\n",
      "Mean batch loss = 0.05825469493865967\n",
      "Pearson on validation = -0.08518906682729721\n",
      "Epoch 16: 0.05601062842777797 // 0.5988938544872217 (20s)\n"
     ]
    }
   ],
   "source": [
    "######################################################################### TRAINING LOOP  ####################################################################################\n",
    "total_losses = []\n",
    "test_losses = []\n",
    "pears = []\n",
    "for i in range(num_epochs):\n",
    "    start = time.time()\n",
    "    loss = train()\n",
    "    test_loss, p = test()\n",
    "    pears.append(p.item())\n",
    "    elapsed = time.time() - start\n",
    "    print(\"Epoch \" + str(i) + \": \" + str(loss) + \" // \" + str(test_loss) + \" (\" + str(int(elapsed)) + \"s)\")\n",
    "    total_losses.append(loss)\n",
    "    test_losses.append(test_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 809
    },
    "colab_type": "code",
    "id": "ELhcpkGh9skM",
    "outputId": "e356cf39-8cd6-4b73-b1ff-be22e98b6b4a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAEICAYAAAB25L6yAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwqmoRUQSTkUtIpJw/wOpmL23b0yH2wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEICAYAAABS0fM3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRCTPqRCIiOQ5FQIRkTz3/wFp8Ms5dYomdwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(total_losses)\n",
    "plt.title(\"Training error\")\n",
    "plt.grid()\n",
    "plt.figure()\n",
    "plt.plot(test_losses, color='orange')\n",
    "plt.title(\"Test error\")\n",
    "plt.grid()\n",
    "plt.figure()\n",
    "plt.plot(pears, color='red')\n",
    "plt.title(\"Pearson\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 562
    },
    "colab_type": "code",
    "id": "3quqvBj5KiFe",
    "outputId": "a327360b-eaa5-4353-c449-ea2e9400d0f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.08518907013045125\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXMAAAEICAYAAACtXxSQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXMAAAEICAYAAACtXxSQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "######################################################################### EVALUATION  ####################################################################################\n",
    "rnn.eval()\n",
    "outputs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    prediction = []\n",
    "    true_score = []\n",
    "    \n",
    "    for i in range(len(X_val)):\n",
    "        en_tensor_line, de_tensor_line = sentence_to_tensors(i, X_val)\n",
    "        output = test_line(en_tensor_line, de_tensor_line)\n",
    "        output = output*(max_ - min_) + min_\n",
    "        outputs.append(output.cpu())\n",
    "        prediction.append(output.item())\n",
    "        true_score.append(val_scores_tensor[i].item())\n",
    "\n",
    "from scipy.stats.stats import pearsonr\n",
    "pearson = pearsonr(true_score, prediction)\n",
    "print(pearson[0])\n",
    "\n",
    "plt.figure()\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-4,4])\n",
    "axes.set_ylim([0,60])\n",
    "\n",
    "plt.hist(outputs, bins=100)\n",
    "plt.title(\"Predicted\")\n",
    "\n",
    "plt.figure()\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-4,4])\n",
    "axes.set_ylim([0,60])\n",
    "plt.hist(val_scores_tensor, bins=200)\n",
    "plt.title(\"Actual\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "G5VD_dNK-7IE",
    "outputId": "5efc9f70-c0af-47e2-a0d8-7c1046b78828"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-0.0630]]),\n",
       " tensor([[0.0978]]),\n",
       " tensor([[0.5928]]),\n",
       " tensor([[-0.3818]]),\n",
       " tensor([[0.9105]]),\n",
       " tensor([[0.7072]]),\n",
       " tensor([[-0.0905]]),\n",
       " tensor([[0.4197]]),\n",
       " tensor([[0.0281]]),\n",
       " tensor([[0.2324]]),\n",
       " tensor([[-0.0842]]),\n",
       " tensor([[-0.2176]]),\n",
       " tensor([[0.3930]]),\n",
       " tensor([[0.8251]]),\n",
       " tensor([[0.1971]]),\n",
       " tensor([[-0.1629]]),\n",
       " tensor([[-0.0549]]),\n",
       " tensor([[0.5516]]),\n",
       " tensor([[-0.0250]]),\n",
       " tensor([[-0.0982]]),\n",
       " tensor([[0.2976]]),\n",
       " tensor([[-0.0974]]),\n",
       " tensor([[-0.3254]]),\n",
       " tensor([[-0.2111]]),\n",
       " tensor([[0.0045]]),\n",
       " tensor([[0.4345]]),\n",
       " tensor([[0.3810]]),\n",
       " tensor([[-0.2827]]),\n",
       " tensor([[0.1807]]),\n",
       " tensor([[0.3955]]),\n",
       " tensor([[-0.0020]]),\n",
       " tensor([[0.2785]]),\n",
       " tensor([[-0.1174]]),\n",
       " tensor([[0.5559]]),\n",
       " tensor([[0.4571]]),\n",
       " tensor([[0.0941]]),\n",
       " tensor([[-0.3103]]),\n",
       " tensor([[0.2516]]),\n",
       " tensor([[-0.2472]]),\n",
       " tensor([[-0.4212]]),\n",
       " tensor([[0.1811]]),\n",
       " tensor([[-0.2610]]),\n",
       " tensor([[-0.0557]]),\n",
       " tensor([[-0.0842]]),\n",
       " tensor([[-0.5116]]),\n",
       " tensor([[0.0733]]),\n",
       " tensor([[0.3001]]),\n",
       " tensor([[-0.2923]]),\n",
       " tensor([[0.3883]]),\n",
       " tensor([[-0.2887]]),\n",
       " tensor([[-0.0106]]),\n",
       " tensor([[-0.0223]]),\n",
       " tensor([[-0.1516]]),\n",
       " tensor([[-0.4394]]),\n",
       " tensor([[0.1084]]),\n",
       " tensor([[0.2994]]),\n",
       " tensor([[0.1393]]),\n",
       " tensor([[0.3030]]),\n",
       " tensor([[-0.3791]]),\n",
       " tensor([[0.1654]]),\n",
       " tensor([[0.1560]]),\n",
       " tensor([[0.6076]]),\n",
       " tensor([[0.2585]]),\n",
       " tensor([[0.4089]]),\n",
       " tensor([[0.0337]]),\n",
       " tensor([[0.2280]]),\n",
       " tensor([[0.5489]]),\n",
       " tensor([[-0.0801]]),\n",
       " tensor([[0.3852]]),\n",
       " tensor([[0.1346]]),\n",
       " tensor([[0.1404]]),\n",
       " tensor([[-0.1556]]),\n",
       " tensor([[-0.3568]]),\n",
       " tensor([[0.2711]]),\n",
       " tensor([[0.1442]]),\n",
       " tensor([[-0.5325]]),\n",
       " tensor([[0.5813]]),\n",
       " tensor([[0.0463]]),\n",
       " tensor([[-0.1174]]),\n",
       " tensor([[0.1594]]),\n",
       " tensor([[0.0306]]),\n",
       " tensor([[-0.1496]]),\n",
       " tensor([[-0.2350]]),\n",
       " tensor([[-0.1807]]),\n",
       " tensor([[0.1635]]),\n",
       " tensor([[-0.3443]]),\n",
       " tensor([[0.4540]]),\n",
       " tensor([[-0.0079]]),\n",
       " tensor([[0.9547]]),\n",
       " tensor([[0.1685]]),\n",
       " tensor([[0.1421]]),\n",
       " tensor([[0.4952]]),\n",
       " tensor([[0.8196]]),\n",
       " tensor([[0.5003]]),\n",
       " tensor([[0.0452]]),\n",
       " tensor([[-0.0927]]),\n",
       " tensor([[-0.1435]]),\n",
       " tensor([[0.6281]]),\n",
       " tensor([[0.0958]]),\n",
       " tensor([[-0.1499]]),\n",
       " tensor([[-0.2054]]),\n",
       " tensor([[-0.3079]]),\n",
       " tensor([[0.1834]]),\n",
       " tensor([[-0.1098]]),\n",
       " tensor([[0.3049]]),\n",
       " tensor([[-0.3527]]),\n",
       " tensor([[0.2781]]),\n",
       " tensor([[0.2571]]),\n",
       " tensor([[0.3541]]),\n",
       " tensor([[0.3334]]),\n",
       " tensor([[-0.5698]]),\n",
       " tensor([[-0.4944]]),\n",
       " tensor([[-0.1614]]),\n",
       " tensor([[0.1219]]),\n",
       " tensor([[0.0142]]),\n",
       " tensor([[0.0600]]),\n",
       " tensor([[0.3167]]),\n",
       " tensor([[-0.2814]]),\n",
       " tensor([[0.0259]]),\n",
       " tensor([[0.3988]]),\n",
       " tensor([[-0.4486]]),\n",
       " tensor([[0.1096]]),\n",
       " tensor([[-0.0778]]),\n",
       " tensor([[0.2774]]),\n",
       " tensor([[-0.0979]]),\n",
       " tensor([[0.3592]]),\n",
       " tensor([[0.0379]]),\n",
       " tensor([[0.0928]]),\n",
       " tensor([[0.1948]]),\n",
       " tensor([[-0.4225]]),\n",
       " tensor([[-0.0045]]),\n",
       " tensor([[0.0516]]),\n",
       " tensor([[-0.0356]]),\n",
       " tensor([[0.0543]]),\n",
       " tensor([[0.5016]]),\n",
       " tensor([[0.3746]]),\n",
       " tensor([[0.6305]]),\n",
       " tensor([[0.4383]]),\n",
       " tensor([[-0.0802]]),\n",
       " tensor([[0.2929]]),\n",
       " tensor([[0.0787]]),\n",
       " tensor([[0.2895]]),\n",
       " tensor([[0.0751]]),\n",
       " tensor([[-0.3581]]),\n",
       " tensor([[-0.0871]]),\n",
       " tensor([[0.6452]]),\n",
       " tensor([[0.9802]]),\n",
       " tensor([[0.3101]]),\n",
       " tensor([[-0.2865]]),\n",
       " tensor([[0.3579]]),\n",
       " tensor([[0.1892]]),\n",
       " tensor([[-0.4164]]),\n",
       " tensor([[0.0018]]),\n",
       " tensor([[0.4604]]),\n",
       " tensor([[-0.1635]]),\n",
       " tensor([[0.0988]]),\n",
       " tensor([[-0.2703]]),\n",
       " tensor([[-0.0997]]),\n",
       " tensor([[-0.1501]]),\n",
       " tensor([[0.1689]]),\n",
       " tensor([[-0.3332]]),\n",
       " tensor([[0.3007]]),\n",
       " tensor([[0.1758]]),\n",
       " tensor([[0.0357]]),\n",
       " tensor([[0.1558]]),\n",
       " tensor([[0.7221]]),\n",
       " tensor([[-0.3689]]),\n",
       " tensor([[0.8142]]),\n",
       " tensor([[-0.1251]]),\n",
       " tensor([[-0.1313]]),\n",
       " tensor([[0.0987]]),\n",
       " tensor([[0.4010]]),\n",
       " tensor([[1.0337]]),\n",
       " tensor([[0.0998]]),\n",
       " tensor([[0.1647]]),\n",
       " tensor([[0.0945]]),\n",
       " tensor([[0.2125]]),\n",
       " tensor([[0.7167]]),\n",
       " tensor([[0.0248]]),\n",
       " tensor([[0.7019]]),\n",
       " tensor([[-0.1245]]),\n",
       " tensor([[0.7559]]),\n",
       " tensor([[-0.4671]]),\n",
       " tensor([[-0.2063]]),\n",
       " tensor([[-0.3576]]),\n",
       " tensor([[-0.0997]]),\n",
       " tensor([[-0.5793]]),\n",
       " tensor([[0.3878]]),\n",
       " tensor([[0.0344]]),\n",
       " tensor([[0.0404]]),\n",
       " tensor([[0.1320]]),\n",
       " tensor([[-0.3824]]),\n",
       " tensor([[0.7566]]),\n",
       " tensor([[0.6558]]),\n",
       " tensor([[0.0398]]),\n",
       " tensor([[-0.1382]]),\n",
       " tensor([[-0.3787]]),\n",
       " tensor([[-0.0404]]),\n",
       " tensor([[0.0342]]),\n",
       " tensor([[0.2395]]),\n",
       " tensor([[0.2265]]),\n",
       " tensor([[-0.2438]]),\n",
       " tensor([[0.1912]]),\n",
       " tensor([[0.0711]]),\n",
       " tensor([[0.3252]]),\n",
       " tensor([[0.3763]]),\n",
       " tensor([[-0.4090]]),\n",
       " tensor([[-0.2253]]),\n",
       " tensor([[-0.0868]]),\n",
       " tensor([[-0.2788]]),\n",
       " tensor([[-0.0487]]),\n",
       " tensor([[-0.0280]]),\n",
       " tensor([[0.5898]]),\n",
       " tensor([[0.2322]]),\n",
       " tensor([[-0.2402]]),\n",
       " tensor([[-0.0686]]),\n",
       " tensor([[-0.1938]]),\n",
       " tensor([[-0.2003]]),\n",
       " tensor([[0.2908]]),\n",
       " tensor([[-0.0993]]),\n",
       " tensor([[0.5484]]),\n",
       " tensor([[-0.0010]]),\n",
       " tensor([[-0.3776]]),\n",
       " tensor([[0.3762]]),\n",
       " tensor([[0.0884]]),\n",
       " tensor([[0.0562]]),\n",
       " tensor([[0.0129]]),\n",
       " tensor([[0.2416]]),\n",
       " tensor([[0.2944]]),\n",
       " tensor([[0.0206]]),\n",
       " tensor([[-0.1373]]),\n",
       " tensor([[0.1403]]),\n",
       " tensor([[0.0241]]),\n",
       " tensor([[0.4495]]),\n",
       " tensor([[0.6125]]),\n",
       " tensor([[-0.3529]]),\n",
       " tensor([[0.5125]]),\n",
       " tensor([[-0.1486]]),\n",
       " tensor([[0.5032]]),\n",
       " tensor([[0.0807]]),\n",
       " tensor([[0.8935]]),\n",
       " tensor([[0.0388]]),\n",
       " tensor([[-0.1933]]),\n",
       " tensor([[0.7708]]),\n",
       " tensor([[0.0835]]),\n",
       " tensor([[-0.0005]]),\n",
       " tensor([[0.6836]]),\n",
       " tensor([[0.5622]]),\n",
       " tensor([[0.3992]]),\n",
       " tensor([[-0.0956]]),\n",
       " tensor([[-0.3421]]),\n",
       " tensor([[0.1735]]),\n",
       " tensor([[-0.3629]]),\n",
       " tensor([[0.4191]]),\n",
       " tensor([[-0.4404]]),\n",
       " tensor([[0.6299]]),\n",
       " tensor([[0.1055]]),\n",
       " tensor([[0.1777]]),\n",
       " tensor([[0.2623]]),\n",
       " tensor([[0.0515]]),\n",
       " tensor([[-0.3414]]),\n",
       " tensor([[0.1755]]),\n",
       " tensor([[0.2423]]),\n",
       " tensor([[0.1410]]),\n",
       " tensor([[-0.1395]]),\n",
       " tensor([[-0.5869]]),\n",
       " tensor([[0.2382]]),\n",
       " tensor([[-0.2379]]),\n",
       " tensor([[0.1362]]),\n",
       " tensor([[0.3763]]),\n",
       " tensor([[-0.4424]]),\n",
       " tensor([[-0.1728]]),\n",
       " tensor([[0.2352]]),\n",
       " tensor([[-0.2081]]),\n",
       " tensor([[0.1556]]),\n",
       " tensor([[-0.0779]]),\n",
       " tensor([[0.0839]]),\n",
       " tensor([[-0.0703]]),\n",
       " tensor([[0.2954]]),\n",
       " tensor([[0.2175]]),\n",
       " tensor([[0.1746]]),\n",
       " tensor([[-0.3071]]),\n",
       " tensor([[0.4744]]),\n",
       " tensor([[0.1372]]),\n",
       " tensor([[-0.1316]]),\n",
       " tensor([[0.0089]]),\n",
       " tensor([[-0.1569]]),\n",
       " tensor([[-0.0749]]),\n",
       " tensor([[0.0941]]),\n",
       " tensor([[-0.3550]]),\n",
       " tensor([[0.3191]]),\n",
       " tensor([[0.1728]]),\n",
       " tensor([[-0.1110]]),\n",
       " tensor([[0.3669]]),\n",
       " tensor([[-0.1405]]),\n",
       " tensor([[0.1956]]),\n",
       " tensor([[0.9409]]),\n",
       " tensor([[-0.0530]]),\n",
       " tensor([[0.1989]]),\n",
       " tensor([[-0.3553]]),\n",
       " tensor([[0.0856]]),\n",
       " tensor([[0.1834]]),\n",
       " tensor([[0.1573]]),\n",
       " tensor([[0.2352]]),\n",
       " tensor([[-0.0309]]),\n",
       " tensor([[-0.2009]]),\n",
       " tensor([[0.1110]]),\n",
       " tensor([[0.2017]]),\n",
       " tensor([[0.2310]]),\n",
       " tensor([[0.5180]]),\n",
       " tensor([[0.3491]]),\n",
       " tensor([[0.2657]]),\n",
       " tensor([[0.2074]]),\n",
       " tensor([[0.1935]]),\n",
       " tensor([[-0.4776]]),\n",
       " tensor([[0.0234]]),\n",
       " tensor([[-0.4791]]),\n",
       " tensor([[-0.4555]]),\n",
       " tensor([[-0.1283]]),\n",
       " tensor([[-0.2231]]),\n",
       " tensor([[0.2844]]),\n",
       " tensor([[0.1032]]),\n",
       " tensor([[-0.0461]]),\n",
       " tensor([[0.3043]]),\n",
       " tensor([[-0.1695]]),\n",
       " tensor([[0.1768]]),\n",
       " tensor([[-0.1829]]),\n",
       " tensor([[0.2688]]),\n",
       " tensor([[0.2698]]),\n",
       " tensor([[0.5454]]),\n",
       " tensor([[0.0369]]),\n",
       " tensor([[-0.1067]]),\n",
       " tensor([[-0.1095]]),\n",
       " tensor([[-0.0538]]),\n",
       " tensor([[-0.2001]]),\n",
       " tensor([[0.0831]]),\n",
       " tensor([[0.2792]]),\n",
       " tensor([[-0.1341]]),\n",
       " tensor([[0.0246]]),\n",
       " tensor([[0.2738]]),\n",
       " tensor([[0.2980]]),\n",
       " tensor([[0.3322]]),\n",
       " tensor([[0.5891]]),\n",
       " tensor([[0.0845]]),\n",
       " tensor([[0.3066]]),\n",
       " tensor([[-0.1336]]),\n",
       " tensor([[0.2363]]),\n",
       " tensor([[0.3478]]),\n",
       " tensor([[0.3475]]),\n",
       " tensor([[-0.3964]]),\n",
       " tensor([[-0.3066]]),\n",
       " tensor([[0.2271]]),\n",
       " tensor([[0.3893]]),\n",
       " tensor([[0.3862]]),\n",
       " tensor([[-0.3408]]),\n",
       " tensor([[0.3724]]),\n",
       " tensor([[-0.1564]]),\n",
       " tensor([[-0.4049]]),\n",
       " tensor([[-0.1292]]),\n",
       " tensor([[0.3191]]),\n",
       " tensor([[0.1700]]),\n",
       " tensor([[-0.3013]]),\n",
       " tensor([[0.4390]]),\n",
       " tensor([[0.2529]]),\n",
       " tensor([[-0.1460]]),\n",
       " tensor([[0.1748]]),\n",
       " tensor([[0.2868]]),\n",
       " tensor([[0.1061]]),\n",
       " tensor([[-0.2109]]),\n",
       " tensor([[-0.0785]]),\n",
       " tensor([[-0.0239]]),\n",
       " tensor([[-0.4018]]),\n",
       " tensor([[0.7508]]),\n",
       " tensor([[0.0335]]),\n",
       " tensor([[0.4667]]),\n",
       " tensor([[1.1214]]),\n",
       " tensor([[-0.1245]]),\n",
       " tensor([[0.0886]]),\n",
       " tensor([[-0.4693]]),\n",
       " tensor([[-0.4784]]),\n",
       " tensor([[0.1759]]),\n",
       " tensor([[0.4609]]),\n",
       " tensor([[0.0285]]),\n",
       " tensor([[-0.0698]]),\n",
       " tensor([[-0.0829]]),\n",
       " tensor([[-0.0226]]),\n",
       " tensor([[0.2666]]),\n",
       " tensor([[-0.0104]]),\n",
       " tensor([[-0.2133]]),\n",
       " tensor([[-0.0317]]),\n",
       " tensor([[-0.1603]]),\n",
       " tensor([[0.0400]]),\n",
       " tensor([[0.2199]]),\n",
       " tensor([[0.7451]]),\n",
       " tensor([[-0.0872]]),\n",
       " tensor([[-0.1498]]),\n",
       " tensor([[0.1017]]),\n",
       " tensor([[-0.0422]]),\n",
       " tensor([[0.6105]]),\n",
       " tensor([[0.1951]]),\n",
       " tensor([[-0.4111]]),\n",
       " tensor([[-0.4257]]),\n",
       " tensor([[0.5801]]),\n",
       " tensor([[0.1317]]),\n",
       " tensor([[0.0213]]),\n",
       " tensor([[0.1112]]),\n",
       " tensor([[-0.4838]]),\n",
       " tensor([[-0.0616]]),\n",
       " tensor([[-0.2061]]),\n",
       " tensor([[0.3698]]),\n",
       " tensor([[0.4635]]),\n",
       " tensor([[-0.1232]]),\n",
       " tensor([[-0.2915]]),\n",
       " tensor([[-0.4458]]),\n",
       " tensor([[0.1673]]),\n",
       " tensor([[-0.0670]]),\n",
       " tensor([[0.0515]]),\n",
       " tensor([[-0.4019]]),\n",
       " tensor([[0.4195]]),\n",
       " tensor([[-0.3547]]),\n",
       " tensor([[0.7479]]),\n",
       " tensor([[-0.4227]]),\n",
       " tensor([[0.7829]]),\n",
       " tensor([[0.4954]]),\n",
       " tensor([[0.1323]]),\n",
       " tensor([[0.4614]]),\n",
       " tensor([[-0.1168]]),\n",
       " tensor([[-0.2559]]),\n",
       " tensor([[-0.1928]]),\n",
       " tensor([[0.3023]]),\n",
       " tensor([[0.5925]]),\n",
       " tensor([[-0.3286]]),\n",
       " tensor([[-0.0959]]),\n",
       " tensor([[0.0450]]),\n",
       " tensor([[0.1259]]),\n",
       " tensor([[0.1824]]),\n",
       " tensor([[0.0832]]),\n",
       " tensor([[-0.1139]]),\n",
       " tensor([[0.2208]]),\n",
       " tensor([[0.0537]]),\n",
       " tensor([[0.2681]]),\n",
       " tensor([[0.5852]]),\n",
       " tensor([[-0.1280]]),\n",
       " tensor([[0.0420]]),\n",
       " tensor([[-0.2814]]),\n",
       " tensor([[-0.3810]]),\n",
       " tensor([[0.1803]]),\n",
       " tensor([[-0.3403]]),\n",
       " tensor([[0.0192]]),\n",
       " tensor([[-0.1715]]),\n",
       " tensor([[0.3863]]),\n",
       " tensor([[0.4297]]),\n",
       " tensor([[0.2802]]),\n",
       " tensor([[-0.0395]]),\n",
       " tensor([[-0.2101]]),\n",
       " tensor([[-0.2216]]),\n",
       " tensor([[0.5327]]),\n",
       " tensor([[0.3376]]),\n",
       " tensor([[0.2710]]),\n",
       " tensor([[0.0296]]),\n",
       " tensor([[-0.1821]]),\n",
       " tensor([[0.1511]]),\n",
       " tensor([[0.2167]]),\n",
       " tensor([[-0.2498]]),\n",
       " tensor([[0.4894]]),\n",
       " tensor([[0.5402]]),\n",
       " tensor([[0.0310]]),\n",
       " tensor([[-0.2799]]),\n",
       " tensor([[0.7242]]),\n",
       " tensor([[-0.0371]]),\n",
       " tensor([[0.0594]]),\n",
       " tensor([[0.5348]]),\n",
       " tensor([[0.1632]]),\n",
       " tensor([[0.2804]]),\n",
       " tensor([[-0.0489]]),\n",
       " tensor([[0.1630]]),\n",
       " tensor([[0.3324]]),\n",
       " tensor([[-0.2955]]),\n",
       " tensor([[-0.0075]]),\n",
       " tensor([[-0.2625]]),\n",
       " tensor([[0.6254]]),\n",
       " tensor([[0.3943]]),\n",
       " tensor([[-0.4538]]),\n",
       " tensor([[-0.1620]]),\n",
       " tensor([[0.5783]]),\n",
       " tensor([[0.6893]]),\n",
       " tensor([[-0.2631]]),\n",
       " tensor([[0.1641]]),\n",
       " tensor([[0.1320]]),\n",
       " tensor([[0.1740]]),\n",
       " tensor([[0.3285]]),\n",
       " tensor([[0.1635]]),\n",
       " tensor([[0.1481]]),\n",
       " tensor([[-0.0799]]),\n",
       " tensor([[0.4743]]),\n",
       " tensor([[0.1862]]),\n",
       " tensor([[0.0176]]),\n",
       " tensor([[-0.1607]]),\n",
       " tensor([[-0.1184]]),\n",
       " tensor([[0.2733]]),\n",
       " tensor([[-0.0269]]),\n",
       " tensor([[0.5261]]),\n",
       " tensor([[0.5068]]),\n",
       " tensor([[-0.0961]]),\n",
       " tensor([[0.4606]]),\n",
       " tensor([[0.1333]]),\n",
       " tensor([[0.3125]]),\n",
       " tensor([[0.4904]]),\n",
       " tensor([[-0.2538]]),\n",
       " tensor([[0.1287]]),\n",
       " tensor([[-0.2159]]),\n",
       " tensor([[0.7051]]),\n",
       " tensor([[0.1509]]),\n",
       " tensor([[0.3787]]),\n",
       " tensor([[0.9959]]),\n",
       " tensor([[-0.0987]]),\n",
       " tensor([[-0.3002]]),\n",
       " tensor([[0.1518]]),\n",
       " tensor([[-0.2360]]),\n",
       " tensor([[0.3529]]),\n",
       " tensor([[-0.0496]]),\n",
       " tensor([[-0.0469]]),\n",
       " tensor([[-0.0595]]),\n",
       " tensor([[-0.0749]]),\n",
       " tensor([[0.1069]]),\n",
       " tensor([[-0.3891]]),\n",
       " tensor([[-0.3027]]),\n",
       " tensor([[0.4616]]),\n",
       " tensor([[-0.1565]]),\n",
       " tensor([[0.3308]]),\n",
       " tensor([[0.2288]]),\n",
       " tensor([[-0.1887]]),\n",
       " tensor([[0.8837]]),\n",
       " tensor([[0.0385]]),\n",
       " tensor([[0.6943]]),\n",
       " tensor([[0.0979]]),\n",
       " tensor([[-0.2132]]),\n",
       " tensor([[0.4119]]),\n",
       " tensor([[0.0383]]),\n",
       " tensor([[0.2375]]),\n",
       " tensor([[0.1283]]),\n",
       " tensor([[0.3518]]),\n",
       " tensor([[0.8475]]),\n",
       " tensor([[-0.1006]]),\n",
       " tensor([[-0.2993]]),\n",
       " tensor([[0.4854]]),\n",
       " tensor([[-0.0099]]),\n",
       " tensor([[-0.2449]]),\n",
       " tensor([[0.3562]]),\n",
       " tensor([[-0.0675]]),\n",
       " tensor([[0.4220]]),\n",
       " tensor([[0.0649]]),\n",
       " tensor([[0.0440]]),\n",
       " tensor([[-0.1364]]),\n",
       " tensor([[-0.0064]]),\n",
       " tensor([[0.0788]]),\n",
       " tensor([[0.3639]]),\n",
       " tensor([[0.3096]]),\n",
       " tensor([[-0.0926]]),\n",
       " tensor([[0.2430]]),\n",
       " tensor([[0.1638]]),\n",
       " tensor([[0.2926]]),\n",
       " tensor([[-0.0330]]),\n",
       " tensor([[0.1700]]),\n",
       " tensor([[-0.2947]]),\n",
       " tensor([[0.7110]]),\n",
       " tensor([[0.1744]]),\n",
       " tensor([[0.0543]]),\n",
       " tensor([[-0.1269]]),\n",
       " tensor([[0.5061]]),\n",
       " tensor([[0.2500]]),\n",
       " tensor([[0.2173]]),\n",
       " tensor([[-0.2891]]),\n",
       " tensor([[0.1738]]),\n",
       " tensor([[0.2129]]),\n",
       " tensor([[0.1807]]),\n",
       " tensor([[0.2563]]),\n",
       " tensor([[0.7718]]),\n",
       " tensor([[0.3794]]),\n",
       " tensor([[0.1764]]),\n",
       " tensor([[0.0173]]),\n",
       " tensor([[0.2629]]),\n",
       " tensor([[-0.0576]]),\n",
       " tensor([[0.0454]]),\n",
       " tensor([[-0.0368]]),\n",
       " tensor([[-0.2758]]),\n",
       " tensor([[0.1748]]),\n",
       " tensor([[-0.4109]]),\n",
       " tensor([[0.4257]]),\n",
       " tensor([[0.0657]]),\n",
       " tensor([[0.7328]]),\n",
       " tensor([[-0.0357]]),\n",
       " tensor([[0.0153]]),\n",
       " tensor([[0.0019]]),\n",
       " tensor([[0.1781]]),\n",
       " tensor([[0.3219]]),\n",
       " tensor([[0.0816]]),\n",
       " tensor([[0.2254]]),\n",
       " tensor([[-0.1035]]),\n",
       " tensor([[0.2916]]),\n",
       " tensor([[0.2574]]),\n",
       " tensor([[0.4404]]),\n",
       " tensor([[-0.0239]]),\n",
       " tensor([[-0.2473]]),\n",
       " tensor([[-0.2178]]),\n",
       " tensor([[0.2780]]),\n",
       " tensor([[-0.0003]]),\n",
       " tensor([[-0.1584]]),\n",
       " tensor([[-0.1393]]),\n",
       " tensor([[0.8375]]),\n",
       " tensor([[-0.2074]]),\n",
       " tensor([[-0.1648]]),\n",
       " tensor([[-0.0160]]),\n",
       " tensor([[0.0663]]),\n",
       " tensor([[0.7462]]),\n",
       " tensor([[0.2893]]),\n",
       " tensor([[-0.0321]]),\n",
       " tensor([[0.0058]]),\n",
       " tensor([[0.2635]]),\n",
       " tensor([[-0.1915]]),\n",
       " tensor([[-0.4538]]),\n",
       " tensor([[0.2593]]),\n",
       " tensor([[0.5988]]),\n",
       " tensor([[0.1287]]),\n",
       " tensor([[-0.4094]]),\n",
       " tensor([[-0.2851]]),\n",
       " tensor([[0.4120]]),\n",
       " tensor([[0.2094]]),\n",
       " tensor([[-0.1406]]),\n",
       " tensor([[0.5255]]),\n",
       " tensor([[0.7888]]),\n",
       " tensor([[-0.3634]]),\n",
       " tensor([[0.1094]]),\n",
       " tensor([[0.0619]]),\n",
       " tensor([[0.6835]]),\n",
       " tensor([[-0.0105]]),\n",
       " tensor([[-0.2069]]),\n",
       " tensor([[-0.1265]]),\n",
       " tensor([[0.5612]]),\n",
       " tensor([[-0.1548]]),\n",
       " tensor([[-0.2849]]),\n",
       " tensor([[0.0530]]),\n",
       " tensor([[0.0492]]),\n",
       " tensor([[0.0741]]),\n",
       " tensor([[-0.1282]]),\n",
       " tensor([[0.1134]]),\n",
       " tensor([[-0.0559]]),\n",
       " tensor([[-0.4547]]),\n",
       " tensor([[0.0867]]),\n",
       " tensor([[-0.2317]]),\n",
       " tensor([[0.2981]]),\n",
       " tensor([[-0.2507]]),\n",
       " tensor([[0.1823]]),\n",
       " tensor([[0.0395]]),\n",
       " tensor([[0.3046]]),\n",
       " tensor([[0.0725]]),\n",
       " tensor([[0.6930]]),\n",
       " tensor([[1.0648]]),\n",
       " tensor([[0.5979]]),\n",
       " tensor([[0.1794]]),\n",
       " tensor([[0.1956]]),\n",
       " tensor([[0.5614]]),\n",
       " tensor([[-0.2258]]),\n",
       " tensor([[-0.2506]]),\n",
       " tensor([[0.3303]]),\n",
       " tensor([[0.0206]]),\n",
       " tensor([[-0.2808]]),\n",
       " tensor([[0.3879]]),\n",
       " tensor([[-0.2936]]),\n",
       " tensor([[-0.0665]]),\n",
       " tensor([[0.1770]]),\n",
       " tensor([[-0.2608]]),\n",
       " tensor([[0.5339]]),\n",
       " tensor([[-0.1921]]),\n",
       " tensor([[-0.3273]]),\n",
       " tensor([[0.7975]]),\n",
       " tensor([[-0.3626]]),\n",
       " tensor([[0.0690]]),\n",
       " tensor([[0.3555]]),\n",
       " tensor([[-0.2177]]),\n",
       " tensor([[0.2665]]),\n",
       " tensor([[0.0564]]),\n",
       " tensor([[-0.1561]]),\n",
       " tensor([[0.4026]]),\n",
       " tensor([[0.2061]]),\n",
       " tensor([[0.1684]]),\n",
       " tensor([[-0.2560]]),\n",
       " tensor([[-0.5596]]),\n",
       " tensor([[0.3835]]),\n",
       " tensor([[0.7392]]),\n",
       " tensor([[-0.2441]]),\n",
       " tensor([[-0.3447]]),\n",
       " tensor([[-0.1568]]),\n",
       " tensor([[-0.2883]]),\n",
       " tensor([[0.4378]]),\n",
       " tensor([[0.3571]]),\n",
       " tensor([[-0.3595]]),\n",
       " tensor([[0.2495]]),\n",
       " tensor([[0.1032]]),\n",
       " tensor([[-0.0194]]),\n",
       " tensor([[-0.3131]]),\n",
       " tensor([[0.7314]]),\n",
       " tensor([[-0.2374]]),\n",
       " tensor([[-0.3599]]),\n",
       " tensor([[0.0721]]),\n",
       " tensor([[0.1530]]),\n",
       " tensor([[0.1398]]),\n",
       " tensor([[0.5719]]),\n",
       " tensor([[-0.0543]]),\n",
       " tensor([[-0.0658]]),\n",
       " tensor([[0.1618]]),\n",
       " tensor([[-0.2031]]),\n",
       " tensor([[0.3665]]),\n",
       " tensor([[-0.0288]]),\n",
       " tensor([[0.2988]]),\n",
       " tensor([[0.1737]]),\n",
       " tensor([[-0.1097]]),\n",
       " tensor([[0.0128]]),\n",
       " tensor([[0.1673]]),\n",
       " tensor([[0.1022]]),\n",
       " tensor([[0.5094]]),\n",
       " tensor([[-0.0141]]),\n",
       " tensor([[-0.2975]]),\n",
       " tensor([[0.3838]]),\n",
       " tensor([[0.2931]]),\n",
       " tensor([[0.5610]]),\n",
       " tensor([[0.1081]]),\n",
       " tensor([[0.1783]]),\n",
       " tensor([[-0.3025]]),\n",
       " tensor([[-0.0856]]),\n",
       " tensor([[-0.2168]]),\n",
       " tensor([[-0.0167]]),\n",
       " tensor([[0.3159]]),\n",
       " tensor([[-0.2780]]),\n",
       " tensor([[-0.0443]]),\n",
       " tensor([[0.1847]]),\n",
       " tensor([[-0.2119]]),\n",
       " tensor([[0.4514]]),\n",
       " tensor([[0.2479]]),\n",
       " tensor([[-0.2583]]),\n",
       " tensor([[-0.0768]]),\n",
       " tensor([[0.1700]]),\n",
       " tensor([[0.6597]]),\n",
       " tensor([[-0.0203]]),\n",
       " tensor([[0.5209]]),\n",
       " tensor([[-0.2331]]),\n",
       " tensor([[0.1134]]),\n",
       " tensor([[0.2808]]),\n",
       " tensor([[0.1323]]),\n",
       " tensor([[0.0787]]),\n",
       " tensor([[-0.1914]]),\n",
       " tensor([[0.3337]]),\n",
       " tensor([[-0.2618]]),\n",
       " tensor([[0.0453]]),\n",
       " tensor([[0.2308]]),\n",
       " tensor([[0.6291]]),\n",
       " tensor([[0.7903]]),\n",
       " tensor([[0.5388]]),\n",
       " tensor([[0.0693]]),\n",
       " tensor([[-0.0590]]),\n",
       " tensor([[-0.0423]]),\n",
       " tensor([[-0.5463]]),\n",
       " tensor([[-0.1501]]),\n",
       " tensor([[0.0575]]),\n",
       " tensor([[-0.0668]]),\n",
       " tensor([[-0.5385]]),\n",
       " tensor([[-0.1953]]),\n",
       " tensor([[0.0642]]),\n",
       " tensor([[-0.3944]]),\n",
       " tensor([[-0.2256]]),\n",
       " tensor([[0.6337]]),\n",
       " tensor([[0.6092]]),\n",
       " tensor([[-0.0378]]),\n",
       " tensor([[-0.3020]]),\n",
       " tensor([[0.1372]]),\n",
       " tensor([[-0.1868]]),\n",
       " tensor([[0.2693]]),\n",
       " tensor([[-0.1769]]),\n",
       " tensor([[-0.1411]]),\n",
       " tensor([[0.6325]]),\n",
       " tensor([[0.2133]]),\n",
       " tensor([[0.3560]]),\n",
       " tensor([[0.4319]]),\n",
       " tensor([[0.1753]]),\n",
       " tensor([[-0.6103]]),\n",
       " tensor([[-0.1166]]),\n",
       " tensor([[-0.1070]]),\n",
       " tensor([[0.5163]]),\n",
       " tensor([[-0.0664]]),\n",
       " tensor([[0.3882]]),\n",
       " tensor([[0.8074]]),\n",
       " tensor([[-0.1142]]),\n",
       " tensor([[0.2593]]),\n",
       " tensor([[-0.2894]]),\n",
       " tensor([[0.1584]]),\n",
       " tensor([[0.0927]]),\n",
       " tensor([[-0.4190]]),\n",
       " tensor([[0.1587]]),\n",
       " tensor([[0.1261]]),\n",
       " tensor([[0.2912]]),\n",
       " tensor([[0.2057]]),\n",
       " tensor([[0.5806]]),\n",
       " tensor([[-0.1918]]),\n",
       " tensor([[-0.0887]]),\n",
       " tensor([[0.3519]]),\n",
       " tensor([[-0.2127]]),\n",
       " tensor([[0.6616]]),\n",
       " tensor([[-0.2498]]),\n",
       " tensor([[0.0590]]),\n",
       " tensor([[0.0764]]),\n",
       " tensor([[0.0296]]),\n",
       " tensor([[-0.1784]]),\n",
       " tensor([[0.5844]]),\n",
       " tensor([[0.0893]]),\n",
       " tensor([[0.7832]]),\n",
       " tensor([[-0.5403]]),\n",
       " tensor([[0.9312]]),\n",
       " tensor([[0.2760]]),\n",
       " tensor([[-0.2693]]),\n",
       " tensor([[-0.0417]]),\n",
       " tensor([[-0.1262]]),\n",
       " tensor([[0.3485]]),\n",
       " tensor([[-0.1314]]),\n",
       " tensor([[0.5247]]),\n",
       " tensor([[-0.2498]]),\n",
       " tensor([[0.2926]]),\n",
       " tensor([[0.2198]]),\n",
       " tensor([[-0.3900]]),\n",
       " tensor([[0.3229]]),\n",
       " tensor([[-0.0861]]),\n",
       " tensor([[0.9062]]),\n",
       " tensor([[0.4466]]),\n",
       " tensor([[0.6144]]),\n",
       " tensor([[-0.2335]]),\n",
       " tensor([[0.5037]]),\n",
       " tensor([[-0.0335]]),\n",
       " tensor([[-0.4066]]),\n",
       " tensor([[-0.1405]]),\n",
       " tensor([[-0.0722]]),\n",
       " tensor([[-0.1231]]),\n",
       " tensor([[-0.4236]]),\n",
       " tensor([[0.5676]]),\n",
       " tensor([[-0.4789]]),\n",
       " tensor([[-0.4032]]),\n",
       " tensor([[0.0502]]),\n",
       " tensor([[-0.0475]]),\n",
       " tensor([[0.5703]]),\n",
       " tensor([[-0.0444]]),\n",
       " tensor([[-0.3207]]),\n",
       " tensor([[0.0262]]),\n",
       " tensor([[-0.0775]]),\n",
       " tensor([[0.2671]]),\n",
       " tensor([[0.1042]]),\n",
       " tensor([[0.0017]]),\n",
       " tensor([[0.1626]]),\n",
       " tensor([[0.0192]]),\n",
       " tensor([[-0.0519]]),\n",
       " tensor([[-0.0942]]),\n",
       " tensor([[-0.4481]]),\n",
       " tensor([[0.0037]]),\n",
       " tensor([[0.2342]]),\n",
       " tensor([[0.0346]]),\n",
       " tensor([[0.4859]]),\n",
       " tensor([[-0.0697]]),\n",
       " tensor([[-0.2904]]),\n",
       " tensor([[0.0940]]),\n",
       " tensor([[0.1190]]),\n",
       " tensor([[0.2569]]),\n",
       " tensor([[-0.2350]]),\n",
       " tensor([[-0.4670]]),\n",
       " tensor([[0.3651]]),\n",
       " tensor([[1.0317]]),\n",
       " tensor([[0.3854]]),\n",
       " tensor([[0.1859]]),\n",
       " tensor([[0.3035]]),\n",
       " tensor([[-0.1952]]),\n",
       " tensor([[-0.2376]]),\n",
       " tensor([[0.3344]]),\n",
       " tensor([[-0.4854]]),\n",
       " tensor([[-0.2091]]),\n",
       " tensor([[0.0883]]),\n",
       " tensor([[0.0244]]),\n",
       " tensor([[-0.0222]]),\n",
       " tensor([[0.3139]]),\n",
       " tensor([[-0.1026]]),\n",
       " tensor([[0.1805]]),\n",
       " tensor([[-0.1251]]),\n",
       " tensor([[-0.0350]]),\n",
       " tensor([[-0.4489]]),\n",
       " tensor([[0.1040]]),\n",
       " tensor([[0.2899]]),\n",
       " tensor([[0.1573]]),\n",
       " tensor([[0.0479]]),\n",
       " tensor([[0.4045]]),\n",
       " tensor([[-0.4613]]),\n",
       " tensor([[0.0009]]),\n",
       " tensor([[-0.1454]]),\n",
       " tensor([[0.4038]]),\n",
       " tensor([[0.2649]]),\n",
       " tensor([[0.6989]]),\n",
       " tensor([[0.3949]]),\n",
       " tensor([[-0.0149]]),\n",
       " tensor([[-0.1893]]),\n",
       " tensor([[0.2769]]),\n",
       " tensor([[0.2401]]),\n",
       " tensor([[-0.2797]]),\n",
       " tensor([[-0.2996]]),\n",
       " tensor([[-0.1805]]),\n",
       " tensor([[0.2805]]),\n",
       " tensor([[0.2038]]),\n",
       " tensor([[-0.0626]]),\n",
       " tensor([[0.2388]]),\n",
       " tensor([[0.3445]]),\n",
       " tensor([[0.6691]]),\n",
       " tensor([[0.2322]]),\n",
       " tensor([[-0.1595]]),\n",
       " tensor([[0.2530]]),\n",
       " tensor([[0.2586]]),\n",
       " tensor([[-0.1412]]),\n",
       " tensor([[0.2434]]),\n",
       " tensor([[-0.2303]]),\n",
       " tensor([[-0.0174]]),\n",
       " tensor([[-0.1584]]),\n",
       " tensor([[0.0864]]),\n",
       " tensor([[0.0049]]),\n",
       " tensor([[-0.4120]]),\n",
       " tensor([[0.0022]]),\n",
       " tensor([[-0.1522]]),\n",
       " tensor([[0.3588]]),\n",
       " tensor([[-0.4050]]),\n",
       " tensor([[-0.2698]]),\n",
       " tensor([[0.4448]]),\n",
       " tensor([[-0.0482]]),\n",
       " tensor([[0.0880]]),\n",
       " tensor([[-0.3014]]),\n",
       " tensor([[-0.1100]]),\n",
       " tensor([[0.6154]]),\n",
       " tensor([[0.3376]]),\n",
       " tensor([[0.2774]]),\n",
       " tensor([[0.7728]]),\n",
       " tensor([[-0.1469]]),\n",
       " tensor([[-0.0199]]),\n",
       " tensor([[-0.1031]]),\n",
       " tensor([[-0.2884]]),\n",
       " tensor([[0.7524]]),\n",
       " tensor([[0.3341]]),\n",
       " tensor([[0.0143]]),\n",
       " tensor([[0.2896]]),\n",
       " tensor([[0.5794]]),\n",
       " tensor([[-0.4017]]),\n",
       " tensor([[-0.1979]]),\n",
       " tensor([[0.5431]]),\n",
       " tensor([[0.0110]]),\n",
       " tensor([[-0.1643]]),\n",
       " tensor([[-0.1555]]),\n",
       " tensor([[0.0138]]),\n",
       " tensor([[0.7414]]),\n",
       " tensor([[0.3056]]),\n",
       " tensor([[0.0487]]),\n",
       " tensor([[-0.1715]]),\n",
       " tensor([[0.1729]]),\n",
       " tensor([[0.2390]]),\n",
       " tensor([[-0.4692]]),\n",
       " tensor([[0.0864]]),\n",
       " tensor([[0.3015]]),\n",
       " tensor([[0.5756]]),\n",
       " tensor([[0.1389]]),\n",
       " tensor([[-0.0869]]),\n",
       " tensor([[-0.2231]]),\n",
       " tensor([[0.3136]]),\n",
       " tensor([[-0.1215]]),\n",
       " tensor([[0.8029]]),\n",
       " tensor([[-0.1922]]),\n",
       " tensor([[0.1092]]),\n",
       " tensor([[0.6193]]),\n",
       " tensor([[-0.2739]]),\n",
       " tensor([[0.2943]]),\n",
       " tensor([[0.1123]]),\n",
       " tensor([[0.4510]]),\n",
       " tensor([[0.0490]]),\n",
       " tensor([[-0.0661]]),\n",
       " tensor([[-0.2026]]),\n",
       " tensor([[0.2046]]),\n",
       " tensor([[0.0887]]),\n",
       " tensor([[0.0220]]),\n",
       " tensor([[-0.1700]]),\n",
       " tensor([[-0.2488]]),\n",
       " tensor([[-0.2677]]),\n",
       " tensor([[0.3645]]),\n",
       " tensor([[0.1814]]),\n",
       " tensor([[-0.2080]]),\n",
       " tensor([[0.2894]]),\n",
       " tensor([[-0.1524]]),\n",
       " tensor([[-0.2584]]),\n",
       " tensor([[0.2041]]),\n",
       " tensor([[-0.2953]]),\n",
       " tensor([[-0.1951]]),\n",
       " tensor([[0.3440]]),\n",
       " tensor([[-0.4827]]),\n",
       " tensor([[0.7552]])]"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G9puD_0zkC2c"
   },
   "source": [
    "### Writing Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oQvvIhPDkUnR"
   },
   "source": [
    "Here is our function to write the scores into a txt file. We can follow the <Method> <ID> <SCORE> template but having only the scores will work too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LN3NtkF4kPxw"
   },
   "outputs": [],
   "source": [
    "def writeScores(method_name,scores):\n",
    "    fn = \"predictions.txt\"\n",
    "    print(\"\")\n",
    "    with open(fn, 'w') as output_file:\n",
    "        for idx,x in enumerate(scores):\n",
    "            #out =  metrics[idx]+\":\"+str(\"{0:.2f}\".format(x))+\"\\n\"\n",
    "            #print(out)\n",
    "            output_file.write(f\"{x}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "FVss_RLBkFei",
    "outputId": "5912757b-bbda-4d4a-da62-9ee36c0009e6"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-04e6661f43ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#Predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mpredictions_de\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf_de\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_de\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'clf_de' is not defined"
     ]
    }
   ],
   "source": [
    "#EN-DE\n",
    "\n",
    "de_test_src = get_embeddings(\"./test.ende.src\",nlp_en,'en')\n",
    "de_test_mt = get_embeddings(\"./test.ende.mt\",nlp_de,'de')\n",
    "\n",
    "X= [np.array(de_test_src),np.array(de_test_mt)]\n",
    "X_test = np.array(X).transpose()\n",
    "\n",
    "#Predict\n",
    "\n",
    "predictions_de = clf_de.predict(X_val_de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XWnNUR0Gku_9"
   },
   "outputs": [],
   "source": [
    "writeScores(\"SVR\",predictions_de)\n",
    "\n",
    "with ZipFile(\"en-de_svr.zip\",\"w\") as newzip:\n",
    "\tnewzip.write(\"predictions.txt\")\n",
    " \n",
    "files.download('en-de_svr.zip') "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "NLP_CW_final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
